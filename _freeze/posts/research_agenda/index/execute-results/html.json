{
  "hash": "40ae3f56045be1c9528e8674fce85b33",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 2021-06-05\ntitle: \"Do regional economists answer the right questions?\"\nnumber-sections: true\nauthor: Thomas de Graaff\nabstract: \"This position paper revolves around two main propositions; namely,\n  (*i*) regional (or spatial) economists are very restrictive in the tool\n  set they apply, and consequently (*ii*) their models do not always\nmatch with the type of questions policy makers are concerned about. To start\nwith the latter, policy makers---whether national, regional or local---are\noftentimes concerned about holistic approaches and future predictions. Exemplary\nquestions are *Which policy instrument works best for my city*, *What\nhappens after the construction of this highway with housing prices and\nemployment throughout the whole region* and *Given limited budget, which\nregion should I first invest in*. Regional economists---actually, most\neconomists---usually isolate phenomena in order to, at best, explain the *causal* impact\nof a single determinant. Indeed, most regional economists feel very\nuncomfortable when asked to predict or give the best set of determinants for a\ncertain phenomenon. This has its consequences for the tool set regional\neconomists apply. Usually a parametric regression type of framework is applied\nisolating the determinant under consideration and controlling as much as\npossible for observables and unobservables, ideally in a pseudo-experimental\nframework. A direct consequence of this approach is that emphasis is very much\non explaining the impact of an isolated determinant and not on predicting\n(non-marginal) changes in larger systems. For many research questions that is the\nright approach. For other research questions (revolving around non-marginal\nchanges and predicting) we need other more data driven tools. Therefore, my main\nargument would be that we allow for more data science techniques in the toolkit\nof the regional economist, both in research and in education.\"\ncategories:\n- Regional economics\n- Predicting\n- Causality\n- Theory driven approach\n- Data science\n---\n\n\n\n## Introduction: two different cultures\n\n\n> The sexiest job in the next 10 years will be statisticians. [@varian2014big]\n\nThe quote above from Hal Varian is in one aspect wrong; nowadays, we do not call them statisticians but data scientists instead. Nevertheless, in the last two decades companies such as Google, Ebay, Whatsapp, Facebook, Booking.com and Airbnb, have not only witnessed enormous growth but to a considerable extent also changed the socio-economic landscape. Indeed, with the increasing abundance of (spatial) data and computer capacity, the ability to gather, process, and visualize data has become highly important and therefore highly in demand as well. And all the models and tools these data scientists within these companies use are very much *data driven* with often remarkable results. \n\nIn his controversial and path-breaking article, @breiman2001statistical presented two different cultures in statistical science. One governed by a (probability) theory-driven modeling approach and one governed by a more (algorithmic) data-driven approach. These two cultures carry over to the econometric and ultimately the empirical regional economics domain [I use a wide definition for the regional economics domain, which consists of most aspects of regional science in general but for  which the theoretical approach is always from an economic perspective. Topics such as, e.g, interregional migration, trade, transport flows and commuting on the one side and regional performance, regional clustering, population growth and specialisation on the other side fall all under this, admittedly, rather wide umbrella.]{.aside} as well, where---commonly for all social sciences---the theory driven approach still very much dominates the landscape of the realm of contemporary regional economics. \n\n::: {#fig-approaches layout-ncol=2}\n\n![Model approach](modelapproach.png){#fig-model}\n\n![Data approach](dataapproach.png){#fig-data}\n\nTwo cultures of statistical/econometric modeling [Insipired by @breiman2001statistical]\n:::\n\n@fig-approaches is an adaptation from the one displayed in @breiman2001statistical and describes the processes governing these two\ncultures. @fig-model is what I refer to as the modeling approach, where a statistical model is postulated and is central to this\nculture. This is the classical approach [Sometimes as well referred to\n  as the frequentists' approach. However, this typically concerns the debate\n  between classical statistics and Bayesian statistics, where the two approaches\n  I refer to are more concerned with wider frameworks, of which the Bayesian\n  approach is just one of the elements. I come back to Bayesian statistics and\n  the frequentists' approach later, but I do not see them necessarily as\n  opposites. And I quite object to the term frequentists' approach as Bayesian\n  statistics is much more focuses on counting then the frequentists' approach.]{.aside}\n  where statistical probability theory\n  meets the empiricism of Karl Popper.[To be honest, Popper himself was not a\n  great fan of simply null hypothesis testing. He actually argued for the\n  falsification of explanatory models, where in his view falsification does not\n  only rely on statistics but on consensus amongst scientists as well.]{.aside} Usually the model assumed is stated as a linear model and in its most simple form can be denoted as:\n\n$$\n\t\\mathbf{y} = \\mathbf{x}\\beta + \\epsilon, \n$${#eq-linreg}\nwhere in (regional) economics language, $\\mathbf{x}$ is referred to as the\nindependent variable, $\\mathbf{y}$ as the dependent variable and $\\epsilon$ as a\nresidual term. In this setup, using the data at hand, one constructs a\nstatistical test to which extent the estimated coefficient (denoted with\n$\\hat{\\beta}$) deviates from a hypothesized value of the coefficient (denoted\nwith $\\beta_0$)---typically the hypothesis $H_0: \\hat{\\beta} = 0$ is used with\nas alternative hypothesis that $H_1: \\hat{\\beta} \\neq 0$. However, that is\nalways within the context of the \\textit{postulated} model. So, when the\nnull-hypothesis is rejected, it not necessarily means that the true $\\beta$ is\nunequal to zero, it might also be caused by errors in measuring $\\bf{x}$ or even\nusing the wrong \\textit{model}![One of the assumptions for regression\n  techniques such as the one used here is actually no misspecification of the\n  model, but---apart from some possible tests on the functional form\n  \\textit{within} a specific regression form---usually little attention is give\n  on the validity of the model used. More importantly, within this framework the\n  model itself is usually not tested \\textit{a posteriori}.]{.aside}[There is\n  another fallacy with this approach that is often overlooked and that is that\n  the alternative hypothesis being true is a probability as well. Namely,\nmost hypotheses researchers test are typically not very probable. Not taken this\ninto account would actually lead to more null hypotheses to be rejected then\nshould be (false positives).]{.aside}\n\n@fig-data yields a schematic overview of a more data driven\napproach. Here, we see an unknown model fed by predictors $\\mathbf{x}$ that lead\nto one or multiple reponses $\\mathbf{y}$. The main objective here is not to test\nhypotheses, but to find the best model instead which able to explain the\n\\emph{in-sample} data and to predict the \\emph{out-of-sample} data.\nUsually, the models are evaluated by some kind of criterion (e.g., the mean squared\nerror), which is not completely unlike the modeling approach. However, there are\ntwo main differences between the two approaches. First, the data driven approach\nconsiders several models in a structural approach. For instance, the question\nwhich variables to include is captured by an exhaustive sourse of all\ncombinations in the modeling approach (e.g., with classification and regression\ntrees or random forests), while in the theory driven approach, the choice of\nvariables is based on the theory and a small number of variations in the\nspecification. Second, measurements on model performance are done\n\\textbf{out-of-sample} in the data driven approach and, typically,\n\\textbf{in-sample} in the model approach. The latter is not that important for\nhypothesis testing, but for prediction this matters enormously, because adding\nparameters might increase the in-sample fit, but actually worsen the\nout-of-sample fit (a phenomenon called overfitting).  \n\nIn economics in general, and in regional economics\nin specific, most of the tools employed are very much **theory or model\n  driven** instead of data driven. My (conservative) estimate would be that at least 90\\%\nof all empirical work in regional economics revolves around postulating a\n(linear) model and testing whether (a) key determinant(s) is (are) significantly\ndifferent from a hypothesized value---usually zero.[In a seminal\n  contribution, @breiman2001statistical states that deep into the 90s 98\\%\n  of the statisticians actually employed the theory driven paradigm and only 2\\%\n  a data driven paradigm. With the advent of the availability of internet\n  connectivity, large (online) data sources, and faster computers the\n  statistical realm changed dramatically. However, this has not permeated yet in\n  the social sciences [see as well @varian2014big].]{.aside} That is, \\textit{within} the context of the model assumed.\n\nAt best, this approach can be seen in a causal inference framework. If a\ndeterminant (such as a policy in the context of regional economics) $x$ changes, does it\ncause then a change in the output $y$ (most economists typically use some\nwelfare measure).[Most of this research actually intends to mimic a\n  **difference-in-difference** approach and gained enormous momentum with\n  the textbook of @angrist2008mostly.]{.aside} This approach thus provides a\nrigid and useful approach to regional policy evaluation. If we implement policy\n$x$, does welfare measure $y$ then improve? Note that this always considers a\n**marginal** change as $x$ is usually isolated from other (confounding) factors.\n\nHowever, policy makers oftentimes have different questions for which they need\nsolutions. Usually, they revolve around questions starting with *What\n  determines performance measure $A$?*, *Which regions can we best\n    invest in?* or, more generally, *What\n  works for my region?*. These types of questions require a different approach\nthan the previous one. Namely, the former type requires an approach focused on **explaining** while the latter type requires an approach focused on **predicting**.\n\nThe remaining part of this position paper is structured as follows. @sec-practices gives an overview of current modeling practices and describes the `traditional' inference based approach as well as some data-driven approaches that have been\nused in the recent past (though by far not as often as the traditional methods). @sec-agenda sets out both a research and an education agenda as it addresses how to bridge the gap between the daily practices of regional economists and the demands of local policy makers. The final section shortly summarizes the main points raised in this position paper.  \n\n## Regional economists turning the blind eye {#sec-practices}\n\nUnmistakably, in the recent decade the two major changes to economic empirical research in general\nare the advent of increasingly larger data sources and the large increase in\ncomputer power [@einav2014economics]. The methods that most economists\nemploy, however, have not changed. Linear regression or one of its close\nrelatives (such as logistic, poisson or negative binomial regression), preferably in a causal\nframework, is still the most common tool. This also applies to regional\neconomists, who---although coming from a tradition to use various methods from\ndifferent disciplines---have increasingly used similar methods as in\n``mainstream'' economics.\n\nThis focus on marginal effects and causality is\ncertainly very worthwhile and brought us many important insights. However, it is\nalso typically done within a very narrow framework and, below, I will lay out what we are missing both in\nresearch and in our educational curricula, when our \\emph{main} focus is on the\nframework above and as advocated so much as in @angrist2008mostly.   \n\n### The blind eye in research\n\nThe traditional model of a (regional) economist looks as follows:\n$$\n  y_i = \\alpha + \\beta x_i + \\mathbf{z}_i\\gamma + \\epsilon_i,\n$${#eq-model_economist}\nwhere $y_i$ is referred to as the dependent variable, $x_i$ is the main variable of\ninterest, and $\\mathbf{z}$ is a vector of other variables. $\\alpha$, $\\beta$ and\n$\\gamma$ are parameters, where we are especially interested in the value of\n$\\beta$. Finally, $\\epsilon_i$ is an identical and independent distributed error\nterm.\n\nUsually the main aim is to estimate $\\beta$ as unbiased as possible in a causal framework. So, ideally, we\nwould like to control for unobserved heterogeneity bias, specification bias,\nmeasurement error, reverse causality, selection bias, and so forth. Econometric\ntheory has produced some very powerful techniques to control for some of these\nbiases, such as instrumental variables, diff-in-diff procedures and the use of\nfixed effects. However, these methods are not panacea for everything. First, they work\nwonders for only specific research questions that have to do with the preferably\ncausal effects of **marginal** changes. Second, some of these techniques require very specific and\nstrong assumptions which are possibly not always met, which leaves doubts upon\nthe validity of the results. \n\nBelow, I will deal with instrumental variables, diff-in-diff and fixed effect\ntechniques consecutively. I will specifically focus on some of the disadvantages. Some of the\narguments are adaptions from @deaton2010instruments and I refer to this\nreference for a more complete treatise on the disadvantages of using\ninstrumental variables and diff-in-diff methods. For all the advantages not\ndealt with in this paper, read @angrist2008mostly. \n\n#### Exogeneity versus independence\n\nEconomists love instrumental variables, because a good instrumental variable can\ntackle reverse causality, measurement error and unobserved heterogeneity bias\nall at one. Originally, instrumental variables come from simultaneous economic\nmodels such as supply and demand models. A classical example in a regional\ncontext would be:\n$$\n\\begin{aligned}\n  P_r &=  \\alpha + \\beta E_r + \\mathbf{z}_r\\gamma + \\epsilon_r, \\label{P}\\\\\n  E_r &=  \\delta + \\kappa P_r+ \\mathbf{w}_r\\lambda + \\nu_r,\\label{E}\n\\end{aligned}\n$${#eq-PE}\nwhere $P$ denotes population, $E$ employment and $z$ and $w$ are vector of other\nregional $r$ characteristics. $\\alpha$, $\\beta$, $\\gamma$, $\\delta$, $\\kappa$\nand $\\lambda$ are parameters to be estimated.\n\nObviously, one can not directly estimate @eq-PE because of the\nintrinsic simultaneity. However, suppose one is interested in estimating the\nimpact of employment on population growth, then one can use the second equation of @eq-PE and search\nfor **exogeneous**[This is not really precise; I mean exogeneous to\n  population variation. I will come back to the use of exogeneous later.]{.aside} variation in employment to use it as an instrumental\nvariable. A possible strategy could be to look into the population changes of\nsurrounding regions (but within commuting distance), as they might not have an\nimpact of the population change in the current region\n[see @DeGraaff2012; @Graaff2012]\n\nThe main point [Directly from @deaton2010instruments.]{.aside},\nhowever, is that equations @eq-PE constitute a full-blown\neconomic \\emph{model} which has direct relations with underlying structural\ntheoretical modeling frameworks [such as @roback1982wages. And the\ninstrument then comes directly (is internal) from the model. \n\nIn practice, however, researchers often take another approach. And that is to\nlook for external instruments. Instruments that have no relation with a\nstructural (simultaneity) model. And there are is (a large) potential pitfall when doing so\nand that is to end up with an instrumental variables that is not independent from\nthe left-hand-side variable. As it seems, there is some confusion about terms as\nindependence and exogeneity, so let's first clarify the exact assumptions a\nvalid instrument should satisfy.\n\nSuppose that somebody is interested in the impact of population on employment; so, one would like to\nidentify $\\kappa$ in @eq-PE. To control for endogeneity researchers then\nsearch for an **exogenous** and **relevant** instrument, $Z_r$. The latter\nindicates that the instrument has an impact on the possible endogeneous variable\n($P_r$) and the former indicates that the instrument does not affect the\nleft-hand-side variable ($E_r$), only via $P_r$ and other instruments. In formal\nnotation: $E_r \\perp Z_r|P_r, w_r$. Thus, exogeneity means that the instrument\nand the left-hand-side variables are **independent** from each other conditional on\nthe instruments.\n\nUnfortunately, exogeneity is often used as an argument for variables that are\nexternal to the system, denote a sudden shock or for phenomena that are\nconsidered to be exogenous in other fields (such as in geography).\nAnd this usually leads to instruments that do not satisfy the\nindependence assumption. I will give three examples below.\n\nFirst, and very often use is the concept of deep lagging. So, in our case, we\nlook for regional population say 100 years ago and use that as an instrument. It\nmust be exogenous because we cannot change it, right? Well, it is definitely\nrelevant, as regional population is remarkably resilient. Where people lived\n100 years ago, they most likely live today. But, if we take model\n@eq-PE seriously, then the population 100 years ago, must at least\nhave affected employment 100 years ago, and if population is resilient then most\nlikely employment as well (and we even do not consider yearly temporal dynamics\nbetween population and employment). So, in all likeliness, employment and\npopulation 100 years are not (conditionally) independent.\n\nThe second type of instruments people often use are regional characteristics\n(preferably deeplagged as well), and specifically accessibility measures as\nroad, railroads and canals. For a large audience the following story typically seems very plausible at first\nsight. At the end of the 19th century the large scale introduction of the\nrailways enabled households to live further from home and escape the heavilty\npolluted inner cities where the factories remained (making use of the same\nrailroads intersecting in city centres). Railroads thus changed the location of\npopulation and not that of employment. While this story is entirely possible,\nwhat is often overlooked is the fact that factories and thus employment changed location as\nwell, but only 20-30 years later, and typically along the same links as opened\nup by railway lines. So, the railway network 140 years ago and contemporary\nlocation of employment are not (conditionally) independent.\n\nA last often used category of candidate instruments is geography-related\nvariables. In our case that could be regions of municipalities. For instance,\nthe Netherlands witnessed for a large period intensive population location\npolicies. This entailed that the dutch government pointed out municipalities\nthat were allowed to grow (in terms of housing policies). Using fixed effects of\nthese specific municipalities then as instruments sound as a viable strategy.\nHowever, this requires strict assumptions. Namely, being a specific municipality\nwill only have an effect on employment through being designated by the Dutch\ngovernment; and by nothing else.\\footnote{A similar argument has been made by\n  \\cite{deaton2010instruments}, who considers economic growth theory, where fixed\n  effects of specific countries are used as instruments because they took part\n  in specific agreements---i.e., the Camp David accord for Egypt. Then the\n  heroic assumptions has to be made that being Egypt has no effect on growth,\n  except for the Camp David accord.}\n\nIs this to say that instrumental variables is a bad technique? No, absolutely\nnot. If the instrument is valid, this is one of the most powerful techniques in\nthe econometric toolbox. The point made here is that good instruments are\nactually hard to find and that structural simultaneous models (typically, in the\ncontext of supply and demand) usually work better to find instruments than\ninstruments that are completely external to your problem. And if you really need\nto use an external instrument, be very specific and open about the assumptions\nyou need to make.\n\n#### Local and average treatment effects\n\nTwo concepts which have received quite some attention recently in econometrics, but is often\noverlooked in applied regional and urban economics are local average treatment\neffects and average treatment effects. The former deals with the interpretation\nof instrumental variables, the latter with the (interpretation) of spatial\ndifference-in-difference methods. Even though these methods are different, they\nhave similar consequences for the interpretation of research findings and their\nunderlying assumptions.\n\nThe **Local Average Treatment Effect** (LATE) deals with the underlying\nassumptions that have to be made so that the instrumental variable estimation\nactually measures what we want [see @imbens1994identification. Referring again to our example above and say\nthat we want to instrument regional population changes with municipalities being\ndesignated by a national policy to increase local housing supply. What we then\nactually measure is the effect of changes in population on employment of those\nmunicipalities that have actually complied with the national policy.\nMunicipalities that dropped out in an earlier stage are not taken into account,\nbut municipalities who did comply but never implemented the policy are.\n\nSo, what is actually measured is the designation of municipalities to a policy,\nwhich might be a very interesting research question indeed, but in all\nlikelihood does not necessarily coincide with the coefficient $\\kappa$ in\nmodel @eq-PE above. In almost all cases the LATE theorem points at a more\nrestrictive effect (and thus interpretation) of the instrumental variable than\nthe \\emph{structural} model sets out to estimate. Only under very strong\nassumptions---homogeneity of regions, perfect compliance, and so forth---\nthe coefficient by the instrumental variable coincides with the coefficient of\nthe structural model.  \n\nA different but related issue is that of the average treatment effects. Since\nthe seminal work of @angrist2008mostly difference-in-difference methods\n(and all its variants) gained enormously in popularity. As well as in regional\neconomics where spatial difference-in-difference are applied as often as\npossible. The idea itself is rather straightforward and originates from the\nsearch for semi-experimental randomized controlled trials (RCT's). \n\nFor the regional domain, assume the following: there is one group of\nmunicipalities that implement a policy (the treatment; $T = 1$) and one group of municipalities that does\nnot ($T = 0$). Both groups of municipalities are measured before ($t = 0$) and after\nimplementation ($t=1$).\nThen we can rewrite model @eq-model_economist as:\n\n$$\n  y_r = \\alpha + \\gamma_1 T_r + \\gamma_2 t_r+ \\beta (T_r \\times t_r) + \\epsilon_r,  \n$$ {#eq-diff}\n\nwhere $Y_r$ denotes a specific regional outcome variable, $T_r$ the set of\nregions that are treated and $t_r$ the post implementation period. In this\nset-up $\\gamma_1$ measures the average impact of the treated regions, $\\gamma_2$\nthe impact of the time period, and $\\beta$ is our coefficient of interest; being\nthe impact of the treatment. Note that $\\beta$ in this setting actually denotes\nthe difference in the outcome of the treatment groups **minus** the\ndifference in the outcome of the non-treated groups: hence, the name\ndifferences-in-differences.\n\nThe main assumption for this technique relies on the trueness of randomization of treatment\nacross, in our case, municipalities. In reality, the concept of randomization is\ndifficult to defend. Poor regions are more likely to receive governmental\nsubsidies, accessibility improvement are usually implemented in dynamic and\nsuccesful regions, and so forth. To circumvent this, researchers look at borders\nbetween regions. Back to our example, we then look at individuals close to a\nborder between two municipalities, where one municipality received a treatment\nand the other did not. It is then defendable that such a band around a border is\nrelatively homogeneous in characteristics an that both regions are thus equal\nexcept for the receivement of treatment.  \n\nThis approach has two main consequences. The most mentioned consequence is that\nthe effect $\\beta$ is a so-called mean treatment effect. Every region, firm or\nindividual benefits (is harmed by) equally from the treatment. So, it might very\nwell be that some regions benefit greatly from some a policy, while it is\nactually harmful for others. Making the treatment effect more heterogenous is\ndifficult and requires a lot from the data as every subgroup needs its own\nsemi-experimental randomized control trial.\n\nExtending this argument to spatial difference-in-difference methods leaves us\neven with the assumption that the whole region should be alike the border area\nin term of benefitting from the policy. Or one should be satisfied with the fact\nthat $\\beta$ only tells us something about the effect of the policy in a border\narea. An area most likely not very representative of the rest of the region. \n\nThe other consequence relates again to the compliance assumption. Regions and\nmunicipalities themselves can be argued to fit well in treatment or\nnon-treatment groups. And if not, non-compliance should be easily detected.\nHowever, for firms and individuals, compliance to randomization\nof treatment is often a very harsh assumption. More and more, evidence is found\nthat especially individuals are very resourceful to circumvent randomization,\nwhether it by allocation to class sizes, elementary schools, or even military\ndraft by lottery.\n\nRandomized controlled trials and difference-in-difference methods are strong\ntechniques for the applied regional economist. The point here is, however, that\nwithout very strong assumptions, findings are mean effects that only applied to\na limited part of the total sample. \n\n#### Fixed effects and heterogeneity\n\nAn often used technique in applied econometrics is the use of fixed effects.\nThey work brilliantly in removing unobserved heterogeneity but they come at a\nprice which is typically overlooked. Namely, they remove valuable variation as well in\nboth the dependent (predictor) $x$ and the independent (reponse) variable $y$.\n\nConsider the following model in @eq-density, which is at the moment a heavily\nresearched issue in both regional and urban economics. The issue here is to what\nextent city density increases individual productivity.  \n\n$$\n  \\ln(w_{ic}) = \\alpha + \\beta \\ln(d_{ic})+\\epsilon_{ic},\n$${#eq-density}\n\n$w_{ic}$ denotes here individual wages (as a proxy for productivity) and\n$d_{ic}$ density of the city $c$ individual $i$ lives in. $\\beta$ is our\nparameter of interest and because of the log-log structure $\\beta$ denotes an\nelasticity. Obviously, direct\nestimation of @eq-density, would lead to a misleading parameter\n$\\beta$ if one is aiming to measure a causal effect.[I specifically do not use\nthe term \\emph{biased} here. Namely, @eq-density is a perfectly fine\nmodel to measure the overall correlation ($\\beta$) between city density and individual\nwages and mosts non-economists are perfectly fine with this model\n[see, e.g., @bettencourt2010unified]. So, whether a parameter is biased depends ultimately upon the research question.]{.aside}\nNamely, $\\beta$ might be influenced by other (confounding) factors than only city density. One\ncan think of factors such as skill level of the city population, accessibility of the city,\nsector structure of the city and city government. Moreover, a phenomenon called\nsorting might occur, where more ambitious, risk-seeking and high-skilled people\nmigrate into larger and more dynamic cities.\\footnote{Another issue we will not\n  deal with here is reverse causality where it might be that higher wages lead\n  to larger in-migration and thus larger density. This can, however, not be solved\nwith fixed effects, but with tools as instrumental variables instead. We\ntherefore leave it out of the discussion in this paragraph.}\n\nTo answer the question to what extent density causes wages, researchers\ntherefore resolved to using fixed effects. A baseline model can be seen in\n@eq-fe. \n\n$$\n  \\ln(w_{ic}) = \\nu_i + \\xi_c + \\beta \\ln(d_{ic})+\\epsilon_{ic},\n$${#eq-fe}\n\nhere, $\\nu_i$ denotes individual $i$ specific fixed effects and $\\xi_c$ city $c$\nspecific fixed effects. So, everything that does not vary over time for\nindividuals and cities is now controlled for. A more insightful way what exactly\nhappens is to write @eq-fe in changes, such as: $\\Delta \\ln(w_{ic}) =\n\\beta \\Delta\\ln( d_{ic}) + \\epsilon_{ic}$.[Using changes (first differences)\nto remove fixed effects is a viable but often overlooked technique for dealing\nwith fixed effects.]{.aside} So, our @eq-fe now identifies the\n\\emph{causal} effect by looking at the impact of a change in density on a change\nin wages \\emph{for the same individual within the same city}.\n\nMultiple improvements have already been to this model including controlling for\nsector/task of work and migrating between cities. Including these fixed effects\n(and many more) has had a profound effect on the value of $\\beta$. Directly\nestimating @eq-density yields an elasticity of around $1.15$,\nwhile estimating a model such as @eq-fe including many fixed effects\nwould yield an elasticity of around $1.02-1.03$. So, there are economies of\nagglomeration, but they are not very large.\n\n::: {#fig-approaches layout-ncol=2}\n\n![fixed effects](fe.png){#fig-fe}\n\n![heterogeneity in $\\beta$](hetero.png){#fig-hetero}\n\nHeterogeneity in levels versus slopes\n:::\n\n\nIs this now the end of the story? Alas, it is not. At least three remarks can be\nmade which put the above into perspective.\n\nFirst of all, note that we need changes over time---in our case in individual wages and\ncity density. Now, if we take the extreme example of a subgroup of individuals\nwho do not face wage changes and cities who remain relatively of equal size,\nthan this subgroups will not be used for determination of $\\beta$. Of course,\nnot many observations will have these characteristics. Unfortunately, with\nmore detailed data on sector structure and migration, we need individuals that\nmove both residence and job for identification. All others are redundant. This\nincreases the risk on what is called sample selection bias---identification is\nbased on a specific subgroup with deviant characteristics. The point made here,\nis that with the use of many fixed effects, much is demanded from the data and\none need always check whether the sample used for identification is not too\nrestrictive.  \n\nSecondly, if there are unobserved factors that both relate to wages and density,\nthen it is actually very likely that these unobserved factors are related to\ntheir **changes** as well. One particular example here is technological\nchange, which might affect density (suburbs) and wages at the same time, and is\ndefinitely not time-invariant. If one thinks about it, most interesting\nsocio-economic phenomena are not time-invariant, except perhaps longitude and\nlatitude. For example, a specific argument to use fixed effects is to control\nfor local attractivity. But what individuals and firms find attractive does\nchange of time, especially within cities, but across cities as well. Before\nair-conditioning cities in Florida and Nevada were definitely not as popular as\ntoday. And malaria-rich areas such as wetlands and river banks were always\navoided until recently.   \n\nThirdly, the use of fixed effects is based upon the assumption that all\nvariation is based on variation of **levels**. That is, each fixed effect\nactually denotes a very specific constant (for each individual and city in our\ncase). However, this really requires a very homogeneous sample except in levels.\nFor illustration, assume that there are three individuals, where individual 3\nhas higher wages than individual 1 and 2, because of, say, differences in skill\nlevels (see as well @fig-fe). However, as @fig-fe clearly shows\nas well, apart from individual level variation, returns to density are similar\nfor individuals 1, 2 and 3. So, each individual benefits equally from moving\nfrom a small village not a large metropolitan area. Now, assume that individuals\nare different with respect to the returns by living in large and denser cities.\nThen the impact $\\beta$ should also differ among individuals as is illustrated\nin @fig-hetero. This is not an argument to say that using fixed\neffects is wrong. But if the sample might be heterogenous, i.e. that\nunits respond differently to different predictors, then using fixed effect might\nnot yield a complete pictures and in some specific cases even a distorted picture. \n\nFixed effect techniques is a must have for every empirical regional economists.\nHowever, the message I would like to convey here is that it does not remove\ntime-invariant unobserved heterogeneity (of which there is more than most\nresearchers realise), is not very suitable for tackling heterogeneity in your\nmain effect and might lead in some cases to sample selection bias.\n\n### The blind eye in education\n\nSo, if the main instruments of regional economists are not always applicable and\nwe miss tools in our toolbox to tackle, e.g., heterogeneity, prediction and\nnon-marginal changes, how do we then fare in teaching? Are the students who now\ngraduate equipped with the right toolbox that they use as well in their later\ncareers? And do we have a consistent curriculum using similar or complementary\ntools running from the bachelor to the graduate studies? These types of\nquestions are not frequently asked, and, if at all, not very well met. Mostly\nbecause of vested interests of departments and researchers.\n\nIn this subsection I will, however, try to answer partly some of these questions\nand identify what is missing in our curriculum. I will first look at the\ntraditional applied econometrics approach and then to the (non-existence) of\nother courses geared towards data science, including the use of statistical software.\n\n#### Statistics & Applied Econometrics\n\nIn contemporary economic bachelor curriculae students typically follow one\napplied statistics course, where some hands-on experience is offered by working\nwith small datasets---typically in menu driven statistical software such as SPSS\nor STATA. In the master phase, if students start to specialise in, e.g.,\nregional economics, students then follow one applied econometrics\ncourse with an emphasis on regression techniques, instrumental variables and the\nuse of fixed effects.\n\nThe statistics courses are very much geared towards traditional socio-economic\nresearch where a hypothesis is formed (usually the difference between two groups\nnot being zero), data is gather (via survey techniques) and statistical tests\nare applied on the difference between two groups (usually with the use of\n$t$-tests).\n\nFor most students, especially applied statistics feel as a very mechinal\nprocedure using a pre-defined set or recipes. @mcelreath2016statistical\nintroduced a nice analogy with the old folkore of the Golem of Prague. The\nGolem was a mindless robot of clay that obeys orders. Scientists also use\ngolems, especially with statistical procedures, where the tests or the estimations one\nperforms are small golems in themselves. A mindless procedure that obeys what you\ntell them do. Sometimes for the better, sometimes for the worse. \n\nFor students this is not completely unlike: if you have this procedure, with\nthese data, you should use this test---if not, use that test. Why to use that test is\nnot an issue, one just follows a particular scheme and deploys one's own golem.\n@fig-flowchart gives a typical example of such scheme or flowchart\nfor the usage of statistical tests.\n\n![An example of a flowchart for statistical tests.](flowchart.jpg){#fig-flowchart}\n \nWhat is problematic with this approach is that students never completely understand what\nthey are doing. Throughout their bachelor (and master) years, the relation between test\nstatistics, $p$-values, significance level and confidence levels is typically\nlost on them.\n\nFor a large part, confusion amongst students is caused by the fact that\n(classical) statistics at the bachelor level is in way rather counter intuitive.\nTake, e.g., the following two statements about the 95% confidence interval.\n\n> A 95% confidence interval means that for a given realized interval there is a 95% probability that the population parameter lies within the interval.\n\n> With numerous repeated samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 95%.\n\nMost students---in fact the audience at large and most scholars as well---would\nchoose the first statement as being true for the 95% confidence interval. But\nin fact, the first statement is wrong and the second is true. The confidence\ninterval is only formed by the (often implicit) assumption of numerous (infite)\nsampling. It does not resemble a statement about a probability of the population\nparameter even though most us feel intuitively that that **should** be the\ncase.\n\nThese concepts of sampling and the associated confusion unfortunately,\ncarry directly over to the applied econometrics domain. However, usually\nstudents find applied econometrics easier as less emphasis is put on the\nstatistical background of the estimators. Unfortunately, applied econometrics\nonly comes back in non-methods courses in the master phase, less so in the bachelor years,\neven though concepts as regression is taught in the first bachelor. This\ntypically leaves bachelor students with a small amount of experience and less to none\nintuition when it comes to applied (econometric) work with empirical datasets.   \n\nas I will argue in the next section there are other ways of teaching students\nconcepts of statistics and probabilities which rely less on sampling and more on\ncounting instead. However, for this, computers and statistical\nsoftware packages are needed, but then at least we can make statement as the\nfirst one above, which feel far more intuitive. \n\n#### Data science\n\nIn addition to statistics and applied econometrics, students are now offered a\n(data science) programming language as well in the\nbachelor, mostly R of Python. They usually only learn the basics and typically\ndo not work with databases, datascience of modeling techniques in these type of\ncourses. And, unfortunately, subsequent bachelor courses do not use these programming\nlanguages for their exercises. This renders the added value of these courses\nquickly to zero.\n\nMaster courses now use more and more data science techniques and languages,\nalthough---in all honesty---typically outside the domain of (regional)\neconomics. Unfortunately, without a solid background in dealing with data\nmanagement and newer and more applied concepts of statistics, students approach these forms of\ntechniques (e.g., classification and regression trees) again as mechanical\ngolems by following recipes without truly understanding the underlying theory.  \n\n\n## Incorporating the data science culture agenda {#sec-agenda}\n\nThe previous section discussed contemporary and cutting-edge applied econometric methods of (regional)\neconomists. As argued, these methods have merits. Not least because they are all\ngeared towards identifying **causal** relationships, and to a far greater\nextent then in other social sciences.\n\nHowever, these methods do come at some costs. First of all, the results should\nbe interpreted as **marginal** effects. A small change in $x$ causes a\ncertain change in $y$. Second, the effect is always **ceteris paribus**.\nAll possible other factors are controlled for. Third, most of these methods face\ndifficulties with **heterogeneous** effects. Fourth, and final, the underlying\nstatistical framework is often difficult to interpret---for students, scholars,\nand the audience at large.\n\nThese disadvantages do have serious consequences for what this traditional\ntoolkit can and what it cannot. First of all, it is very\ngood in explaining but very bad in predicting. Second, system-wide changes and\nimpacts are difficult to incorporate. Third, it has difficulties with different\nheterogeneous subgroups. Fourth, the underlying statistical framework makes it\ndifficult to evaluate models. And, as last, the statistical framework also make\nit difficult to deal with non-linear models and non-parametric techniques are\ndifficult to yield with this specification.\n\nBelow, I first explain how using techniques from the data driven approach side, or the\ndata science side, can help research in the field of regional economics further in three\ndirections: model comparison, heterogeneous effect sizes, and predicting.\n\nThereafter, I describe what needs to be changed in education, so that future\nstudents will better enabled to deal with the abundance of larger datasets, the\nneed for better predictions and a more intuitive understanding of probabilities\nand testing.\n\n### In research\n\n#### Dealing with heterogeneity\n\nOne of the weaknesses of the theory driven approach---or the more classical\nresearch methods---is dealing with heterogeneity. Fixed effects regressions\nonly deal with removing level effects and not varying slope effects,\ndifference-in-difference designs only give average treatment effects and\ninstrumental variables have difficulties with incorporating\nheterogeneity.[There are some advances made in introducing\n  heterogeneous instruments in quantile regression techniques, but the exact\n  mechanism is still not clear-cut.]{.aside}\n\nThe argument made against heterogeneity is that it only affects efficiency\n(i.e., the standard errors), but in most cases this is not true. In non-linear\nmodels, such as discrete choice and duration models heterogeneity affects the\nconsistency (i.a., the parameter of interest) as well. Moreover, interpretation of\nthe parameter of interest might be completely off when not allowing for\nheterogeneous groups. \n\n![A mixture of two distributions of housing prices](featured.png){#fig-mixture}\n\nConsider @fig-mixture where in the left panel a density\ndistribution is given from a distribution of lognormally distributed housing\nprices. When interested in explaining the effect of a variable $x$ on housing prices\nthis is typically the first descriptive plot an applied researcher creates. The\nmiddle panel enlights this plot further by combining both a density plot and a\nhistogram. But what if the sample consists of \\emph{two} types of housing\nmarkets. One overheated and one with ample housing supply. Then most likely the\nmechanism on both markets are different and the effect $\\beta$ could be very\nwell different for both markets. Indeed, the right panel shows that the density\ndistribution from the left panel is actually a realization of a mixture of two\n(in this case normal) distributions. The housing market with ample supply of\nhouses is then represented by group 1 and the overheated housing market is\nrepresented by group 2.\n\nThese latent class approaches are typically not much applied in (regional)\neconomics [see for an exception, e.g., @lankhuizen2015].[In other\neconomic or social science fields, such as market and transportation science,\nhowever, this is already a common approach.]{.aside} However, correct identification of\nsubmarkets or subgroups could be very important for policy makers as the average\ntreatment effect may very well not even apply to anyone [an argument, in\nDutch, made as well in @DeGraaff2014misc].  \n\nSlowly, the notation that fixed effects contain much useful information\npermeated in the regional economics domain. An insightful and straightforward\nway to do this is by adapting the wage model in @eq-fe as follows:\n\n$$\n\\begin{align}\n  \\ln(w_{ic}) &= \\xi_c + \\beta \\ln(d_{ic})+\\mathbf{z_{ic}}\\gamma + \\epsilon_{ic} \\notag \\\\\n  \\xi_c&=\\alpha + \\mathbf{x_c}\\delta + \\mu_c,\n\\end{align}\n$${#eq-fe2}\n\nwhere the individual wage model is now split up in two stages. The first stage\nmodel the individual variation and regional fixed effects. The second stage now\nregresses regional variables on the estimated fixed effects. This approach is\nnow frequently applied [for example, in the so-called sorting model @Bayer2004; @Bayer2007a; @Wang2016; and\n  @bernasco2017social]. \n\nTwo large advantages of this approach are that the standard errors on both the\nindividual and the regional level are correctly estimated and that, if needed,\ninstrumental variable methods may be applied in the second stage. There is one\ndisadvantage and that is the fixed effects in the second stage are not observed\nbut estimated (imputed) and that has an effect on the standard errors.\n\nNote that model @eq-fe2 is very much alike multilevel models, which are\nvery often used both in the data driven approach and in other social science\napart from economics. Multilevel modeling works great in both correctly\nestimating a model with observations on various levels (such as individuals, firms,\nsectors and regions) and in retrieving heterogeneous estimates (both in levels\nand in slopes). And with the increasing advent of micro-data,\ncombining a individual-regional model as in @eq-fe2 with the more\nrigorous structure of multilevel modeling is definitely worth more attention in\nthe near future. \n\nInterestingly, more (spatial) non-parametric approaches [see, e.g., the\ngeograpically weighted regression exercise in @Thissen2016] have become more\npopular as well in the last decade (typically, because of increased computer\npower). This approach needs more attention as well, as the connection with\n(economic) theory is often lost. And, especially regional geographers apply\nspatial non-parametric techniques, not the regional economists. \n\n#### Model comparison\n\nOne element that is notoriously weak in the theory-driven approach is model\ncomparison---or the models should be nested. And in many cases, model comparison\nis often very much asked by policy makers and the audience at large, if not only\nfor finding the correct specification. The latter question is concerned with the\nquestion which variables (predictors) to include in a model and which predictors\nof them perform best. Note that this is analogous to questions regional policy makers\nmight have: such as, which policy instruments best to deploy given limited\nfinancial budgets.\n\nA typical example can be found in the field of spatial econometrics model where comparison is an important\nissue as typically there are several competing theories, non-nested, for the\ndistance decay function (usually measured with a so-called spatial weight matrix\n$\\mathbf{W}$). And usually those theories are very much related (e.g., distance\ndecay measured in Eucledian distance or generalized travel costs).\n\nAnother field where model comparison is of large importance is in the estimation\nof the strength of socio-ecoomic networks. In theory, socio-economic networks\nshould produce so-called power-laws: or a loglinear relation between the size of\nnodes and the number of connections.[One of the most famous of these\n  relations is Zipf's law, where the ordering of cities and the size of the\n  population follows an almost perfect loglinear distribution; see for an\n  in-depth treatment @gabaix1999zipf.]{.aside} Empirically, these relations often follow a\nslightly different distribution. What kind of distribution fits then best is\nstill a matter of debate.  \n\nFor proper model comparison, a Bayesian approach is almost unavoidable. The key\ndifference between the frequentist and the Bayesian approach is how to interpret\nuncertainty. In the frequentist approach uncertainty originates from sampling,\nwhile in the Bayesian approach uncertainty is caused by not having enough\ninformation. So, a Bayesian statistician lives in a deterministic world but has\na limited observational view.[Both frequentists and bayesians rely\n  heavily on sampling. However, sampling in frequentist statistics is a device to construct undercertainty\naround an estimate. Sampling in Bayesian statistics is a way to perform integral\ncalculus (or to simulate observations).]{.aside}. Note that the rule of Bayes is not\nunique for Bayesian statistics. Namely, this rule is central for all probability\ntheory.\n\nWhat is unique for each Bayesian model is that it has a prior and posterior. The\nprior is an assumption about something that you do not know (uncertainty\nmeasured by a parameter). With additional information (data), knowledge about\nthe uncertainty is then updated (and hopefully the uncertainty is diminished).\nThe updated probabilities are represented in a posterior distribution. To\nunderstand the probabilities then is simply a matter of sampling from the\nposterior distribution. So, the\nfrequentist approach typically give a point estimate of a parameter, the Bayesian approach\ngives the whole distribution of the parameter. Note that under the Bayesian\nparadigm, everything (including the data) is regarded as an variable with\nassociated uncertainty.\n\n$$\n\\begin{align}\n  \\ln(h_r) & \\sim \\text{Normal}(\\mu_r, \\sigma) \\tag{likelihood}\\\\\n  \\mu_r & = \\alpha + \\beta x_r \\tag{linear model}\\\\\n  \\alpha & \\sim \\text{Normal}(12,3) \\tag{$\\alpha$ prior}\\\\\n  \\beta & \\sim \\text{Normal}(5,10) \\tag{$\\beta$ prior}\\\\\n  \\sigma &\\sim \\text{Uniform}(0,2) \\tag{$\\sigma$ prior} \n\\end{align}\n$${#eq-bayes}\n\n@eq-bayes gives an example of a Bayesian linear regression\nmodel.[Under relatively mild assumptions this should yield similar\n  results as the frequentist approach.]{.aside}. Here, we want to model the relation\nbetween regional housing prices ($h_r$) and the regional percentage of open\nspace ($o_c$). Note that all parameters and the distribution of the data\n(likelihood) require distributional assumptions. This is a disadvantage in\nrelation to the inference based frequentist approach, where no distributional\nassumptions are needed. But, note as well that @eq-bayes specifies\n**explicitly** all assumptions for this model (e.g., a linear model and a\nnormal distribution for the likelihood). If you think the model is incorrect you\ncan rather easily change the assumptions.\n\nEstimating Bayesian models have always been computationally cumbersome, especially\nwith more parameters as sampling from the posterior distribution equalizes\nsampling from a multi-dimensional integral. Fortunately, computational power has\nincreased dramatically in the last decades and techniques for sampling from the\nposterior distribution have become rather efficient (the most often used\ntechniques nowadays are Monte Carlo Markov Chain algorithms which is basically a\nsimulation of the posterior distribution).\n\nAlthough Bayesian statistics has already been applied to spatial\neconometrics [see the excellent textbook\nof @lesage2009introduction], applications have not permeated much to other fields\nin regional economics, such as in regional growth estimations,\nindividual-regional multi-level modeling and population-employment modeling.  \n\n#### Predicting\n\nA last field not well developed in (regional) economics is that of predicting.\nMost economists[In popular media, though, they are less hesitant to\n  offer predictions.]{.aside} would shy away from predictions as, in their opinion, identifying\ncausal relations is already difficult enough (they have a point there). What\neconomists love to do is giving counterfactuals instead. For example, if\nregional open space would decrease significantly, what would happens with\nregional housing prices. This counterfactual approach looks very much as a\nprediction, however there are two large disadvantages associated with\ncounterfactuals.\n\nFirst, counterfactual are always made **in sample**. Actually, all marginal\neffects are made **in-sample**. Splitting the sample in a training set and a\ntest set is not something that (regional) economists are prone to do. There is\nan intrinsic worry then for \\emph{overfitting}, especially when using many fixed\neffects. Explanatory power may be very high, but could also be very situation\nrelated. What works in one region, does not necessarily works in another region.\nNote that predicting in spatial settings is more difficult as the unit of\nanalysis is typically a spatial system. And sub-setting a spatial system in a\ntraining and test set is often difficult.\n\nConsider, e.g., the following often used gravity model in linear form as\ndepicted in @eq-gravity: \n\n$$\n  \\ln(c_{ij}) = \\alpha + \\beta \\ln(P_i) + \\gamma \\ln(E_j) + \\ln(d_{ij}) + \\epsilon_{ij}.\n$${#eq-gravity}\n\nHere, we aim to model the number of commuters ($c$) from region $i$ to region\n$j$, by looking at the total labor force $P_i$ in region $i$, the total number of\njobs $E_j$ in region $j$ and the distance ($d_{ij}$) between the two regions.\nSuppose, we can improve the distance between region $i$ and $j$ by, e.g.,\nenlarging highway capacity. This does not only change the commuter flow between\n$i$ and $j$, but also between other region; say between $i$ and $k$. As usual\nthere is no free lunch and total employment and population in each should remain\nconstant, at least in the short-run.\n\nHowever, this make sub-setting difficult and correctly predicting cumbersome.\nBut, @eq-gravity of above is just one example of a large class of\nmodels that all face this difficulty. And policy makers (and firms) are actually\nvery much interested in the questions associated with these predictions.\nQuestions related to the impact of Brexit on other countries, total network\neffects of infrastructure improvements, identifying profitable routes for\nairlines, impact of housing projects on commuting quickly come to mind. So, it\nis especially the relation between predicting and spatial (interaction) systems\nthat need considerable attention.\n\nA second problem with the counterfactual approach is that it considers marginal\nchanges. Unfortunately, in models as @eq-gravity this would not work. A\nmarginal change on the link between $i$ and $j$ would have marginal changes on\nmost other links. Marginal changes in a network setting is still a relatively\nunderdeveloped area.[That is, in applied empirical statistical work.\n  Computational equilibrium models and to a lesser extent input-output models\n  are able to model network-wide impacts. However, these models are cumbersome\n  to construct and are less based on data.]{.aside}\n\nSo, on of the main research challenges in the regional economic domain for the near future would be to combine\nthe data science models with the concept of spatial interaction models in such a\nway that both predictions can be made and that model restrictions are still\nsatisfied. \n\n### In education\n\nAs discussed above, in regional economics---in fact, in all social sciences---introductory\nstatistics courses are like cook books. In this situation you need that recipe\n(test), in that situation you need that recipe (test). These recipes even\nperfectly coincide with the drop-down menu from certain, grapically user\ninterface driven, statistical software packages such as SPSS. This causes\nstudents not to understand the underlying mechanism but just to apply procedures\n(or actually push buttons).  \n\nWithout going into the need for using a frequentist or a Bayesian approach (they\ncoincide more than most people think), I would actually argue very much for\nalready using computers and coding in an early phase in students' education.\nThis could coincide with more traditional probability theory, but has an\nadvantage that it is a general approach instead of a flowchart.\n\n![Overlapping normal distributions](normal.png){#fig-normal}\n\nAs an example, consider @fig-normal where two normal distributions\nare depicted. The left one has a mean of $-1$, the right one has a mean of $1$.\nBoth have a standard deviation of 1. The question is now to what extent these\ndistributions are different from each other. (Slightly rephrased: this is actually the problem whether\ntwo coefficients in a frequentist framework are different from each other). One\napproach is to search for a suitable test (and quickly run in a plethora of $F$,\n$z$ and $t$-tests); so, following a flowchart again.\n\nAnother approach would actually be to **count**---well, take the integral of\n---the number of observations in the area that belongs to both distributions. By\nhand this is infeasible, but with a computer this is rather easy. Just sample a\nreasonable amount of realisations from both distributions (say $N$ times) and\ncount how many times the realisation from the second distribution is smaller\nthan the first distributions. To get a probability, divide by $N$. In R the code\nsimply boils down to:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  N = 100000          # number of draws\n  # Draw from first distribution\n  n1 <- rnorm(N, (-1), 1) \n  # Draw from second distribution\n  n2 <- rnorm(N, 1, 1) \n  # Count how many times second dist.\n  # is smaller than first dist.\n  count <- sum(n2 < n1)    \n  count/N       # get probability \n```\n:::\n\n\nAnd for those who are interested, the probability is approximately $0.079$. Note\nthat this is a full-blown probability and not so much a test. You could easily\nturn this into a test when comparing this with a pre-defined probability. If you\nfind this probability to high, then you actually have large doubts whether these two\ncoefficients are different.[Note that I refrain from using the term\n  significance level. This concept is truly a frequentists' concept and on a\n  fundamental level relies on the concept of infinite sampling to get uncertainty.]{.aside} \n\nAlthough this approach is definitely intuitive and arguable very powerful (if\nyou understand the approach above, you basically understand Bayesian statistics\nas well) it does require computer literate skills from students. And contrary to\npopular belief, most students actually face large difficulties with coding,\ncommand line tools, working with file systems, and so on. This is caused by the\nfact that all tools they usually work with[Typically they work with Powerpoint, Excel\n  and Word.]{.aside} are driven by drop-down menu's,\ntemplates and strong graphical user interfaces.  \n\nThis is also caused by the fact that in regional economics (actually in al the\nsocial sciences), remarkably little attention has been given to the set of\ncomputer-related tools students could use, why they should use them and the\nrelation between them [with some exceptions as, amongst some others, by @Rey:2014cl; @arribas2015woow; @Arribas2016].\nThis is even more remarkable as reproducibility and robustness of results become\nmore important in research and teaching. \n\nAnd this does not not apply for statistical software tools such as R or Python,\nbut as well to other fields. Gathering data, manipulating data, visualising data\nand communing results are all skills that arguably are very important for\nstudents and scientist and become even more important in the future\n[@varian2014big]. There are some exceptions as\n@schwabish2014economist, but in (regional) economics these skill still\nreceive not much attention---in research, but especially in education.\n\nI conclude this section by arguing that we miss three main elements in our\ncurriculum. The first on being a larger emphasis on computer literature skills, such as coding, command\nline tools, visualization of data, an so forth. The second is more room for the\ndata driven approach, where using software packages such as R of Python,\nproblems are solved with data science techniques, with its larger emphasis on\npredicting and non-linear modeling. To be clear, simulation exercises as above\nfunctions as well as a data driven approach. Most importantly, students should\nunderstand the underlying mechanism, instead of applying procedures. The third\nand final element that is missing is consistently throughout the curriculum.\nThis is often understood as applying the same tools for each course, but this is\nnot necessarily the case. What I mean with consistently is that elements from\nmethod courses should come back in regular courses. Nowadays, most courses could\nimplement an empirical element, such as regression techniques, data\nvisualization, data manipulation, and perhaps coding as well. Why otherwise give\na Python in the first year of the bachelor, without using that in other courses?\n\n## Into the abyss\n\nI started this paper with the observation, that, in the words of\n@breiman2001statistical, there seems so be two cultures in statistical or\neconometric modeling; a theory driven and a data driven approach. These two\napproaches are not mutually exclusive, but complementary. And both have their\nown strengths and weaknesses. However, especially in economics---and thus in\nregional economics as well---the theory driven approach still seems to be highly\ndominant, even with the advent of increasingly larger (micro-)databases.\nArguably, this is problematic as the theory driven approach has difficulties when answering questions typically\nasked by policy makers; questions such as *What works best for my region?*,\n*What happens with the capacity of my whole network when I invest in a specific\nhighway link?* and *In which region should I invest to get highest returns?*.\n\nSo, the main argument of this paper lies in introducing more data approach/data\nscience techniques in the toolkit of the regional economist. Other related fields, even\nin the social sciences, have already made large advances, such as predictive\npolicing in criminology, latent class approaches in transportation modeling, and\nthe use of deep learning techniques in marketing sciences. \n\nObviously, this needs large investments (mostly in time), both for researchers\nand for teachers. The first group needs to invest in new techniques and probably\nin new statistical software. The second group needs to change parts of the\ncurriculum in terms of the specific contents of methods courses and exercises.\nFortunately, many online and open source manuals, videos and even textbooks are\navailable.[Especially for the R programming environment @R2017 there is a vast amount of\n  material available on the internet, such as [R for Data Science](http://r4ds.had.co.nz/)\n  and [Efficient R programming](https://csgillespie.github.io/efficientR/).]{.aside}\nMoreover, companies such as DataCamp allow for free subscriptions as long as the\nmaterial is used for classes.} \n\nTo conclude, I would like to note that apart from the intrinsic scientific\narguments there are two other very compelling arguments to invest at least some\ntime in data driven approaches. First, it coincides wonderfully with other\ntechniques, such as versioning, blogging (publishing to HTML), and command line\ntools. All these approaches ensure that research becomes more reproducible.\nSomething that becomes more and more a hard requirement by both university and\nthe audience at large. Second, when looking at recent advances both in industry\n(e.g., all the dotcom companies but also others, such as more traditional media\ncompanies) and in other scientific disciplines, it is not the question **if** regional economists\nshould invest more in the data science approach, but the question **how soon**\ncan we start.  \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}