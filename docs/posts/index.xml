<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Thomas de Graaff</title>
<link>http://thomasdegraaff.nl/posts/</link>
<atom:link href="http://thomasdegraaff.nl/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>A spatial economist employed at the department of Spatial Economics of VU University Amsterdam</description>
<generator>quarto-1.4.553</generator>
<lastBuildDate>Fri, 04 Jun 2021 22:00:00 GMT</lastBuildDate>
<item>
  <title>Do regional economists answer the right questions?</title>
  <dc:creator>Thomas de Graaff</dc:creator>
  <link>http://thomasdegraaff.nl/posts/research_agenda/</link>
  <description><![CDATA[ 

<!-- source: https://github.com/gadenbuie/garrickadenbuie-com/blob/main/_partials/title-block-link-buttons/title-block.html -->
<!-- 
<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title"> 
-->
<!-- <header id="title-block-header" class="quarto-title-block default page-columns"> -->
<!-- <div class="quarto-title page-columns page-full featured-image p-4" style="background-image: url(featured.png), url(featured.jpg), url(../featured.jpg);"> -->



<section id="introduction-two-different-cultures" class="level2 page-columns page-full" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction-two-different-cultures"><span class="header-section-number">1</span> Introduction: two different cultures</h2>
<blockquote class="blockquote">
<p>The sexiest job in the next 10 years will be statisticians. <span class="citation" data-cites="varian2014big">(Varian 2014)</span></p>
</blockquote>
<p>The quote above from Hal Varian is in one aspect wrong; nowadays, we do not call them statisticians but data scientists instead. Nevertheless, in the last two decades companies such as Google, Ebay, Whatsapp, Facebook, Booking.com and Airbnb, have not only witnessed enormous growth but to a considerable extent also changed the socio-economic landscape. Indeed, with the increasing abundance of (spatial) data and computer capacity, the ability to gather, process, and visualize data has become highly important and therefore highly in demand as well. And all the models and tools these data scientists within these companies use are very much <em>data driven</em> with often remarkable results.</p>
<div class="page-columns page-full"><p>In his controversial and path-breaking article, <span class="citation" data-cites="breiman2001statistical">Breiman (2001)</span> presented two different cultures in statistical science. One governed by a (probability) theory-driven modeling approach and one governed by a more (algorithmic) data-driven approach. These two cultures carry over to the econometric and ultimately the empirical regional economics domain  as well, where—commonly for all social sciences—the theory driven approach still very much dominates the landscape of the realm of contemporary regional economics.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">I use a wide definition for the regional economics domain, which consists of most aspects of regional science in general but for which the theoretical approach is always from an economic perspective. Topics such as, e.g, interregional migration, trade, transport flows and commuting on the one side and regional performance, regional clustering, population growth and specialisation on the other side fall all under this, admittedly, rather wide umbrella.</span></div></div>
<div id="fig-approaches" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-approaches-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-approaches" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-model" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/modelapproach.png" class="img-fluid figure-img" data-ref-parent="fig-approaches">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Model approach
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-approaches" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-data" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/dataapproach.png" class="img-fluid figure-img" data-ref-parent="fig-approaches">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Data approach
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-approaches-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Two cultures of statistical/econometric modeling <span class="citation" data-cites="breiman2001statistical">(Insipired by Breiman 2001)</span>
</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>Figure&nbsp;2 is an adaptation from the one displayed in <span class="citation" data-cites="breiman2001statistical">Breiman (2001)</span> and describes the processes governing these two cultures. Figure&nbsp;2 (a) is what I refer to as the modeling approach, where a statistical model is postulated and is central to this culture. This is the classical approach  where statistical probability theory meets the empiricism of Karl Popper. Usually the model assumed is stated as a linear model and in its most simple form can be denoted as:</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Sometimes as well referred to as the frequentists’ approach. However, this typically concerns the debate between classical statistics and Bayesian statistics, where the two approaches I refer to are more concerned with wider frameworks, of which the Bayesian approach is just one of the elements. I come back to Bayesian statistics and the frequentists’ approach later, but I do not see them necessarily as opposites. And I quite object to the term frequentists’ approach as Bayesian statistics is much more focuses on counting then the frequentists’ approach.</span><span class="margin-aside">To be honest, Popper himself was not a great fan of simply null hypothesis testing. He actually argued for the falsification of explanatory models, where in his view falsification does not only rely on statistics but on consensus amongst scientists as well.</span></div></div>
<div class="page-columns page-full"><p><span id="eq-linreg"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathbf%7By%7D%20=%20%5Cmathbf%7Bx%7D%5Cbeta%20+%20%5Cepsilon,%0A%5Ctag%7B1%7D"></span> where in (regional) economics language, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> is referred to as the independent variable, <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> as the dependent variable and <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> as a residual term. In this setup, using the data at hand, one constructs a statistical test to which extent the estimated coefficient (denoted with <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D">) deviates from a hypothesized value of the coefficient (denoted with <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0">)—typically the hypothesis <img src="https://latex.codecogs.com/png.latex?H_0:%20%5Chat%7B%5Cbeta%7D%20=%200"> is used with as alternative hypothesis that <img src="https://latex.codecogs.com/png.latex?H_1:%20%5Chat%7B%5Cbeta%7D%20%5Cneq%200">. However, that is always within the context of the model. So, when the null-hypothesis is rejected, it not necessarily means that the true <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is unequal to zero, it might also be caused by errors in measuring <img src="https://latex.codecogs.com/png.latex?%5Cbf%7Bx%7D"> or even using the wrong ![One of the assumptions for regression techniques such as the one used here is actually no misspecification of the model, but—apart from some possible tests on the functional form a specific regression form—usually little attention is give on the validity of the model used. More importantly, within this framework the model itself is usually not tested .]</p><div class="no-row-height column-margin column-container"><span class="margin-aside">There is another fallacy with this approach that is often overlooked and that is that the alternative hypothesis being true is a probability as well. Namely, most hypotheses researchers test are typically not very probable. Not taken this into account would actually lead to more null hypotheses to be rejected then should be (false positives).</span></div></div>
<p>Figure&nbsp;2 (b) yields a schematic overview of a more data driven approach. Here, we see an unknown model fed by predictors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> that lead to one or multiple reponses <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">. The main objective here is not to test hypotheses, but to find the best model instead which able to explain the data and to predict the data. Usually, the models are evaluated by some kind of criterion (e.g., the mean squared error), which is not completely unlike the modeling approach. However, there are two main differences between the two approaches. First, the data driven approach considers several models in a structural approach. For instance, the question which variables to include is captured by an exhaustive sourse of all combinations in the modeling approach (e.g., with classification and regression trees or random forests), while in the theory driven approach, the choice of variables is based on the theory and a small number of variations in the specification. Second, measurements on model performance are done in the data driven approach and, typically, in the model approach. The latter is not that important for hypothesis testing, but for prediction this matters enormously, because adding parameters might increase the in-sample fit, but actually worsen the out-of-sample fit (a phenomenon called overfitting).</p>
<div class="page-columns page-full"><p>In economics in general, and in regional economics in specific, most of the tools employed are very much <strong>theory or model driven</strong> instead of data driven. My (conservative) estimate would be that at least 90% of all empirical work in regional economics revolves around postulating a (linear) model and testing whether (a) key determinant(s) is (are) significantly different from a hypothesized value—usually zero. That is, the context of the model assumed.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">In a seminal contribution, <span class="citation" data-cites="breiman2001statistical">Breiman (2001)</span> states that deep into the 90s 98% of the statisticians actually employed the theory driven paradigm and only 2% a data driven paradigm. With the advent of the availability of internet connectivity, large (online) data sources, and faster computers the statistical realm changed dramatically. However, this has not permeated yet in the social sciences <span class="citation" data-cites="varian2014big">(see as well Varian 2014)</span>.</span></div></div>
<div class="page-columns page-full"><p>At best, this approach can be seen in a causal inference framework. If a determinant (such as a policy in the context of regional economics) <img src="https://latex.codecogs.com/png.latex?x"> changes, does it cause then a change in the output <img src="https://latex.codecogs.com/png.latex?y"> (most economists typically use some welfare measure). This approach thus provides a rigid and useful approach to regional policy evaluation. If we implement policy <img src="https://latex.codecogs.com/png.latex?x">, does welfare measure <img src="https://latex.codecogs.com/png.latex?y"> then improve? Note that this always considers a <strong>marginal</strong> change as <img src="https://latex.codecogs.com/png.latex?x"> is usually isolated from other (confounding) factors.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Most of this research actually intends to mimic a <strong>difference-in-difference</strong> approach and gained enormous momentum with the textbook of <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (2008)</span>.</span></div></div>
<p>However, policy makers oftentimes have different questions for which they need solutions. Usually, they revolve around questions starting with <em>What determines performance measure <img src="https://latex.codecogs.com/png.latex?A">?</em>, <em>Which regions can we best invest in?</em> or, more generally, <em>What works for my region?</em>. These types of questions require a different approach than the previous one. Namely, the former type requires an approach focused on <strong>explaining</strong> while the latter type requires an approach focused on <strong>predicting</strong>.</p>
<p>The remaining part of this position paper is structured as follows. Section&nbsp;2 gives an overview of current modeling practices and describes the `traditional’ inference based approach as well as some data-driven approaches that have been used in the recent past (though by far not as often as the traditional methods). Section&nbsp;3 sets out both a research and an education agenda as it addresses how to bridge the gap between the daily practices of regional economists and the demands of local policy makers. The final section shortly summarizes the main points raised in this position paper.</p>
</section>
<section id="sec-practices" class="level2 page-columns page-full" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-practices"><span class="header-section-number">2</span> Regional economists turning the blind eye</h2>
<p>Unmistakably, in the recent decade the two major changes to economic empirical research in general are the advent of increasingly larger data sources and the large increase in computer power <span class="citation" data-cites="einav2014economics">(Einav and Levin 2014)</span>. The methods that most economists employ, however, have not changed. Linear regression or one of its close relatives (such as logistic, poisson or negative binomial regression), preferably in a causal framework, is still the most common tool. This also applies to regional economists, who—although coming from a tradition to use various methods from different disciplines—have increasingly used similar methods as in ``mainstream’’ economics.</p>
<p>This focus on marginal effects and causality is certainly very worthwhile and brought us many important insights. However, it is also typically done within a very narrow framework and, below, I will lay out what we are missing both in research and in our educational curricula, when our focus is on the framework above and as advocated so much as in <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (2008)</span>.</p>
<section id="the-blind-eye-in-research" class="level3 page-columns page-full" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="the-blind-eye-in-research"><span class="header-section-number">2.1</span> The blind eye in research</h3>
<p>The traditional model of a (regional) economist looks as follows: <span id="eq-model_economist"><img src="https://latex.codecogs.com/png.latex?%0A%20%20y_i%20=%20%5Calpha%20+%20%5Cbeta%20x_i%20+%20%5Cmathbf%7Bz%7D_i%5Cgamma%20+%20%5Cepsilon_i,%0A%5Ctag%7B2%7D"></span> where <img src="https://latex.codecogs.com/png.latex?y_i"> is referred to as the dependent variable, <img src="https://latex.codecogs.com/png.latex?x_i"> is the main variable of interest, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bz%7D"> is a vector of other variables. <img src="https://latex.codecogs.com/png.latex?%5Calpha">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> and <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> are parameters, where we are especially interested in the value of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Finally, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i"> is an identical and independent distributed error term.</p>
<p>Usually the main aim is to estimate <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> as unbiased as possible in a causal framework. So, ideally, we would like to control for unobserved heterogeneity bias, specification bias, measurement error, reverse causality, selection bias, and so forth. Econometric theory has produced some very powerful techniques to control for some of these biases, such as instrumental variables, diff-in-diff procedures and the use of fixed effects. However, these methods are not panacea for everything. First, they work wonders for only specific research questions that have to do with the preferably causal effects of <strong>marginal</strong> changes. Second, some of these techniques require very specific and strong assumptions which are possibly not always met, which leaves doubts upon the validity of the results.</p>
<p>Below, I will deal with instrumental variables, diff-in-diff and fixed effect techniques consecutively. I will specifically focus on some of the disadvantages. Some of the arguments are adaptions from <span class="citation" data-cites="deaton2010instruments">Deaton (2010)</span> and I refer to this reference for a more complete treatise on the disadvantages of using instrumental variables and diff-in-diff methods. For all the advantages not dealt with in this paper, read <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (2008)</span>.</p>
<section id="exogeneity-versus-independence" class="level4 page-columns page-full" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="exogeneity-versus-independence"><span class="header-section-number">2.1.1</span> Exogeneity versus independence</h4>
<p>Economists love instrumental variables, because a good instrumental variable can tackle reverse causality, measurement error and unobserved heterogeneity bias all at one. Originally, instrumental variables come from simultaneous economic models such as supply and demand models. A classical example in a regional context would be: <span id="eq-PE"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%20%20P_r%20&amp;=%20%20%5Calpha%20+%20%5Cbeta%20E_r%20+%20%5Cmathbf%7Bz%7D_r%5Cgamma%20+%20%5Cepsilon_r,%20%5Clabel%7BP%7D%5C%5C%0A%20%20E_r%20&amp;=%20%20%5Cdelta%20+%20%5Ckappa%20P_r+%20%5Cmathbf%7Bw%7D_r%5Clambda%20+%20%5Cnu_r,%5Clabel%7BE%7D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span> where <img src="https://latex.codecogs.com/png.latex?P"> denotes population, <img src="https://latex.codecogs.com/png.latex?E"> employment and <img src="https://latex.codecogs.com/png.latex?z"> and <img src="https://latex.codecogs.com/png.latex?w"> are vector of other regional <img src="https://latex.codecogs.com/png.latex?r"> characteristics. <img src="https://latex.codecogs.com/png.latex?%5Calpha">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, <img src="https://latex.codecogs.com/png.latex?%5Cgamma">, <img src="https://latex.codecogs.com/png.latex?%5Cdelta">, <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> are parameters to be estimated.</p>
<div class="page-columns page-full"><p>Obviously, one can not directly estimate Equation&nbsp;3 because of the intrinsic simultaneity. However, suppose one is interested in estimating the impact of employment on population growth, then one can use the second equation of Equation&nbsp;3 and search for <strong>exogeneous</strong> variation in employment to use it as an instrumental variable. A possible strategy could be to look into the population changes of surrounding regions (but within commuting distance), as they might not have an impact of the population change in the current region <span class="citation" data-cites="DeGraaff2012 Graaff2012">(see Graaff, Oort, and Florax 2012a, 2012b)</span></p><div class="no-row-height column-margin column-container"><span class="margin-aside">This is not really precise; I mean exogeneous to population variation. I will come back to the use of exogeneous later.</span></div></div>
<div class="page-columns page-full"><p>The main point , however, is that equations Equation&nbsp;3 constitute a full-blown economic which has direct relations with underlying structural theoretical modeling frameworks [such as <span class="citation" data-cites="roback1982wages">Roback (1982)</span>. And the instrument then comes directly (is internal) from the model.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Directly from <span class="citation" data-cites="deaton2010instruments">Deaton (2010)</span>.</span></div></div>
<p>In practice, however, researchers often take another approach. And that is to look for external instruments. Instruments that have no relation with a structural (simultaneity) model. And there are is (a large) potential pitfall when doing so and that is to end up with an instrumental variables that is not independent from the left-hand-side variable. As it seems, there is some confusion about terms as independence and exogeneity, so let’s first clarify the exact assumptions a valid instrument should satisfy.</p>
<p>Suppose that somebody is interested in the impact of population on employment; so, one would like to identify <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> in Equation&nbsp;3. To control for endogeneity researchers then search for an <strong>exogenous</strong> and <strong>relevant</strong> instrument, <img src="https://latex.codecogs.com/png.latex?Z_r">. The latter indicates that the instrument has an impact on the possible endogeneous variable (<img src="https://latex.codecogs.com/png.latex?P_r">) and the former indicates that the instrument does not affect the left-hand-side variable (<img src="https://latex.codecogs.com/png.latex?E_r">), only via <img src="https://latex.codecogs.com/png.latex?P_r"> and other instruments. In formal notation: <img src="https://latex.codecogs.com/png.latex?E_r%20%5Cperp%20Z_r%7CP_r,%20w_r">. Thus, exogeneity means that the instrument and the left-hand-side variables are <strong>independent</strong> from each other conditional on the instruments.</p>
<p>Unfortunately, exogeneity is often used as an argument for variables that are external to the system, denote a sudden shock or for phenomena that are considered to be exogenous in other fields (such as in geography). And this usually leads to instruments that do not satisfy the independence assumption. I will give three examples below.</p>
<p>First, and very often use is the concept of deep lagging. So, in our case, we look for regional population say 100 years ago and use that as an instrument. It must be exogenous because we cannot change it, right? Well, it is definitely relevant, as regional population is remarkably resilient. Where people lived 100 years ago, they most likely live today. But, if we take model Equation&nbsp;3 seriously, then the population 100 years ago, must at least have affected employment 100 years ago, and if population is resilient then most likely employment as well (and we even do not consider yearly temporal dynamics between population and employment). So, in all likeliness, employment and population 100 years are not (conditionally) independent.</p>
<p>The second type of instruments people often use are regional characteristics (preferably deeplagged as well), and specifically accessibility measures as road, railroads and canals. For a large audience the following story typically seems very plausible at first sight. At the end of the 19th century the large scale introduction of the railways enabled households to live further from home and escape the heavilty polluted inner cities where the factories remained (making use of the same railroads intersecting in city centres). Railroads thus changed the location of population and not that of employment. While this story is entirely possible, what is often overlooked is the fact that factories and thus employment changed location as well, but only 20-30 years later, and typically along the same links as opened up by railway lines. So, the railway network 140 years ago and contemporary location of employment are not (conditionally) independent.</p>
<p>A last often used category of candidate instruments is geography-related variables. In our case that could be regions of municipalities. For instance, the Netherlands witnessed for a large period intensive population location policies. This entailed that the dutch government pointed out municipalities that were allowed to grow (in terms of housing policies). Using fixed effects of these specific municipalities then as instruments sound as a viable strategy. However, this requires strict assumptions. Namely, being a specific municipality will only have an effect on employment through being designated by the Dutch government; and by nothing else.</p>
<p>Is this to say that instrumental variables is a bad technique? No, absolutely not. If the instrument is valid, this is one of the most powerful techniques in the econometric toolbox. The point made here is that good instruments are actually hard to find and that structural simultaneous models (typically, in the context of supply and demand) usually work better to find instruments than instruments that are completely external to your problem. And if you really need to use an external instrument, be very specific and open about the assumptions you need to make.</p>
</section>
<section id="local-and-average-treatment-effects" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="local-and-average-treatment-effects"><span class="header-section-number">2.1.2</span> Local and average treatment effects</h4>
<p>Two concepts which have received quite some attention recently in econometrics, but is often overlooked in applied regional and urban economics are local average treatment effects and average treatment effects. The former deals with the interpretation of instrumental variables, the latter with the (interpretation) of spatial difference-in-difference methods. Even though these methods are different, they have similar consequences for the interpretation of research findings and their underlying assumptions.</p>
<p>The <strong>Local Average Treatment Effect</strong> (LATE) deals with the underlying assumptions that have to be made so that the instrumental variable estimation actually measures what we want [see <span class="citation" data-cites="imbens1994identification">Imbens and Angrist (1994)</span>. Referring again to our example above and say that we want to instrument regional population changes with municipalities being designated by a national policy to increase local housing supply. What we then actually measure is the effect of changes in population on employment of those municipalities that have actually complied with the national policy. Municipalities that dropped out in an earlier stage are not taken into account, but municipalities who did comply but never implemented the policy are.</p>
<p>So, what is actually measured is the designation of municipalities to a policy, which might be a very interesting research question indeed, but in all likelihood does not necessarily coincide with the coefficient <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> in model Equation&nbsp;3 above. In almost all cases the LATE theorem points at a more restrictive effect (and thus interpretation) of the instrumental variable than the model sets out to estimate. Only under very strong assumptions—homogeneity of regions, perfect compliance, and so forth— the coefficient by the instrumental variable coincides with the coefficient of the structural model.</p>
<p>A different but related issue is that of the average treatment effects. Since the seminal work of <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (2008)</span> difference-in-difference methods (and all its variants) gained enormously in popularity. As well as in regional economics where spatial difference-in-difference are applied as often as possible. The idea itself is rather straightforward and originates from the search for semi-experimental randomized controlled trials (RCT’s).</p>
<p>For the regional domain, assume the following: there is one group of municipalities that implement a policy (the treatment; <img src="https://latex.codecogs.com/png.latex?T%20=%201">) and one group of municipalities that does not (<img src="https://latex.codecogs.com/png.latex?T%20=%200">). Both groups of municipalities are measured before (<img src="https://latex.codecogs.com/png.latex?t%20=%200">) and after implementation (<img src="https://latex.codecogs.com/png.latex?t=1">). Then we can rewrite model Equation&nbsp;2 as:</p>
<p><span id="eq-diff"><img src="https://latex.codecogs.com/png.latex?%0A%20%20y_r%20=%20%5Calpha%20+%20%5Cgamma_1%20T_r%20+%20%5Cgamma_2%20t_r+%20%5Cbeta%20(T_r%20%5Ctimes%20t_r)%20+%20%5Cepsilon_r,%20%20%0A%5Ctag%7B4%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y_r"> denotes a specific regional outcome variable, <img src="https://latex.codecogs.com/png.latex?T_r"> the set of regions that are treated and <img src="https://latex.codecogs.com/png.latex?t_r"> the post implementation period. In this set-up <img src="https://latex.codecogs.com/png.latex?%5Cgamma_1"> measures the average impact of the treated regions, <img src="https://latex.codecogs.com/png.latex?%5Cgamma_2"> the impact of the time period, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is our coefficient of interest; being the impact of the treatment. Note that <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> in this setting actually denotes the difference in the outcome of the treatment groups <strong>minus</strong> the difference in the outcome of the non-treated groups: hence, the name differences-in-differences.</p>
<p>The main assumption for this technique relies on the trueness of randomization of treatment across, in our case, municipalities. In reality, the concept of randomization is difficult to defend. Poor regions are more likely to receive governmental subsidies, accessibility improvement are usually implemented in dynamic and succesful regions, and so forth. To circumvent this, researchers look at borders between regions. Back to our example, we then look at individuals close to a border between two municipalities, where one municipality received a treatment and the other did not. It is then defendable that such a band around a border is relatively homogeneous in characteristics an that both regions are thus equal except for the receivement of treatment.</p>
<p>This approach has two main consequences. The most mentioned consequence is that the effect <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is a so-called mean treatment effect. Every region, firm or individual benefits (is harmed by) equally from the treatment. So, it might very well be that some regions benefit greatly from some a policy, while it is actually harmful for others. Making the treatment effect more heterogenous is difficult and requires a lot from the data as every subgroup needs its own semi-experimental randomized control trial.</p>
<p>Extending this argument to spatial difference-in-difference methods leaves us even with the assumption that the whole region should be alike the border area in term of benefitting from the policy. Or one should be satisfied with the fact that <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> only tells us something about the effect of the policy in a border area. An area most likely not very representative of the rest of the region.</p>
<p>The other consequence relates again to the compliance assumption. Regions and municipalities themselves can be argued to fit well in treatment or non-treatment groups. And if not, non-compliance should be easily detected. However, for firms and individuals, compliance to randomization of treatment is often a very harsh assumption. More and more, evidence is found that especially individuals are very resourceful to circumvent randomization, whether it by allocation to class sizes, elementary schools, or even military draft by lottery.</p>
<p>Randomized controlled trials and difference-in-difference methods are strong techniques for the applied regional economist. The point here is, however, that without very strong assumptions, findings are mean effects that only applied to a limited part of the total sample.</p>
</section>
<section id="fixed-effects-and-heterogeneity" class="level4 page-columns page-full" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="fixed-effects-and-heterogeneity"><span class="header-section-number">2.1.3</span> Fixed effects and heterogeneity</h4>
<p>An often used technique in applied econometrics is the use of fixed effects. They work brilliantly in removing unobserved heterogeneity but they come at a price which is typically overlooked. Namely, they remove valuable variation as well in both the dependent (predictor) <img src="https://latex.codecogs.com/png.latex?x"> and the independent (reponse) variable <img src="https://latex.codecogs.com/png.latex?y">.</p>
<p>Consider the following model in Equation&nbsp;5, which is at the moment a heavily researched issue in both regional and urban economics. The issue here is to what extent city density increases individual productivity.</p>
<p><span id="eq-density"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cln(w_%7Bic%7D)%20=%20%5Calpha%20+%20%5Cbeta%20%5Cln(d_%7Bic%7D)+%5Cepsilon_%7Bic%7D,%0A%5Ctag%7B5%7D"></span></p>
<div class="page-columns page-full"><p><img src="https://latex.codecogs.com/png.latex?w_%7Bic%7D"> denotes here individual wages (as a proxy for productivity) and <img src="https://latex.codecogs.com/png.latex?d_%7Bic%7D"> density of the city <img src="https://latex.codecogs.com/png.latex?c"> individual <img src="https://latex.codecogs.com/png.latex?i"> lives in. <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is our parameter of interest and because of the log-log structure <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> denotes an elasticity. Obviously, direct estimation of Equation&nbsp;5, would lead to a misleading parameter <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> if one is aiming to measure a causal effect. Namely, <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> might be influenced by other (confounding) factors than only city density. One can think of factors such as skill level of the city population, accessibility of the city, sector structure of the city and city government. Moreover, a phenomenon called sorting might occur, where more ambitious, risk-seeking and high-skilled people migrate into larger and more dynamic cities.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">I specifically do not use the term here. Namely, Equation&nbsp;5 is a perfectly fine model to measure the overall correlation (<img src="https://latex.codecogs.com/png.latex?%5Cbeta">) between city density and individual wages and mosts non-economists are perfectly fine with this model <span class="citation" data-cites="bettencourt2010unified">(see, e.g., Bettencourt and West 2010)</span>. So, whether a parameter is biased depends ultimately upon the research question.</span></div></div>
<p>To answer the question to what extent density causes wages, researchers therefore resolved to using fixed effects. A baseline model can be seen in Equation&nbsp;6.</p>
<p><span id="eq-fe"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cln(w_%7Bic%7D)%20=%20%5Cnu_i%20+%20%5Cxi_c%20+%20%5Cbeta%20%5Cln(d_%7Bic%7D)+%5Cepsilon_%7Bic%7D,%0A%5Ctag%7B6%7D"></span></p>
<div class="page-columns page-full"><p>here, <img src="https://latex.codecogs.com/png.latex?%5Cnu_i"> denotes individual <img src="https://latex.codecogs.com/png.latex?i"> specific fixed effects and <img src="https://latex.codecogs.com/png.latex?%5Cxi_c"> city <img src="https://latex.codecogs.com/png.latex?c"> specific fixed effects. So, everything that does not vary over time for individuals and cities is now controlled for. A more insightful way what exactly happens is to write Equation&nbsp;6 in changes, such as: <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cln(w_%7Bic%7D)%20=%0A%5Cbeta%20%5CDelta%5Cln(%20d_%7Bic%7D)%20+%20%5Cepsilon_%7Bic%7D">. So, our Equation&nbsp;6 now identifies the effect by looking at the impact of a change in density on a change in wages .</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Using changes (first differences) to remove fixed effects is a viable but often overlooked technique for dealing with fixed effects.</span></div></div>
<p>Multiple improvements have already been to this model including controlling for sector/task of work and migrating between cities. Including these fixed effects (and many more) has had a profound effect on the value of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Directly estimating Equation&nbsp;5 yields an elasticity of around <img src="https://latex.codecogs.com/png.latex?1.15">, while estimating a model such as Equation&nbsp;6 including many fixed effects would yield an elasticity of around <img src="https://latex.codecogs.com/png.latex?1.02-1.03">. So, there are economies of agglomeration, but they are not very large.</p>
<div id="fig-approaches" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-approaches-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-approaches" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-fe" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-fe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/fe.png" class="img-fluid figure-img" data-ref-parent="fig-approaches">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-fe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) fixed effects
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-approaches" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-hetero" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-hetero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/hetero.png" class="img-fluid figure-img" data-ref-parent="fig-approaches">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-hetero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) heterogeneity in <img src="https://latex.codecogs.com/png.latex?%5Cbeta">
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-approaches-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Heterogeneity in levels versus slopes
</figcaption>
</figure>
</div>
<p>Is this now the end of the story? Alas, it is not. At least three remarks can be made which put the above into perspective.</p>
<p>First of all, note that we need changes over time—in our case in individual wages and city density. Now, if we take the extreme example of a subgroup of individuals who do not face wage changes and cities who remain relatively of equal size, than this subgroups will not be used for determination of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Of course, not many observations will have these characteristics. Unfortunately, with more detailed data on sector structure and migration, we need individuals that move both residence and job for identification. All others are redundant. This increases the risk on what is called sample selection bias—identification is based on a specific subgroup with deviant characteristics. The point made here, is that with the use of many fixed effects, much is demanded from the data and one need always check whether the sample used for identification is not too restrictive.</p>
<p>Secondly, if there are unobserved factors that both relate to wages and density, then it is actually very likely that these unobserved factors are related to their <strong>changes</strong> as well. One particular example here is technological change, which might affect density (suburbs) and wages at the same time, and is definitely not time-invariant. If one thinks about it, most interesting socio-economic phenomena are not time-invariant, except perhaps longitude and latitude. For example, a specific argument to use fixed effects is to control for local attractivity. But what individuals and firms find attractive does change of time, especially within cities, but across cities as well. Before air-conditioning cities in Florida and Nevada were definitely not as popular as today. And malaria-rich areas such as wetlands and river banks were always avoided until recently.</p>
<p>Thirdly, the use of fixed effects is based upon the assumption that all variation is based on variation of <strong>levels</strong>. That is, each fixed effect actually denotes a very specific constant (for each individual and city in our case). However, this really requires a very homogeneous sample except in levels. For illustration, assume that there are three individuals, where individual 3 has higher wages than individual 1 and 2, because of, say, differences in skill levels (see as well Figure&nbsp;2 (a)). However, as Figure&nbsp;2 (a) clearly shows as well, apart from individual level variation, returns to density are similar for individuals 1, 2 and 3. So, each individual benefits equally from moving from a small village not a large metropolitan area. Now, assume that individuals are different with respect to the returns by living in large and denser cities. Then the impact <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> should also differ among individuals as is illustrated in Figure&nbsp;2 (b). This is not an argument to say that using fixed effects is wrong. But if the sample might be heterogenous, i.e.&nbsp;that units respond differently to different predictors, then using fixed effect might not yield a complete pictures and in some specific cases even a distorted picture.</p>
<p>Fixed effect techniques is a must have for every empirical regional economists. However, the message I would like to convey here is that it does not remove time-invariant unobserved heterogeneity (of which there is more than most researchers realise), is not very suitable for tackling heterogeneity in your main effect and might lead in some cases to sample selection bias.</p>
</section>
</section>
<section id="the-blind-eye-in-education" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="the-blind-eye-in-education"><span class="header-section-number">2.2</span> The blind eye in education</h3>
<p>So, if the main instruments of regional economists are not always applicable and we miss tools in our toolbox to tackle, e.g., heterogeneity, prediction and non-marginal changes, how do we then fare in teaching? Are the students who now graduate equipped with the right toolbox that they use as well in their later careers? And do we have a consistent curriculum using similar or complementary tools running from the bachelor to the graduate studies? These types of questions are not frequently asked, and, if at all, not very well met. Mostly because of vested interests of departments and researchers.</p>
<p>In this subsection I will, however, try to answer partly some of these questions and identify what is missing in our curriculum. I will first look at the traditional applied econometrics approach and then to the (non-existence) of other courses geared towards data science, including the use of statistical software.</p>
<section id="statistics-applied-econometrics" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="statistics-applied-econometrics"><span class="header-section-number">2.2.1</span> Statistics &amp; Applied Econometrics</h4>
<p>In contemporary economic bachelor curriculae students typically follow one applied statistics course, where some hands-on experience is offered by working with small datasets—typically in menu driven statistical software such as SPSS or STATA. In the master phase, if students start to specialise in, e.g., regional economics, students then follow one applied econometrics course with an emphasis on regression techniques, instrumental variables and the use of fixed effects.</p>
<p>The statistics courses are very much geared towards traditional socio-economic research where a hypothesis is formed (usually the difference between two groups not being zero), data is gather (via survey techniques) and statistical tests are applied on the difference between two groups (usually with the use of <img src="https://latex.codecogs.com/png.latex?t">-tests).</p>
<p>For most students, especially applied statistics feel as a very mechinal procedure using a pre-defined set or recipes. <span class="citation" data-cites="mcelreath2016statistical">McElreath (2016)</span> introduced a nice analogy with the old folkore of the Golem of Prague. The Golem was a mindless robot of clay that obeys orders. Scientists also use golems, especially with statistical procedures, where the tests or the estimations one performs are small golems in themselves. A mindless procedure that obeys what you tell them do. Sometimes for the better, sometimes for the worse.</p>
<p>For students this is not completely unlike: if you have this procedure, with these data, you should use this test—if not, use that test. Why to use that test is not an issue, one just follows a particular scheme and deploys one’s own golem. Figure&nbsp;3 gives a typical example of such scheme or flowchart for the usage of statistical tests.</p>
<div id="fig-flowchart" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/flowchart.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: An example of a flowchart for statistical tests.
</figcaption>
</figure>
</div>
<p>What is problematic with this approach is that students never completely understand what they are doing. Throughout their bachelor (and master) years, the relation between test statistics, <img src="https://latex.codecogs.com/png.latex?p">-values, significance level and confidence levels is typically lost on them.</p>
<p>For a large part, confusion amongst students is caused by the fact that (classical) statistics at the bachelor level is in way rather counter intuitive. Take, e.g., the following two statements about the 95% confidence interval.</p>
<blockquote class="blockquote">
<p>A 95% confidence interval means that for a given realized interval there is a 95% probability that the population parameter lies within the interval.</p>
</blockquote>
<blockquote class="blockquote">
<p>With numerous repeated samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 95%.</p>
</blockquote>
<p>Most students—in fact the audience at large and most scholars as well—would choose the first statement as being true for the 95% confidence interval. But in fact, the first statement is wrong and the second is true. The confidence interval is only formed by the (often implicit) assumption of numerous (infite) sampling. It does not resemble a statement about a probability of the population parameter even though most us feel intuitively that that <strong>should</strong> be the case.</p>
<p>These concepts of sampling and the associated confusion unfortunately, carry directly over to the applied econometrics domain. However, usually students find applied econometrics easier as less emphasis is put on the statistical background of the estimators. Unfortunately, applied econometrics only comes back in non-methods courses in the master phase, less so in the bachelor years, even though concepts as regression is taught in the first bachelor. This typically leaves bachelor students with a small amount of experience and less to none intuition when it comes to applied (econometric) work with empirical datasets.</p>
<p>as I will argue in the next section there are other ways of teaching students concepts of statistics and probabilities which rely less on sampling and more on counting instead. However, for this, computers and statistical software packages are needed, but then at least we can make statement as the first one above, which feel far more intuitive.</p>
</section>
<section id="data-science" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="data-science"><span class="header-section-number">2.2.2</span> Data science</h4>
<p>In addition to statistics and applied econometrics, students are now offered a (data science) programming language as well in the bachelor, mostly R of Python. They usually only learn the basics and typically do not work with databases, datascience of modeling techniques in these type of courses. And, unfortunately, subsequent bachelor courses do not use these programming languages for their exercises. This renders the added value of these courses quickly to zero.</p>
<p>Master courses now use more and more data science techniques and languages, although—in all honesty—typically outside the domain of (regional) economics. Unfortunately, without a solid background in dealing with data management and newer and more applied concepts of statistics, students approach these forms of techniques (e.g., classification and regression trees) again as mechanical golems by following recipes without truly understanding the underlying theory.</p>
</section>
</section>
</section>
<section id="sec-agenda" class="level2 page-columns page-full" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-agenda"><span class="header-section-number">3</span> Incorporating the data science culture agenda</h2>
<p>The previous section discussed contemporary and cutting-edge applied econometric methods of (regional) economists. As argued, these methods have merits. Not least because they are all geared towards identifying <strong>causal</strong> relationships, and to a far greater extent then in other social sciences.</p>
<p>However, these methods do come at some costs. First of all, the results should be interpreted as <strong>marginal</strong> effects. A small change in <img src="https://latex.codecogs.com/png.latex?x"> causes a certain change in <img src="https://latex.codecogs.com/png.latex?y">. Second, the effect is always <strong>ceteris paribus</strong>. All possible other factors are controlled for. Third, most of these methods face difficulties with <strong>heterogeneous</strong> effects. Fourth, and final, the underlying statistical framework is often difficult to interpret—for students, scholars, and the audience at large.</p>
<p>These disadvantages do have serious consequences for what this traditional toolkit can and what it cannot. First of all, it is very good in explaining but very bad in predicting. Second, system-wide changes and impacts are difficult to incorporate. Third, it has difficulties with different heterogeneous subgroups. Fourth, the underlying statistical framework makes it difficult to evaluate models. And, as last, the statistical framework also make it difficult to deal with non-linear models and non-parametric techniques are difficult to yield with this specification.</p>
<p>Below, I first explain how using techniques from the data driven approach side, or the data science side, can help research in the field of regional economics further in three directions: model comparison, heterogeneous effect sizes, and predicting.</p>
<p>Thereafter, I describe what needs to be changed in education, so that future students will better enabled to deal with the abundance of larger datasets, the need for better predictions and a more intuitive understanding of probabilities and testing.</p>
<section id="in-research" class="level3 page-columns page-full" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="in-research"><span class="header-section-number">3.1</span> In research</h3>
<section id="dealing-with-heterogeneity" class="level4 page-columns page-full" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="dealing-with-heterogeneity"><span class="header-section-number">3.1.1</span> Dealing with heterogeneity</h4>
<div class="page-columns page-full"><p>One of the weaknesses of the theory driven approach—or the more classical research methods—is dealing with heterogeneity. Fixed effects regressions only deal with removing level effects and not varying slope effects, difference-in-difference designs only give average treatment effects and instrumental variables have difficulties with incorporating heterogeneity.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">There are some advances made in introducing heterogeneous instruments in quantile regression techniques, but the exact mechanism is still not clear-cut.</span></div></div>
<p>The argument made against heterogeneity is that it only affects efficiency (i.e., the standard errors), but in most cases this is not true. In non-linear models, such as discrete choice and duration models heterogeneity affects the consistency (i.a., the parameter of interest) as well. Moreover, interpretation of the parameter of interest might be completely off when not allowing for heterogeneous groups.</p>
<div id="fig-mixture" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mixture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/featured.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mixture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: A mixture of two distributions of housing prices
</figcaption>
</figure>
</div>
<p>Consider Figure&nbsp;4 where in the left panel a density distribution is given from a distribution of lognormally distributed housing prices. When interested in explaining the effect of a variable <img src="https://latex.codecogs.com/png.latex?x"> on housing prices this is typically the first descriptive plot an applied researcher creates. The middle panel enlights this plot further by combining both a density plot and a histogram. But what if the sample consists of types of housing markets. One overheated and one with ample housing supply. Then most likely the mechanism on both markets are different and the effect <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> could be very well different for both markets. Indeed, the right panel shows that the density distribution from the left panel is actually a realization of a mixture of two (in this case normal) distributions. The housing market with ample supply of houses is then represented by group 1 and the overheated housing market is represented by group 2.</p>
<div class="page-columns page-full"><p>These latent class approaches are typically not much applied in (regional) economics <span class="citation" data-cites="lankhuizen2015">(see for an exception, e.g., Lankhuizen, De Graaff, and De Groot 2015)</span>. However, correct identification of submarkets or subgroups could be very important for policy makers as the average treatment effect may very well not even apply to anyone <span class="citation" data-cites="DeGraaff2014misc">(an argument, in Dutch, made as well in Graaff 2014)</span>.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">In other economic or social science fields, such as market and transportation science, however, this is already a common approach.</span></div></div>
<p>Slowly, the notation that fixed effects contain much useful information permeated in the regional economics domain. An insightful and straightforward way to do this is by adapting the wage model in Equation&nbsp;6 as follows:</p>
<p><span id="eq-fe2"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cln(w_%7Bic%7D)%20&amp;=%20%5Cxi_c%20+%20%5Cbeta%20%5Cln(d_%7Bic%7D)+%5Cmathbf%7Bz_%7Bic%7D%7D%5Cgamma%20+%20%5Cepsilon_%7Bic%7D%20%5Cnotag%20%5C%5C%0A%20%20%5Cxi_c&amp;=%5Calpha%20+%20%5Cmathbf%7Bx_c%7D%5Cdelta%20+%20%5Cmu_c,%0A%5Cend%7Balign%7D%0A%5Ctag%7B7%7D"></span></p>
<p>where the individual wage model is now split up in two stages. The first stage model the individual variation and regional fixed effects. The second stage now regresses regional variables on the estimated fixed effects. This approach is now frequently applied <span class="citation" data-cites="Bayer2004 Bayer2007a Wang2016 bernasco2017social">(for example, in the so-called sorting model Bayer, McMillan, and Rueben 2004; Bayer and Timmins 2007; Zhiling, Graaff, and Nijkamp 2016; and Bernasco et al. 2017)</span>.</p>
<p>Two large advantages of this approach are that the standard errors on both the individual and the regional level are correctly estimated and that, if needed, instrumental variable methods may be applied in the second stage. There is one disadvantage and that is the fixed effects in the second stage are not observed but estimated (imputed) and that has an effect on the standard errors.</p>
<p>Note that model Equation&nbsp;7 is very much alike multilevel models, which are very often used both in the data driven approach and in other social science apart from economics. Multilevel modeling works great in both correctly estimating a model with observations on various levels (such as individuals, firms, sectors and regions) and in retrieving heterogeneous estimates (both in levels and in slopes). And with the increasing advent of micro-data, combining a individual-regional model as in Equation&nbsp;7 with the more rigorous structure of multilevel modeling is definitely worth more attention in the near future.</p>
<p>Interestingly, more (spatial) non-parametric approaches <span class="citation" data-cites="Thissen2016">(see, e.g., the geograpically weighted regression exercise in Thissen, Graaff, and Oort 2016)</span> have become more popular as well in the last decade (typically, because of increased computer power). This approach needs more attention as well, as the connection with (economic) theory is often lost. And, especially regional geographers apply spatial non-parametric techniques, not the regional economists.</p>
</section>
<section id="model-comparison" class="level4 page-columns page-full" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="model-comparison"><span class="header-section-number">3.1.2</span> Model comparison</h4>
<p>One element that is notoriously weak in the theory-driven approach is model comparison—or the models should be nested. And in many cases, model comparison is often very much asked by policy makers and the audience at large, if not only for finding the correct specification. The latter question is concerned with the question which variables (predictors) to include in a model and which predictors of them perform best. Note that this is analogous to questions regional policy makers might have: such as, which policy instruments best to deploy given limited financial budgets.</p>
<p>A typical example can be found in the field of spatial econometrics model where comparison is an important issue as typically there are several competing theories, non-nested, for the distance decay function (usually measured with a so-called spatial weight matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D">). And usually those theories are very much related (e.g., distance decay measured in Eucledian distance or generalized travel costs).</p>
<div class="page-columns page-full"><p>Another field where model comparison is of large importance is in the estimation of the strength of socio-ecoomic networks. In theory, socio-economic networks should produce so-called power-laws: or a loglinear relation between the size of nodes and the number of connections. Empirically, these relations often follow a slightly different distribution. What kind of distribution fits then best is still a matter of debate.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">One of the most famous of these relations is Zipf’s law, where the ordering of cities and the size of the population follows an almost perfect loglinear distribution; see for an in-depth treatment <span class="citation" data-cites="gabaix1999zipf">Gabaix (1999)</span>.</span></div></div>
<div class="page-columns page-full"><p>For proper model comparison, a Bayesian approach is almost unavoidable. The key difference between the frequentist and the Bayesian approach is how to interpret uncertainty. In the frequentist approach uncertainty originates from sampling, while in the Bayesian approach uncertainty is caused by not having enough information. So, a Bayesian statistician lives in a deterministic world but has a limited observational view.. Note that the rule of Bayes is not unique for Bayesian statistics. Namely, this rule is central for all probability theory.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Both frequentists and bayesians rely heavily on sampling. However, sampling in frequentist statistics is a device to construct undercertainty around an estimate. Sampling in Bayesian statistics is a way to perform integral calculus (or to simulate observations).</span></div></div>
<p>What is unique for each Bayesian model is that it has a prior and posterior. The prior is an assumption about something that you do not know (uncertainty measured by a parameter). With additional information (data), knowledge about the uncertainty is then updated (and hopefully the uncertainty is diminished). The updated probabilities are represented in a posterior distribution. To understand the probabilities then is simply a matter of sampling from the posterior distribution. So, the frequentist approach typically give a point estimate of a parameter, the Bayesian approach gives the whole distribution of the parameter. Note that under the Bayesian paradigm, everything (including the data) is regarded as an variable with associated uncertainty.</p>
<p><span id="eq-bayes"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%5Cln(h_r)%20&amp;%20%5Csim%20%5Ctext%7BNormal%7D(%5Cmu_r,%20%5Csigma)%20%5Ctag%7Blikelihood%7D%5C%5C%0A%20%20%5Cmu_r%20&amp;%20=%20%5Calpha%20+%20%5Cbeta%20x_r%20%5Ctag%7Blinear%20model%7D%5C%5C%0A%20%20%5Calpha%20&amp;%20%5Csim%20%5Ctext%7BNormal%7D(12,3)%20%5Ctag%7B$%5Calpha$%20prior%7D%5C%5C%0A%20%20%5Cbeta%20&amp;%20%5Csim%20%5Ctext%7BNormal%7D(5,10)%20%5Ctag%7B$%5Cbeta$%20prior%7D%5C%5C%0A%20%20%5Csigma%20&amp;%5Csim%20%5Ctext%7BUniform%7D(0,2)%20%5Ctag%7B$%5Csigma$%20prior%7D%0A%5Cend%7Balign%7D%0A%5Ctag%7B8%7D"></span></p>
<div class="page-columns page-full"><p>Equation&nbsp;8 gives an example of a Bayesian linear regression model.. Here, we want to model the relation between regional housing prices (<img src="https://latex.codecogs.com/png.latex?h_r">) and the regional percentage of open space (<img src="https://latex.codecogs.com/png.latex?o_c">). Note that all parameters and the distribution of the data (likelihood) require distributional assumptions. This is a disadvantage in relation to the inference based frequentist approach, where no distributional assumptions are needed. But, note as well that Equation&nbsp;8 specifies <strong>explicitly</strong> all assumptions for this model (e.g., a linear model and a normal distribution for the likelihood). If you think the model is incorrect you can rather easily change the assumptions.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Under relatively mild assumptions this should yield similar results as the frequentist approach.</span></div></div>
<p>Estimating Bayesian models have always been computationally cumbersome, especially with more parameters as sampling from the posterior distribution equalizes sampling from a multi-dimensional integral. Fortunately, computational power has increased dramatically in the last decades and techniques for sampling from the posterior distribution have become rather efficient (the most often used techniques nowadays are Monte Carlo Markov Chain algorithms which is basically a simulation of the posterior distribution).</p>
<p>Although Bayesian statistics has already been applied to spatial econometrics <span class="citation" data-cites="lesage2009introduction">(see the excellent textbook of LeSage and Pace 2009)</span>, applications have not permeated much to other fields in regional economics, such as in regional growth estimations, individual-regional multi-level modeling and population-employment modeling.</p>
</section>
<section id="predicting" class="level4 page-columns page-full" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="predicting"><span class="header-section-number">3.1.3</span> Predicting</h4>
<div class="page-columns page-full"><p>A last field not well developed in (regional) economics is that of predicting. Most economists would shy away from predictions as, in their opinion, identifying causal relations is already difficult enough (they have a point there). What economists love to do is giving counterfactuals instead. For example, if regional open space would decrease significantly, what would happens with regional housing prices. This counterfactual approach looks very much as a prediction, however there are two large disadvantages associated with counterfactuals.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">In popular media, though, they are less hesitant to offer predictions.</span></div></div>
<p>First, counterfactual are always made <strong>in sample</strong>. Actually, all marginal effects are made <strong>in-sample</strong>. Splitting the sample in a training set and a test set is not something that (regional) economists are prone to do. There is an intrinsic worry then for , especially when using many fixed effects. Explanatory power may be very high, but could also be very situation related. What works in one region, does not necessarily works in another region. Note that predicting in spatial settings is more difficult as the unit of analysis is typically a spatial system. And sub-setting a spatial system in a training and test set is often difficult.</p>
<p>Consider, e.g., the following often used gravity model in linear form as depicted in Equation&nbsp;9:</p>
<p><span id="eq-gravity"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cln(c_%7Bij%7D)%20=%20%5Calpha%20+%20%5Cbeta%20%5Cln(P_i)%20+%20%5Cgamma%20%5Cln(E_j)%20+%20%5Cln(d_%7Bij%7D)%20+%20%5Cepsilon_%7Bij%7D.%0A%5Ctag%7B9%7D"></span></p>
<p>Here, we aim to model the number of commuters (<img src="https://latex.codecogs.com/png.latex?c">) from region <img src="https://latex.codecogs.com/png.latex?i"> to region <img src="https://latex.codecogs.com/png.latex?j">, by looking at the total labor force <img src="https://latex.codecogs.com/png.latex?P_i"> in region <img src="https://latex.codecogs.com/png.latex?i">, the total number of jobs <img src="https://latex.codecogs.com/png.latex?E_j"> in region <img src="https://latex.codecogs.com/png.latex?j"> and the distance (<img src="https://latex.codecogs.com/png.latex?d_%7Bij%7D">) between the two regions. Suppose, we can improve the distance between region <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> by, e.g., enlarging highway capacity. This does not only change the commuter flow between <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">, but also between other region; say between <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?k">. As usual there is no free lunch and total employment and population in each should remain constant, at least in the short-run.</p>
<p>However, this make sub-setting difficult and correctly predicting cumbersome. But, Equation&nbsp;9 of above is just one example of a large class of models that all face this difficulty. And policy makers (and firms) are actually very much interested in the questions associated with these predictions. Questions related to the impact of Brexit on other countries, total network effects of infrastructure improvements, identifying profitable routes for airlines, impact of housing projects on commuting quickly come to mind. So, it is especially the relation between predicting and spatial (interaction) systems that need considerable attention.</p>
<div class="page-columns page-full"><p>A second problem with the counterfactual approach is that it considers marginal changes. Unfortunately, in models as Equation&nbsp;9 this would not work. A marginal change on the link between <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> would have marginal changes on most other links. Marginal changes in a network setting is still a relatively underdeveloped area.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">That is, in applied empirical statistical work. Computational equilibrium models and to a lesser extent input-output models are able to model network-wide impacts. However, these models are cumbersome to construct and are less based on data.</span></div></div>
<p>So, on of the main research challenges in the regional economic domain for the near future would be to combine the data science models with the concept of spatial interaction models in such a way that both predictions can be made and that model restrictions are still satisfied.</p>
</section>
</section>
<section id="in-education" class="level3 page-columns page-full" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="in-education"><span class="header-section-number">3.2</span> In education</h3>
<p>As discussed above, in regional economics—in fact, in all social sciences—introductory statistics courses are like cook books. In this situation you need that recipe (test), in that situation you need that recipe (test). These recipes even perfectly coincide with the drop-down menu from certain, grapically user interface driven, statistical software packages such as SPSS. This causes students not to understand the underlying mechanism but just to apply procedures (or actually push buttons).</p>
<p>Without going into the need for using a frequentist or a Bayesian approach (they coincide more than most people think), I would actually argue very much for already using computers and coding in an early phase in students’ education. This could coincide with more traditional probability theory, but has an advantage that it is a general approach instead of a flowchart.</p>
<div id="fig-normal" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="http://thomasdegraaff.nl/posts/research_agenda/normal.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Overlapping normal distributions
</figcaption>
</figure>
</div>
<p>As an example, consider Figure&nbsp;5 where two normal distributions are depicted. The left one has a mean of <img src="https://latex.codecogs.com/png.latex?-1">, the right one has a mean of <img src="https://latex.codecogs.com/png.latex?1">. Both have a standard deviation of 1. The question is now to what extent these distributions are different from each other. (Slightly rephrased: this is actually the problem whether two coefficients in a frequentist framework are different from each other). One approach is to search for a suitable test (and quickly run in a plethora of <img src="https://latex.codecogs.com/png.latex?F">, <img src="https://latex.codecogs.com/png.latex?z"> and <img src="https://latex.codecogs.com/png.latex?t">-tests); so, following a flowchart again.</p>
<p>Another approach would actually be to <strong>count</strong>—well, take the integral of —the number of observations in the area that belongs to both distributions. By hand this is infeasible, but with a computer this is rather easy. Just sample a reasonable amount of realisations from both distributions (say <img src="https://latex.codecogs.com/png.latex?N"> times) and count how many times the realisation from the second distribution is smaller than the first distributions. To get a probability, divide by <img src="https://latex.codecogs.com/png.latex?N">. In R the code simply boils down to:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">  N <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>          <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># number of draws</span></span>
<span id="cb1-2">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Draw from first distribution</span></span>
<span id="cb1-3">  n1 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(N, (<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb1-4">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Draw from second distribution</span></span>
<span id="cb1-5">  n2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rnorm</span>(N, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb1-6">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Count how many times second dist.</span></span>
<span id="cb1-7">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># is smaller than first dist.</span></span>
<span id="cb1-8">  count <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(n2 <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> n1)    </span>
<span id="cb1-9">  count<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>N       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get probability </span></span></code></pre></div>
</div>
<div class="page-columns page-full"><p>And for those who are interested, the probability is approximately <img src="https://latex.codecogs.com/png.latex?0.079">. Note that this is a full-blown probability and not so much a test. You could easily turn this into a test when comparing this with a pre-defined probability. If you find this probability to high, then you actually have large doubts whether these two coefficients are different.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Note that I refrain from using the term significance level. This concept is truly a frequentists’ concept and on a fundamental level relies on the concept of infinite sampling to get uncertainty.</span></div></div>
<div class="page-columns page-full"><p>Although this approach is definitely intuitive and arguable very powerful (if you understand the approach above, you basically understand Bayesian statistics as well) it does require computer literate skills from students. And contrary to popular belief, most students actually face large difficulties with coding, command line tools, working with file systems, and so on. This is caused by the fact that all tools they usually work with are driven by drop-down menu’s, templates and strong graphical user interfaces.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Typically they work with Powerpoint, Excel and Word.</span></div></div>
<p>This is also caused by the fact that in regional economics (actually in al the social sciences), remarkably little attention has been given to the set of computer-related tools students could use, why they should use them and the relation between them <span class="citation" data-cites="Rey:2014cl arribas2015woow Arribas2016">(with some exceptions as, amongst some others, by Rey 2014; Arribas-Bel and Graaff 2015; Arribas-Bel, Graaff, and Rey \noop{3002}Forthcoming)</span>. This is even more remarkable as reproducibility and robustness of results become more important in research and teaching.</p>
<p>And this does not not apply for statistical software tools such as R or Python, but as well to other fields. Gathering data, manipulating data, visualising data and communing results are all skills that arguably are very important for students and scientist and become even more important in the future <span class="citation" data-cites="varian2014big">(Varian 2014)</span>. There are some exceptions as <span class="citation" data-cites="schwabish2014economist">Schwabish (2014)</span>, but in (regional) economics these skill still receive not much attention—in research, but especially in education.</p>
<p>I conclude this section by arguing that we miss three main elements in our curriculum. The first on being a larger emphasis on computer literature skills, such as coding, command line tools, visualization of data, an so forth. The second is more room for the data driven approach, where using software packages such as R of Python, problems are solved with data science techniques, with its larger emphasis on predicting and non-linear modeling. To be clear, simulation exercises as above functions as well as a data driven approach. Most importantly, students should understand the underlying mechanism, instead of applying procedures. The third and final element that is missing is consistently throughout the curriculum. This is often understood as applying the same tools for each course, but this is not necessarily the case. What I mean with consistently is that elements from method courses should come back in regular courses. Nowadays, most courses could implement an empirical element, such as regression techniques, data visualization, data manipulation, and perhaps coding as well. Why otherwise give a Python in the first year of the bachelor, without using that in other courses?</p>
</section>
</section>
<section id="into-the-abyss" class="level2 page-columns page-full" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="into-the-abyss"><span class="header-section-number">4</span> Into the abyss</h2>
<p>I started this paper with the observation, that, in the words of <span class="citation" data-cites="breiman2001statistical">Breiman (2001)</span>, there seems so be two cultures in statistical or econometric modeling; a theory driven and a data driven approach. These two approaches are not mutually exclusive, but complementary. And both have their own strengths and weaknesses. However, especially in economics—and thus in regional economics as well—the theory driven approach still seems to be highly dominant, even with the advent of increasingly larger (micro-)databases. Arguably, this is problematic as the theory driven approach has difficulties when answering questions typically asked by policy makers; questions such as <em>What works best for my region?</em>, <em>What happens with the capacity of my whole network when I invest in a specific highway link?</em> and <em>In which region should I invest to get highest returns?</em>.</p>
<p>So, the main argument of this paper lies in introducing more data approach/data science techniques in the toolkit of the regional economist. Other related fields, even in the social sciences, have already made large advances, such as predictive policing in criminology, latent class approaches in transportation modeling, and the use of deep learning techniques in marketing sciences.</p>
<div class="page-columns page-full"><p>Obviously, this needs large investments (mostly in time), both for researchers and for teachers. The first group needs to invest in new techniques and probably in new statistical software. The second group needs to change parts of the curriculum in terms of the specific contents of methods courses and exercises. Fortunately, many online and open source manuals, videos and even textbooks are available. Moreover, companies such as DataCamp allow for free subscriptions as long as the material is used for classes.}</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Especially for the R programming environment <span class="citation" data-cites="R2017">R Core Team (2017)</span> there is a vast amount of material available on the internet, such as <a href="http://r4ds.had.co.nz/">R for Data Science</a> and <a href="https://csgillespie.github.io/efficientR/">Efficient R programming</a>.</span></div></div>
<p>To conclude, I would like to note that apart from the intrinsic scientific arguments there are two other very compelling arguments to invest at least some time in data driven approaches. First, it coincides wonderfully with other techniques, such as versioning, blogging (publishing to HTML), and command line tools. All these approaches ensure that research becomes more reproducible. Something that becomes more and more a hard requirement by both university and the audience at large. Second, when looking at recent advances both in industry (e.g., all the dotcom companies but also others, such as more traditional media companies) and in other scientific disciplines, it is not the question <strong>if</strong> regional economists should invest more in the data science approach, but the question <strong>how soon</strong> can we start.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-angrist2008mostly" class="csl-entry">
Angrist, Joshua D, and Jörn-Steffen Pischke. 2008. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Princeton university press.
</div>
<div id="ref-arribas2015woow" class="csl-entry">
Arribas-Bel, Daniel, and Thomas de Graaff. 2015. <span>“WooW-II: Workshop on Open Workflows.”</span> <em>REGION</em> 2 (2): 1–2.
</div>
<div id="ref-Arribas2016" class="csl-entry">
Arribas-Bel, Daniel, Thomas de Graaff, and Sergio Rey. \noop{3002}Forthcoming. <span>“Looking at John Snow’s Cholera Map from the XXIst Century: A Practical Primer on Reproducibility and Open Science.”</span> In <em>Regional Research Frontiers: The Next 50 Years</em>, edited by Randall Jackson and Peter Schaeffer. Berlin: Springer.
</div>
<div id="ref-Bayer2004" class="csl-entry">
Bayer, Patrick, Robert McMillan, and Kim Rueben. 2004. <span>“An Equilibrium Model of Sorting in an Urban Housing Market.”</span>
</div>
<div id="ref-Bayer2007a" class="csl-entry">
Bayer, Patrick, and Christopher Timmins. 2007. <span>“Estimating Equilibrium Models of Sorting Across Locations.”</span> <em>The Economic Journal</em> 117 (518): 353–74.
</div>
<div id="ref-bernasco2017social" class="csl-entry">
Bernasco, Wim, Thomas de Graaff, Jan Rouwendal, and Wouter Steenbeek. 2017. <span>“Social Interactions and Crime Revisited: An Investigation Using Individual Offender Data in Dutch Neighborhoods.”</span> <em>Review of Economics and Statistics</em> 99 (4): 622–36.
</div>
<div id="ref-bettencourt2010unified" class="csl-entry">
Bettencourt, Luis, and Geoffrey West. 2010. <span>“A Unified Theory of Urban Living.”</span> <em>Nature</em> 467 (7318): 912–13.
</div>
<div id="ref-breiman2001statistical" class="csl-entry">
Breiman, Leo. 2001. <span>“Statistical Modeling: The Two Cultures.”</span> <em>Statistical Science</em> 16 (3): 199–231.
</div>
<div id="ref-deaton2010instruments" class="csl-entry">
Deaton, Angus. 2010. <span>“Instruments, Randomization, and Learning about Development.”</span> <em>Journal of Economic Literature</em> 48: 424–55.
</div>
<div id="ref-einav2014economics" class="csl-entry">
Einav, Liran, and Jonathan Levin. 2014. <span>“Economics in the Age of Big Data.”</span> <em>Science</em> 346 (6210): 1243089.
</div>
<div id="ref-gabaix1999zipf" class="csl-entry">
Gabaix, Xavier. 1999. <span>“Zipf’s Law for Cities: An Explanation.”</span> <em>The Quarterly Journal of Economics</em> 114 (3): 739–67.
</div>
<div id="ref-DeGraaff2014misc" class="csl-entry">
Graaff, Thomas de. 2014. <span>“Stedelijke Voorzieningen En Bevolking: Wie Woont Waar En Waarom?”</span> in: Jessie Bakens, Henri de Groot, Peter Mulder and Cees-Jan Pen (eds.), <em>Soort zoekt soort: Clustering en sociaal-economische scheidslijnen in Nederland</em>, platform31.
</div>
<div id="ref-DeGraaff2012" class="csl-entry">
Graaff, Thomas de, Frank G. van Oort, and Raymond J. G. M. Florax. 2012a. <span>“Regional Population-Employment Dynamics Across Different Sectors of the Economy.”</span> <em>Journal of Regional Science</em> 52 (1): 60–84.
</div>
<div id="ref-Graaff2012" class="csl-entry">
———. 2012b. <span>“Sectoral Heterogeneity, Accessibility and Population-Employment Dynamics in Dutch Cities.”</span> <em>Journal of Transport Geography</em> 25: 115–27.
</div>
<div id="ref-imbens1994identification" class="csl-entry">
Imbens, Guido W, and Joshua D Angrist. 1994. <span>“Identification and Estimation of Local Average Treatment Effects.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 467–75.
</div>
<div id="ref-lankhuizen2015" class="csl-entry">
Lankhuizen, Maureen B. M., Thomas De Graaff, and Henri L. F. De Groot. 2015. <span>“Product Heterogeneity, Intangible Barriers and Distance Decay: The Effect of Multiple Dimensions of Distance on Trade Across Different Product Categories.”</span> <em>Spatial Economic Analysis</em> 10 (2): 137–59.
</div>
<div id="ref-lesage2009introduction" class="csl-entry">
LeSage, James, and Robert Kelley Pace. 2009. <em>Introduction to Spatial Econometrics</em>. CRC Press.
</div>
<div id="ref-mcelreath2016statistical" class="csl-entry">
McElreath, Richard. 2016. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. Vol. 122. CRC Press.
</div>
<div id="ref-R2017" class="csl-entry">
R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-Rey:2014cl" class="csl-entry">
Rey, Sergio J. 2014. <span>“Open Regional Science.”</span> <em>Annals of Regional Science</em> 52 (3): 825–37.
</div>
<div id="ref-roback1982wages" class="csl-entry">
Roback, Jennifer. 1982. <span>“Wages, Rents, and the Quality of Life.”</span> <em>Journal of Political Economy</em> 90 (6): 1257–78.
</div>
<div id="ref-schwabish2014economist" class="csl-entry">
Schwabish, Jonathan A. 2014. <span>“An Economist’s Guide to Visualizing Data.”</span> <em>The Journal of Economic Perspectives</em> 28 (1): 209–33.
</div>
<div id="ref-Thissen2016" class="csl-entry">
Thissen, Mark, Thomas de Graaff, and Frank G. van Oort. 2016. <span>“Competitive Network Positions in Trade and Structural Economic Growth: A Geographically Weighted Regression Analysis for European Regions.”</span> <em>Papers in Regional Science</em> 95 (1): 159–80.
</div>
<div id="ref-varian2014big" class="csl-entry">
Varian, Hal R. 2014. <span>“Big Data: New Tricks for Econometrics.”</span> <em>The Journal of Economic Perspectives</em> 28 (2): 3–27.
</div>
<div id="ref-Wang2016" class="csl-entry">
Zhiling, Wang, Thomas de Graaff, and Peter Nijkamp. 2016. <span>“Cultural Diversity and Cultural Distance as Choice Determinants of Migration Destination.”</span> <em>Spatial Economic Analysis</em> 11 (2): 176–200. <a href="https://doi.org/10.1080/17421772.2016.1102956">https://doi.org/10.1080/17421772.2016.1102956</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-de graaff2021" class="csl-entry quarto-appendix-citeas">
Graaff, Thomas de. 2021. <span>“Do Regional Economists Answer the Right
Questions?”</span> June 5, 2021. <a href="http://thomasdegraaff.nl//posts/research_agenda">http://thomasdegraaff.nl//posts/research_agenda</a>.
</div></div></section></div> ]]></description>
  <category>Regional economics</category>
  <category>Predicting</category>
  <category>Causality</category>
  <category>Theory driven approach</category>
  <category>Data science</category>
  <guid>http://thomasdegraaff.nl/posts/research_agenda/</guid>
  <pubDate>Fri, 04 Jun 2021 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Stated choice experiments</title>
  <dc:creator>Thomas de Graaf</dc:creator>
  <link>http://thomasdegraaff.nl/posts/stated_choice/</link>
  <description><![CDATA[ 

<!-- source: https://github.com/gadenbuie/garrickadenbuie-com/blob/main/_partials/title-block-link-buttons/title-block.html -->
<!-- 
<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title"> 
-->
<!-- <header id="title-block-header" class="quarto-title-block default page-columns"> -->
<!-- <div class="quarto-title page-columns page-full featured-image p-4" style="background-image: url(featured.png), url(featured.jpg), url(../featured.jpg);"> -->



<section id="what-is-this-about" class="level1">
<h1>What is this about?</h1>
<p>This is a more static and general post and deals with all sorts of materials considering <strong>stated choice experiments</strong> to help master students in conducting their analysis for their thesis. In the last year(s), we developed all sorts of materials, especially in the form of knowledge clips, that may help students in setting up the <strong>design</strong>, in their <strong>estimations</strong>, and in <strong>interpreting</strong> their results.</p>
<section id="knowledge-clips" class="level2">
<h2 class="anchored" data-anchor-id="knowledge-clips">Knowledge clips</h2>
<p>Paul Koster has made several clip on stated choice experiments</p>
<ul>
<li><a href="https://video.vu.nl/media/Choice+experiments/1_ej62gyoc">An introduction</a></li>
<li><a href="https://video.vu.nl/media/Statistical+design+of+choice+experiments+in+Excel/1_hksvwdnm">About designing in Excel</a></li>
<li><a href="https://video.vu.nl/media/Behavioural+modelling+for+PACE/1_ya3kd12h">About behavioural modelling</a></li>
</ul>
<p>Note that Paul Koster uses a different approach than usual as he allocates points to choices, which is applicable for studying different policies. The attractive feature of this is than one can uses just OLS instead of applying a logit (as typically in the case of discerning between two or more alternatives).</p>
<p>I myself have made some clips this year. One is on an exam question (which is about the interpretation of the outcome and possible biases that may arise in Stated Preference surveys); the other deals with working with logistics regressions and why it is so different from ordinary least squares.</p>
<ul>
<li><a href="https://video.vu.nl/media/vosl/1_ato8hlbc">Exam question: the value of statistical life</a></li>
<li><a href="https://video.vu.nl/media/logistic%20regression/1_09nkiqyb">Logistis regression</a></li>
</ul>
<p>Vincent van de Berg also created content in the context of transport economics:</p>
<ul>
<li><a href="https://video.vu.nl/media/2+Stated+choice+experiments+%28empirical+transport+economics%29/1_75utwqeb">Stated choice experiments (empirical transport economics)</a></li>
</ul>
</section>
<section id="background-material" class="level2">
<h2 class="anchored" data-anchor-id="background-material">Background material</h2>
<p>Paul Koster refers in his knowledge clip to the work of Sanko, Daly and Kroes (2002), so does Vincent van den Berg in this <a href="../../docs/SCE.pdf">lecture</a>. You can find this paper <a href="../../docs/sanko.pdf">here</a>.</p>
</section>
<section id="previous-theses" class="level2">
<h2 class="anchored" data-anchor-id="previous-theses">Previous theses</h2>
<p>Already many theses have been written using Stated preference techniques. Below are some we do find of good quality and that also display the variation in topics. These theses should only be used for inspiration. Do not copy their methodological approach!</p>
<p>Theses:</p>
<ul>
<li><a href="../../docs/gerben_de_jong.pdf">Gerben de Jong (2014)</a></li>
<li><a href="../../docs/lena_pax.pdf">Lena Pax (2020)</a></li>
<li><a href="../../docs/de_keyser.pdf">Erika de Keyser (2020)</a></li>
</ul>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-de graaf2021" class="csl-entry quarto-appendix-citeas">
Graaf, Thomas de. 2021. <span>“Stated Choice Experiments.”</span> April
2, 2021. <a href="http://thomasdegraaff.nl//posts/stated_choice">http://thomasdegraaff.nl//posts/stated_choice</a>.
</div></div></section></div> ]]></description>
  <category>Teaching</category>
  <category>Reproducability</category>
  <guid>http://thomasdegraaff.nl/posts/stated_choice/</guid>
  <pubDate>Thu, 01 Apr 2021 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Krugman’s Increasing Returns and Economic Geography</title>
  <link>http://thomasdegraaff.nl/posts/krugman_neg/</link>
  <description><![CDATA[ 

<!-- source: https://github.com/gadenbuie/garrickadenbuie-com/blob/main/_partials/title-block-link-buttons/title-block.html -->
<!-- 
<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title"> 
-->
<!-- <header id="title-block-header" class="quarto-title-block default page-columns"> -->
<!-- <div class="quarto-title page-columns page-full featured-image p-4" style="background-image: url(featured.png), url(featured.jpg), url(../featured.jpg);"> -->



<section id="drawing-the-diagram-of-a-stylized-version-of-krugmans-increasing-returns-and-economic-geography" class="level2">
<h2 class="anchored" data-anchor-id="drawing-the-diagram-of-a-stylized-version-of-krugmans-increasing-returns-and-economic-geography">Drawing the diagram of a stylized version of Krugman’s <em>Increasing Returns and Economic Geography</em></h2>
<p>For educational purposes we teach in the second year’s course <em>regional and urban economics</em> a simplified version of Krugman’s model in his paper titled <em>Increasing Returns and Economic Geography</em>. The model we have adopted goes as follows:</p>
<p>We consider a simplified economy with two regions and 1 (million) workers ( <img src="https://latex.codecogs.com/png.latex?L=1"> ) in total. Region 1 is inhabited by 100,000 farmers (bound to their land so immobile), while in Region 2 there are 200,000 farmers. Note that in the notation of Krugman this boils down to <img src="https://latex.codecogs.com/png.latex?%5Cpi_1%20=%200.1"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi_2%20=%200.2">. All other workers work in manufacturing. Assume now that there is a representative firm that has to choose if and in which region if would settle or that it would settle in both regions by having two branches (one in each region). The fixed costs to establish a firm (or branch) is 0.15. The transportcosts to move goods between region 1 and 2 are equal to 1 and each worker consumer consumes exactly one unit of the final product.</p>
<p>It is now up to the student to determine the equilibria in this economy (whether stable or unstable) and identify the trade-off for the firm. Doing this the most insightful and simple way is draw the so-called PP-line and the MM-line. Using the Tikz package within Latex simplifies this enormously. The following code shows how this can be done.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode tex code-with-copy"><code class="sourceCode latex"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Krugman91---Firm location in two regions </span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Author: Thomas de Graaff</span></span>
<span id="cb1-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">\documentclass</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">article</span>}</span>
<span id="cb1-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">\usepackage</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">tikz, verbatim</span>}</span>
<span id="cb1-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">\usepackage</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pgfplots</span>}   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%include other needed packages here</span></span>
<span id="cb1-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">\usepackage</span>[active,tightpage]{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">preview</span>}</span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\PreviewEnvironment</span>{tikzpicture}</span>
<span id="cb1-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\setlength\PreviewBorder</span>{0pt}<span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span></span>
<span id="cb1-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\begin</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">comment</span>}</span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:Title: Krugman91---Firm location in two regions </span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:Tags: Economic geography, economics, location behavior, </span></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">multiple equilbria</span></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:Author: Thomas de Graaff</span></span>
<span id="cb1-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\end</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">comment</span>}</span>
<span id="cb1-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\begin</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">document</span>}</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\begin</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">tikzpicture</span>}[scale=1,thick]</span>
<span id="cb1-18"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\usetikzlibrary</span>{calc, intersections}   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%allows coordinate calculations.</span></span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Define parameters</span></span>
<span id="cb1-21"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\L</span>{1}               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Total amount of workers (normalized)</span></span>
<span id="cb1-22"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Pa</span>{0.1}            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Total amount of farmers in regio 1</span></span>
<span id="cb1-23"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Pb</span>{0.2}            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Total amount of farmers in regio 2</span></span>
<span id="cb1-24"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\x</span>{1}               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Total demand (normalized)</span></span>
<span id="cb1-25"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\F</span>{0.15}            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Fixed costs to set up a plant</span></span>
<span id="cb1-26"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\t</span>{1}               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">% Transportcosts</span></span>
<span id="cb1-27"></span>
<span id="cb1-28"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Fa</span>{<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\F</span>/(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\t*\x</span>)}</span>
<span id="cb1-29"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Fb</span>{1-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\F</span>/(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\t*\x</span>)}</span>
<span id="cb1-30"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Eq</span>{(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>/(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa+\Pb</span>)}</span>
<span id="cb1-31"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Eqa</span>{((<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>)/(1-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pb</span>)}</span>
<span id="cb1-32"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Eqb</span>{((<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>)/(1-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pb</span>)}</span>
<span id="cb1-33"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Eqleft</span>{min(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>, 0)}</span>
<span id="cb1-34"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\def\Eqright</span>{max(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>-(1-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pb</span>), 0)}</span>
<span id="cb1-35"></span>
<span id="cb1-36"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\begin</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">axis</span>}[</span>
<span id="cb1-37">restrict y to domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>,</span>
<span id="cb1-38">samples = 1000,             </span>
<span id="cb1-39">xmin = 0, xmax = <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>,</span>
<span id="cb1-40">ymin = 0, ymax = <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>,</span>
<span id="cb1-41">xlabel=<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">$S_m$</span>,</span>
<span id="cb1-42">ylabel=<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">$S_p$</span>,</span>
<span id="cb1-43">y axis line style={-}, </span>
<span id="cb1-44">x axis line style={-},</span>
<span id="cb1-45">grid=major,</span>
<span id="cb1-46">legend pos=north west,</span>
<span id="cb1-47">legend entries={45<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">$^</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\circ</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">$</span> line,PP line, MM line}</span>
<span id="cb1-48">]</span>
<span id="cb1-49"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[dotted, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] {x};</span>
<span id="cb1-50"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, red, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] coordinates {(0,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa</span>) (1,1-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pb</span>)};</span>
<span id="cb1-51"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, blue, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] coordinates {(0,0) (0,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>)};          </span>
<span id="cb1-52"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, blue, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] coordinates {(1,1) (1,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>)};</span>
<span id="cb1-53"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, blue, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] coordinates {(0,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>) (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>)};          </span>
<span id="cb1-54"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, blue, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] coordinates {(1,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>) (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>)};</span>
<span id="cb1-55"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, blue, mark=none, domain=0:<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\L</span>] coordinates {(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>) (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>)};</span>
<span id="cb1-56"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, mark=*, fill=red!90] coordinates {(0,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pa+</span>1000*<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Eqleft</span>)};            </span>
<span id="cb1-57"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, mark=*, fill=red!90] coordinates {(1,1-<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Pb+</span>1000*<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Eqright</span>)}; </span>
<span id="cb1-58"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, mark=*, fill=red!90] coordinates {(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Eq</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Eq</span>)};</span>
<span id="cb1-59"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, mark=*, fill=red!10] coordinates {(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Eqa</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fa</span>)};              </span>
<span id="cb1-60"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\addplot</span>[thick, mark=*, fill=red!10] coordinates {(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Eqb</span>,<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">\Fb</span>)};          </span>
<span id="cb1-61"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\end</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">axis</span>}</span>
<span id="cb1-62"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\end</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">tikzpicture</span>}</span>
<span id="cb1-63"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">\end</span>{<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">document</span>}</span></code></pre></div>
<p>This produces the following diagram:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="http://thomasdegraaff.nl/posts/krugman_neg/featured.png" class="img-fluid figure-img"></p>
<figcaption>Equilibria in a stylized version of Krugman (1991)</figcaption>
</figure>
</div>
<p>Clearly, there are with this configuration 3 equilibria; two are stable and one is unstable.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-2015" class="csl-entry quarto-appendix-citeas">
<span>“Krugman’s Increasing Returns and Economic Geography.”</span>
2015. October 5, 2015. <a href="http://thomasdegraaff.nl//posts/krugman_neg">http://thomasdegraaff.nl//posts/krugman_neg</a>.
</div></div></section></div> ]]></description>
  <category>Interaction</category>
  <category>LaTeX</category>
  <category>Education</category>
  <guid>http://thomasdegraaff.nl/posts/krugman_neg/</guid>
  <pubDate>Sun, 04 Oct 2015 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
