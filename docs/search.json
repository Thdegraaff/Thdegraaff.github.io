[
  {
    "objectID": "licence/license.html",
    "href": "licence/license.html",
    "title": "License",
    "section": "",
    "text": "All material are released under a Creative Commons Attribution-ShareAlike 4.0 International License license"
  },
  {
    "objectID": "licence/license.html#non-complicance",
    "href": "licence/license.html#non-complicance",
    "title": "License",
    "section": "Non-complicance",
    "text": "Non-complicance\nYou have to watch:"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Thomas de Graaff",
    "section": "",
    "text": "I am a spatial and urban economist employed at the department of Spatial Economics of the Vrije Universiteit Amsterdam. My fields of professional interests include (inter)national migration, working from home, (spatial) econometrics, transport economics, and most aspects of spatial economics in general. Most recent research focusses on the impact and the use of (un-)observed heterogeneity, and I am rather interested in all sorts of applied Bayesian multilevel models. Basically, this site aims to give access to my research, code snippets and workshops and to give additional information about some of my work, thoughts and other musings.\n Curriculum Vitae  Email me"
  },
  {
    "objectID": "about/index.html#not-so-official-biography",
    "href": "about/index.html#not-so-official-biography",
    "title": "Thomas de Graaff",
    "section": "",
    "text": "I am a spatial and urban economist employed at the department of Spatial Economics of the Vrije Universiteit Amsterdam. My fields of professional interests include (inter)national migration, working from home, (spatial) econometrics, transport economics, and most aspects of spatial economics in general. Most recent research focusses on the impact and the use of (un-)observed heterogeneity, and I am rather interested in all sorts of applied Bayesian multilevel models. Basically, this site aims to give access to my research, code snippets and workshops and to give additional information about some of my work, thoughts and other musings.\n Curriculum Vitae  Email me"
  },
  {
    "objectID": "posts/post/stated_choice/index.html",
    "href": "posts/post/stated_choice/index.html",
    "title": "Stated choice experiments",
    "section": "",
    "text": "This is a more static and general post and deals with all sorts of materials considering stated choice experiments to help master students in conducting their analysis for their thesis. In the last year(s), we developed all sorts of materials, especially in the form of knowledge clips, that may help students in setting up the design, in their estimations, and in interpreting their results.\n\n\n\nAn example of a stated choice question\n\n\n\n\nPaul Koster has made several clip on stated choice experiments\n\nAn introduction\nAbout designing in Excel\nAbout behavioural modelling\n\nNote that Paul Koster uses a different approach than usual as he allocates points to choices, which is applicable for studying different policies. The attractive feature of this is than one can uses just OLS instead of applying a logit (as typically in the case of discerning between two or more alternatives).\nI myself have made some clips this year. One is on an exam question (which is about the interpretation of the outcome and possible biases that may arise in Stated Preference surveys); the other deals with working with logistics regressions and why it is so different from ordinary least squares.\n\nExam question: the value of statistical life\nLogistis regression\n\nVincent van de Berg also created content in the context of transport economics:\n\nStated choice experiments (empirical transport economics)\n\n\n\n\nPaul Koster refers in his knowledge clip to the work of Sanko, Daly and Kroes (2002), so does Vincent van den Berg in this lecture. You can find this paper here.\n\n\n\nAlready many theses have been written using Stated preference techniques. Below are some we do find of good quality and that also display the variation in topics. These theses should only be used for inspiration. Do not copy their methodological approach!\nTheses:\n\nGerben de Jong (2014)\nLena Pax (2020)\nErika de Keyser (2020)"
  },
  {
    "objectID": "posts/post/stated_choice/index.html#knowledge-clips",
    "href": "posts/post/stated_choice/index.html#knowledge-clips",
    "title": "Stated choice experiments",
    "section": "",
    "text": "Paul Koster has made several clip on stated choice experiments\n\nAn introduction\nAbout designing in Excel\nAbout behavioural modelling\n\nNote that Paul Koster uses a different approach than usual as he allocates points to choices, which is applicable for studying different policies. The attractive feature of this is than one can uses just OLS instead of applying a logit (as typically in the case of discerning between two or more alternatives).\nI myself have made some clips this year. One is on an exam question (which is about the interpretation of the outcome and possible biases that may arise in Stated Preference surveys); the other deals with working with logistics regressions and why it is so different from ordinary least squares.\n\nExam question: the value of statistical life\nLogistis regression\n\nVincent van de Berg also created content in the context of transport economics:\n\nStated choice experiments (empirical transport economics)"
  },
  {
    "objectID": "posts/post/stated_choice/index.html#background-material",
    "href": "posts/post/stated_choice/index.html#background-material",
    "title": "Stated choice experiments",
    "section": "",
    "text": "Paul Koster refers in his knowledge clip to the work of Sanko, Daly and Kroes (2002), so does Vincent van den Berg in this lecture. You can find this paper here."
  },
  {
    "objectID": "posts/post/stated_choice/index.html#previous-theses",
    "href": "posts/post/stated_choice/index.html#previous-theses",
    "title": "Stated choice experiments",
    "section": "",
    "text": "Already many theses have been written using Stated preference techniques. Below are some we do find of good quality and that also display the variation in topics. These theses should only be used for inspiration. Do not copy their methodological approach!\nTheses:\n\nGerben de Jong (2014)\nLena Pax (2020)\nErika de Keyser (2020)"
  },
  {
    "objectID": "posts/post/stat_software/index.html",
    "href": "posts/post/stat_software/index.html",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "",
    "text": "Econometrics is much easier without the data—Marno Verbeek\n\nThe quote above does not only apply to economics and econometrics, but to all of the social sciences in general. Empirical research—that is, dealing with data in all its forms—requires a rigorous approach, even more so, with the increasing emphasis on openness and reproducibility of all kinds of scientific research. Therefore, it is strange that in academic education there is not much guidance in choosing which research tools to use and in the philosophy behing choosing an efficient and reproducable workflow.1\nThis note deals with the suitability of various software packages for applying applied econometrics in specific and data science in general.2 Specifically, In will focus on STATA, R and Python.3 I will not say which tool to use. Instead, I will focus on the various strengths and weaknessess of each software package combined with it specific approach. The main elements I will consider are the package suitability for education and how well it can be integrated in an efficient workflow. The former is mostly important for doing (small) data exercises, whilst the latter is vital for larger research project, such as theses and later on perhaps research papers.\nTo illustrate why the concept of reproducibility is vital, note that a typical empirical workflow in the social sciences looks as follows:\n\nGenerate data\n\nData is read from an external source (recording, questionnaire, file or online database) or is simulated.\n\nManipulate data\n\nThis is usually the most time demanding phase4 and includes (amongst many other things) manipulating missing data, merging data and relabeling data\n\nAnalyse data\n\nThis phase includes not only standard ecometrics and statistical or machine learning techniques, but as well as graphical representations as maps and figures.\n\nPresent results\n\nFinally, documents in the forms of papers, posters, theses, or presentations have to be drafted. Note that ideally one wants to do so in various formats, such as in pdf for physical papers and in html for web display.\n\n\nUnfortunately, all these steps do not necessarily run sequentially in time. Supervisors, referees, colleagues, and the future you, always want to add or delete elements to or from your research. For instance, variables have to be added, model specifications have to be checked, and 3D pie charts have to be changed in something useful.5\nTherefore, it is vital that all these steps are both (i) very well documented so that the future you can easily retrace your steps, implement changes and redo the whole research if needed, and (ii) well connected to each other. The latter does not necessarily entail that the whole research should be done in one software environment, but instead that the outcome of one research step (e.g., generating data) can easily serve as an input for another step (e.g., data manipulation).\nIn the next section I will lay out the strengths and weaknesses of three statistical software packages with this workflow in mind. Subsequently, I discuss the suitability of each package in teaching and, thereafter, I conclude with some more general comments."
  },
  {
    "objectID": "posts/post/stat_software/index.html#introduction-the-empirical-workflow",
    "href": "posts/post/stat_software/index.html#introduction-the-empirical-workflow",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "",
    "text": "Econometrics is much easier without the data—Marno Verbeek\n\nThe quote above does not only apply to economics and econometrics, but to all of the social sciences in general. Empirical research—that is, dealing with data in all its forms—requires a rigorous approach, even more so, with the increasing emphasis on openness and reproducibility of all kinds of scientific research. Therefore, it is strange that in academic education there is not much guidance in choosing which research tools to use and in the philosophy behing choosing an efficient and reproducable workflow.1\nThis note deals with the suitability of various software packages for applying applied econometrics in specific and data science in general.2 Specifically, In will focus on STATA, R and Python.3 I will not say which tool to use. Instead, I will focus on the various strengths and weaknessess of each software package combined with it specific approach. The main elements I will consider are the package suitability for education and how well it can be integrated in an efficient workflow. The former is mostly important for doing (small) data exercises, whilst the latter is vital for larger research project, such as theses and later on perhaps research papers.\nTo illustrate why the concept of reproducibility is vital, note that a typical empirical workflow in the social sciences looks as follows:\n\nGenerate data\n\nData is read from an external source (recording, questionnaire, file or online database) or is simulated.\n\nManipulate data\n\nThis is usually the most time demanding phase4 and includes (amongst many other things) manipulating missing data, merging data and relabeling data\n\nAnalyse data\n\nThis phase includes not only standard ecometrics and statistical or machine learning techniques, but as well as graphical representations as maps and figures.\n\nPresent results\n\nFinally, documents in the forms of papers, posters, theses, or presentations have to be drafted. Note that ideally one wants to do so in various formats, such as in pdf for physical papers and in html for web display.\n\n\nUnfortunately, all these steps do not necessarily run sequentially in time. Supervisors, referees, colleagues, and the future you, always want to add or delete elements to or from your research. For instance, variables have to be added, model specifications have to be checked, and 3D pie charts have to be changed in something useful.5\nTherefore, it is vital that all these steps are both (i) very well documented so that the future you can easily retrace your steps, implement changes and redo the whole research if needed, and (ii) well connected to each other. The latter does not necessarily entail that the whole research should be done in one software environment, but instead that the outcome of one research step (e.g., generating data) can easily serve as an input for another step (e.g., data manipulation).\nIn the next section I will lay out the strengths and weaknesses of three statistical software packages with this workflow in mind. Subsequently, I discuss the suitability of each package in teaching and, thereafter, I conclude with some more general comments."
  },
  {
    "objectID": "posts/post/stat_software/index.html#statistical-software-packages",
    "href": "posts/post/stat_software/index.html#statistical-software-packages",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "2 Statistical software packages",
    "text": "2 Statistical software packages\nI review the various packages according to several criteria. There are several others that will be discussed, but these I find most important for a suitable software package to be used for teaching in the social sciences.\n\nOpen source\n\nThe most important argument to use an open source package is reproducibility. Your work is simply less accessible and thus reproducible if the code can only be run with applications that costs over .\n\nLearning curve\n\nFirst, and foremost, students should be able to use the package for straightforward econometric research. If that is not possible after one six-week’s course, the software package is not particularly suitable.\n\nSize of the community\n\nNobody wants to be locked in with obsolete technology. A large userbase ensures a high probability that the software package will be used and maintained in the future as well. Moreover, all sorts of indirect effects, such as user written routines, packages and documentation, come along for free with a large community.\n\nUsefulness outside academia\n\nOften forgotten as an argument but outside academic life, some applications are more used than others. And with the recent emphasis on better preparation for the labor market, this argument seems to become more important. By the way, the application still mostly used would be the ubiquitous Excel and its related visual basic language.\n\nFlexibility\n\nIdeally, a software approach should be both extendable and scalable. The former ensures that slight deviations from standard approaches can relatively easy be implemented. The latter is important when the size of the database increases, as typically is the case with recent improvements in remote sensing techniques.\n\nScriptable\n\nFinally, a software package should be scriptable—both internally as externally. Internally scriptable indicates that within the package scripts or programs can be written so that every step within the workflow can be reproduced. With externally scriptable I mean that the software package should also be used in combination with other software packages or languages, such as LaTeX, markdown, make, html, sql, C++, etc.\n\n\nOther software packages that I will not review, typically score badly on more than one of these criteria. For example, SPSS is proprietary software, which has some issues with scriptability and is not very flexible. Moreover, a sizeable part of the user community moved over to other software packages (most notably R). Other software packages I will not review for reasons of similarity (such as Matlab to some extent) or my lack of experience with them (most notably SAS).6"
  },
  {
    "objectID": "posts/post/stat_software/index.html#stata",
    "href": "posts/post/stat_software/index.html#stata",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "3 STATA",
    "text": "3 STATA\nSTATA is proprietary software copyrighted by the StataCorp LLC corporation.7 It is especially popular among economists, although the growth of STATA seems almost comparable with that of R (Python is in its own league, however).8\nThe biggest comparative advantage of STATA is its learning curve (although some students might disagree). It is relatively straightforward to teach students basic econometrics (including time series, panel data and count data techniques, discrete choice models and duration models). Indeed, STATA has the advantage of being one of the only tools dedicated to econometric research. While it is not so flexible, it is often possible (and simple) to apply cutting-edge models and other techniques. For example, regressing y on x1 and x2 can simply be stated as:\nreg y x1 x2\nMoreover, you can do so in a structured way by writing scripts (the so-called do files). Many students appreciate as well the fact that STATA command can be given interactively (via a drop-down menu) of directly from the console enabling them to memorize the various commands on the fly.\nThere is a large STATA community. This ensures the availability of many useful so-called user-written routines and tutorial material, whether via Youtube or in pdf (there is a STATA journal). The STATA community is not only huge, but as well very helpful (although the help documentation is oftentimes cumbersome). The first hit on Google on a sometimes not very focused question usually suffices.\nUnfortunately, not many organizations outside academia use STATA—although I heard recently that some Danish consultancy agencies in Denmark do. Most organizations now typically use R or Python in combination with even more focused software applications as Hadoop or Julia (again, apart from Excel). Usually, this does not matter much as programming skills are easily transferable, but, unfortunately, the STATA language is hardly a programming language. Instead, it is more a sequence of very specific commands. Therefore, it is hard to relate the commands to something as Python.\nAnother downside is the flexibility of STATA. For procedures that slightly deviate from the ‘basic’ ones9, STATA can give you a hard time. Mostly, because the source code is not known, working with matrices is cumbersome (to say the least) and the number of programming tools is rather limited. Moreover, STATA can only work with one active dataset, which makes merging datasets sometimes difficult. Finally, although there are some plugins, STATA is not designed for working with data stored on servers somewhere else via application programming interfaces (API’s). Unfortunately, working via API’s10 becomes increasingly important, if not only for the recent surge in open access data via local governments, Google, Twitter, Foursquare, etc.\nSTATA itself is well scriptable internally. although it is less programming and more sequencing commands, for the majority of research projects this is definitely good enough. Externally, however, there are other issues which has to do with its proprietary nature. Other applications, such as great ?nix utilities as make and pandoc but as well git and github have difficulties ‘communicating’ with STATA. It is possible, however, to export from STATA to other formats, especially to ubiquitous text files format, which enables automatic generation of LaTeX tables.\nFinally, alas, STATA is not open source. This hinders reproducibility, insight in the source code (“How the hell did they do that?”) as well as the transferability of data (the STATA .dta dataformat is not accessible to read directly, although many other packages, including R, have created special read procedures for STATA dataformats)."
  },
  {
    "objectID": "posts/post/stat_software/index.html#r",
    "href": "posts/post/stat_software/index.html#r",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "4 R",
    "text": "4 R\nR emerged in 1995 as the open-source (GNU) version of the S language and quickly surpassed its predecessor in popularity.11 R itself consists of a base distribution which can be enhanced with packages, and it is exactly the ease of writing, distributing and using packages that makes R so attractive for many users. At the moment there are more than 10,000 packages on the official CRAN website, but Github contains many more.12\nBecause of its open-source nature and its large and very active community, the quality of most of the packages is almost guaranteed. Indeed, if there are killer applications in the data science world, then R has most of them, with as most notifiable examples dplyr for quick and robust data manipulation and ggplot2 for structural plotting using the grammar of graphics approach as advocated by Wilkinson (2012) and implemented by Wickham (2006).\n\n\nggplot2 really advocates for the creation of diagrams with multiple dimensions, of which an early example is the well-know Minard map as below\n\n\n\n\n\n\nFigure 1: Minard map\n\n\n\nThe learning curve of R is however steeper than that of STATA.13 Having said that, learning basic R is still relatively quick (and accessible) and can be done in a short time with great free online courses. Today, teaching Stata to students requires at least one tutorial class, where teaching R can—theoretically, at least— be self-contained.\nTypically, R is characterized as:\n\nR is written by statisticians, for statisticians.\n\nUnfortunately, this can be seen in the R language itself. Best described as ‘quirky’, the R language is not the most beautiful or best designed programming language. For example, usually there is more than one (often actually more than three) ways to accomplish something, which leads to a myriad of styles and types of coding. In applied statistics, though, R shines. Although slightly more cumbersome than STATA, commands for basic econometric techniques are relatively straightforward. Estimating a linear regression simply looks as follows:\n# ordinary least squares regression\nmodel &lt;- lm(y ~ x1 + x2, data = data1)\nsummary(model)\nNote the difference with STATA, where in STATA one simply uses reg y x1 x2. However, in R one can store everything in an object (in this case the object “model”) and use it later, and one can use several databases (‘dataframes’) at the same time. Both features are extremely useful for dealing with larger research projects. And although STATA still is more straightforward in basic econometrics, R users have developed several packages to emulate STATA’s ease of use.14\nBecause R is very flexible and the community is that huge (across disciplines as well), for almost every (statistical) problem a package most likely is developed (if feasible). The problem is, however, that terminology with respect to models differs across fields. Moreover, what economists find important (causality anyone?) is generally less important within biometrics, physics, statistics of sociology. So, using R also means spending a great deal of time finding and checking the correct package.15\nIf anything, R if flexible in terms of what it can do. Spatial data analysis, Bayesian inference, optimization, network analysis and—above all—producing wonderful plots that can even be used as interactive web applications. Therefore, it is used intensively outside academia as well by large companies such as Facebook, Google, Twitter, Microsoft, Booking.com, Uber and Airbnb, mainly for ‘quick and dirty’ data science and data and output visualization.\nUnfortunately, R is not very scalable, as only internal computer memory (RAM) is used for data storage (similar to matlab, STATA and comparable languages). There are ways to circumvent this (typically in combination with other languages, such as Hadoop or using parallel computing techniques), but it is cumbersome for very large research projects with huge amounts of data (remote sensing data in geographical analysis comes to mind or up to 100 million records when using micro data).\nFinally, R is extremely scriptable, both internally and externally. Internally, the R language is a full blow object-oriented language, although perhaps not the most beautiful one. Especially the ease of writing and reading packages is very useful for groups of researchers who are working on the same project. Externally, R really has a comparative advantage as that it can be scripted from a command line (a terminal), can be used in combination with other languages (e.g., python via rpy and C++ via rcpp to speed up procedures if needed) and works as a charm in combination with both LaTeX and html for creating webpages.16"
  },
  {
    "objectID": "posts/post/stat_software/index.html#python",
    "href": "posts/post/stat_software/index.html#python",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "5 Python",
    "text": "5 Python\nOpposite to STATA and even R, Python is an open source general object oriented programming language more in line with a language such as C++.17 Python was implemented by Guido van Rossum in the late 1980’s and was built on the Modula-3 programming language. As it is a general progamming language one needs several so-called packages in order to make it work for econometrics in particular and data science in general, most notably: NumPy, SciPy, Matplotlib and pandas.\nUsing Python for applied econometrics is not as straightforward as in STATA but nowadays relatively similar to R using SciPy.18 A typical regression routine looks as follows:\nimport ols\nmymodel = ols.ols(y,x,y_varnm,x_varnm)\nmymodel_summary()\nwhich is rather close to R, although the ols class from SciPy needs to be loaded. The class is however not as general as the lm function from R.\nFor an introductory econometrics course this might be too cumbersome. However, the language itself is defined beautifully and everything works similarly. So, once you get used to the seemingly cumbersome approach of Python with respect to basic econometrics, other functionalities requires far less investment in time, because the grammar, style and approach is always consistent.19 Note, that this is rather different from R which is far more chaotic in terms of approaches, which might lead to the argument that Python is actually simpler than R to learn because of its uniformity.\nSo, arguably the learning curve of Python is much steeper than that of STATA or R. However, in the end it does pay off when learning the language as it is not only a good language for learning programming, it is as well very useful for projects just outside the realm of basic econometrics. Working with strings (text analysis), images, spatial data, API’s, large amounts of data, etcetera, is all very common and relatively straightforward in Python. Moreover, the community is very large (larger than that of R). But, not all of them are working in data science.\nR and Python are often compared in their usefulness for statistical research in particular and data science in general.20 The outcome is still undecided. R has the killer applications but Python is catching up fast. Python is perhaps faster and at least better scalable, so perhaps should be considered for larger projects. One of Python benefits is that it is a more general skill which could be applied for other tasks as well. In any case, both of them are used very frequently outside academia.\nFinally, Python is perhaps even more scriptable than R and can be combined with other languages as well. In terms of literature programming, Python has interactive notebooks (now called Jupyter notebooks) which are web applications where text and code can be run interactively."
  },
  {
    "objectID": "posts/post/stat_software/index.html#statistics-in-social-science-education",
    "href": "posts/post/stat_software/index.html#statistics-in-social-science-education",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "6 Statistics in social science education",
    "text": "6 Statistics in social science education\nChoosing which statistical package in academic tuition to be used is a tough call—mostly because of limited course time available, possible lock-in effects and vested interests in academic staff. And each of the software packages reviewed above have their own pros and cons.\nSTATA is arguably the winner when it comes to applying basic econometrics. However, it is not very flexible when it comes to non-econometric statistics (e.g., statistical or machine learning techniques), visualization and web interfaces. Moreover, it is not open source.\nR is slightly more cumbersome than STATA when it comes to basic econometrics techniques, but it shines in flexibility, variety of packages (of which some have become killer application status), visualization support and interaction with other languages and applications.\nPython is the most cumbersome in dealing with basic econometrics, but once learned the language can be used for a variety of applications and together with packages such as pandas and SciPy it can accomplish most tasks R can do but then in a more elegant, structured and consistent manner.\nIn the end it depends on the education goal. If students in the end should be able to apply ‘straighforward’ econometrics techniques, such as diff-in-diff approaches as put forward by Angrist and Pischke (2008), then STATA definitely suffices (and more than that). If only STATA would be open source.\nHowever, if students need to be more flexible as in working with geo-referenced data, application programming interfaces, bayesian techniques, network analysis, more complex discrete choice models or writing their own likelihood functions, then STATA does not suffice anymore and should one, e.g., choose for R or Python. As perhaps anecdotal evidence, but I have seen many PhD students move from STATA to R as the former was not able to do anymore what they wanted.\nAs perhaps a final argument, R and especially Python skills are very transferable to other languages and outside academia. Not only the programming techniques, but as well working from command lines and integrating several files in a reproducible manner, are skills that are very useful, but unfortunately very missing as well."
  },
  {
    "objectID": "posts/post/stat_software/index.html#concluding-remarks",
    "href": "posts/post/stat_software/index.html#concluding-remarks",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "7 Concluding remarks",
    "text": "7 Concluding remarks\nWhat is often heard by students (and staff) is that they do not want to be programmers but social sience researchers instead, and I acknowledge that. My plea here is perhaps not to definitely choose for one of the applications above, but for a more systematic thinking in the type of research tools we teach our students and for trying to do so consistently throughout the academic curriculum. Teaching an introductory Python course, for example, and not using elements of that course in later courses seems a definite waste of time. So, above all, if STATA, Python or R is learned, let it come back in some form later at least in every period. Most courses somehow do use data (if only to create 3D pie charts).\nFinally, by the audience at large there is now a greater need for reproducibility. This requires teaching our students perhaps different research tools and perhaps a different workflow when it comes to empirical research. At least, greater emphasis should be put on revision management, openness and scripting. Granted, some research tools are better in this than others, but it typically is the combination of research tools and the philosophy behind the set of research tools used that is essential."
  },
  {
    "objectID": "posts/post/stat_software/index.html#footnotes",
    "href": "posts/post/stat_software/index.html#footnotes",
    "title": "Choosing statistical software packages for education in the social sciences",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some exceptions, see, e.g., Healy (2011) and Arribas-Bel and Graaff (2014) for a workshop I gave together with Daniel Arribas-Bel.↩︎\nThere is a difference between econometrics on the one hand and applied statistics (including the now very popular data science) on the other hand. Economics students first and foremost need to able to apply applied econometric techniques, such as presented in Stock and Watson (2020) and perhaps later in Angrist and Pischke (2008).↩︎\nI will also briefly touch upon some other packages, but these three mentioned are most likely the ones most used in economics, except of course for the ubiquitous Excel.↩︎\nThere is and old saying that says that 80% of your research time goes in transforming data, while 20% is only spent on analysing the data.↩︎\nFor a wonderful timelapse video on the nonlinearity and even sometimes chaos of writing a research paper, see this link.↩︎\nI should mention the object oriented matrix language Ox as well, which is mostly used by econometricians. Because of the steep learning curve and relatively small community I will not consider it here, but I am aware of its popularity in some particular research groups.↩︎\nSee https://www.stata.com/.↩︎\nI have to be careful though in making these sorts of statements. Usually, the popularity of software applications is researched by looking at search engine counts for jobs, frequently asked questions or counting popularity on the stackoverflow site (https://stackoverflow.com/). An interesting recent overview can be found on http://r4stats.com/2017/06/19/scholarly-articles/.↩︎\nAlthough the set of ‘basic’ procedures is quite extensive.↩︎\nNote that I do hesitate to drop the hype term ‘big data’ here.↩︎\nSee https://www.r-project.org/about.html.↩︎\nSee http://blog.revolutionanalytics.com/2017/01/cran-10000.html.↩︎\nSteep learning curves are in principle not problematic as it indicates that you learn rapidly and in the end the pay-off should be large. For ‘quick and dirty’ solutions, however, steep learning curves are problematic.↩︎\nGood examples are the margins package in R that has similar features as the very useful margins command in STATA and the plm package for implementing fixed effects.↩︎\nFortunately, there is the great website CRAN Task Views which gives good overviews of the most important packages in several subfields, such as dealing with spatial data, Bayesian inference, econometrics and mathematical programming.↩︎\nR has one of the best implementations of so-called literate programming via the package knitr. Literate programming as originally coined by Knuth (1984) indicates that output of coding and documentation are generated simultaneously. Whether or not this is a good idea for larger research projects, literate programming can be very useful for generating documentation of R packages, web blogs, teaching assignments and smaller research papers.↩︎\nSee https://www.python.org/.↩︎\nAlthough there are some good tutorial texts to be found on internet, such as Sargent and Stachurski (2015).↩︎\nFor example, working with geospatial data by using the excellent GeoPandas package is then a breeze, which as a bonus might also obfuscates the need for using expensive and bloated GIS software applications.↩︎\nThis seems another of these heavily disputed standard wars, such as the notorious editor war. A nice infographic of pros and cons of Python and R can be found here.↩︎"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Crime\n\n\nUrban development\n\n\nAgglomeration economies\n\n\nAmenity\n\n\nNew Zealand\n\n\n\nJournal of Housing Economics\n\n\n\n\n\nJun, 2024\n\n\nStuart Donovan, Thomas de Graaff, Henri L. F. de Groot, Aaron Schiff\n\n\n\n\n\n\n\n\n\n\n\n\nAgglomeration\n\n\nCities\n\n\nMeta-analysis\n\n\nProductivity\n\n\nUrbanization\n\n\n\nJournal of Economic Surveys\n\n\n\n\n\nJan, 2024\n\n\nStuart Donovan, Thomas de Graaff, Henri L. F. de Groot, Carl C. Koopman\n\n\n\n\n\n\n\n\n\n\n\n\ncultural heritage\n\n\nEuropean regions\n\n\nmultilevel gravity model\n\n\ntourism flows\n\n\n\nPapers in Regional Science\n\n\n\n\n\nFeb, 2021\n\n\nElisa Panzera, Thomas de Graaff, Henri L. F. de Groot\n\n\n\n\n\n\n\n\n\n\n\n\nHeterogeneity\n\n\nEthnic diversity\n\n\nValuation\n\n\nAmenities\n\n\nMicrodata\n\n\n\nJournal of Economic Geography\n\n\n\n\n\nJan, 2020\n\n\nJessie Bakens, Thomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nTeleworking\n\n\nCommuting distance\n\n\nTreated professions\n\n\n\nThe Annals of Regional Science\n\n\n\n\n\nJan, 2019\n\n\nSergejs Gubins, Jos van Ommeren, Thomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nMigration\n\n\nCulture\n\n\nNetworks\n\n\nLanguage\n\n\n\nREGION\n\n\n\n\n\nJan, 2018\n\n\nZhiling Wang, Thomas de Graaff, Peter Nijkamp\n\n\n\n\n\n\n\n\n\n\n\n\nCrime\n\n\nNeighborhoods\n\n\nSocial interaction\n\n\nSorting model\n\n\n\nReview of Economics and Statistics\n\n\n\n\n\nOct, 2017\n\n\nWim Bernasco, Thomas de Graaff, Jan Rouwendal, Wouter Steenbeek\n\n\n\n\n\n\n\n\n\n\n\n\nMigration\n\n\nCultural diversity\n\n\nCultural distance\n\n\nDestination choice\n\n\nSorting model\n\n\n\nSpatial Economic Analysis\n\n\n\n\n\nJan, 2016\n\n\nWang Zhiling, Thomas de Graaff, Peter Nijkamp\n\n\n\n\n\n\n\n\n\n\n\n\nWillingness to pay for information\n\n\nPrivate road operator\n\n\nPrivate information provider\n\n\nICT\n\n\n\nTransportation Research A\n\n\n\n\n\nOct, 2012\n\n\nSergejs Gubins, Erik T. Verhoef, Thomas de Graaff”\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "",
    "text": "Crime\n\n\nUrban development\n\n\nAgglomeration economies\n\n\nAmenity\n\n\nNew Zealand\n\n\n\nJournal of Housing Economics\n\n\n\n\n\nJun, 2024\n\n\nStuart Donovan, Thomas de Graaff, Henri L. F. de Groot, Aaron Schiff\n\n\n\n\n\n\n\n\n\n\n\n\nAgglomeration\n\n\nCities\n\n\nMeta-analysis\n\n\nProductivity\n\n\nUrbanization\n\n\n\nJournal of Economic Surveys\n\n\n\n\n\nJan, 2024\n\n\nStuart Donovan, Thomas de Graaff, Henri L. F. de Groot, Carl C. Koopman\n\n\n\n\n\n\n\n\n\n\n\n\ncultural heritage\n\n\nEuropean regions\n\n\nmultilevel gravity model\n\n\ntourism flows\n\n\n\nPapers in Regional Science\n\n\n\n\n\nFeb, 2021\n\n\nElisa Panzera, Thomas de Graaff, Henri L. F. de Groot\n\n\n\n\n\n\n\n\n\n\n\n\nHeterogeneity\n\n\nEthnic diversity\n\n\nValuation\n\n\nAmenities\n\n\nMicrodata\n\n\n\nJournal of Economic Geography\n\n\n\n\n\nJan, 2020\n\n\nJessie Bakens, Thomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nTeleworking\n\n\nCommuting distance\n\n\nTreated professions\n\n\n\nThe Annals of Regional Science\n\n\n\n\n\nJan, 2019\n\n\nSergejs Gubins, Jos van Ommeren, Thomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nMigration\n\n\nCulture\n\n\nNetworks\n\n\nLanguage\n\n\n\nREGION\n\n\n\n\n\nJan, 2018\n\n\nZhiling Wang, Thomas de Graaff, Peter Nijkamp\n\n\n\n\n\n\n\n\n\n\n\n\nCrime\n\n\nNeighborhoods\n\n\nSocial interaction\n\n\nSorting model\n\n\n\nReview of Economics and Statistics\n\n\n\n\n\nOct, 2017\n\n\nWim Bernasco, Thomas de Graaff, Jan Rouwendal, Wouter Steenbeek\n\n\n\n\n\n\n\n\n\n\n\n\nMigration\n\n\nCultural diversity\n\n\nCultural distance\n\n\nDestination choice\n\n\nSorting model\n\n\n\nSpatial Economic Analysis\n\n\n\n\n\nJan, 2016\n\n\nWang Zhiling, Thomas de Graaff, Peter Nijkamp\n\n\n\n\n\n\n\n\n\n\n\n\nWillingness to pay for information\n\n\nPrivate road operator\n\n\nPrivate information provider\n\n\nICT\n\n\n\nTransportation Research A\n\n\n\n\n\nOct, 2012\n\n\nSergejs Gubins, Erik T. Verhoef, Thomas de Graaff”\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/articles/Donovan-2024-housing-economics/index.html",
    "href": "research/articles/Donovan-2024-housing-economics/index.html",
    "title": "An urban overhead? Crime, agglomeration, and amenity",
    "section": "",
    "text": "We study the effects of crime and agglomeration on urban amenity using data for 134 locations in New Zealand and report three key findings. First, the negative effects of crime operate mostly via rents, with elasticities that range from −0.15 to −0.44. Accounting for endogeneity leads to larger elasticities in some specifications, possibly due to sorting effects. Second, crime has negative effects on the value of urban amenities, with elasticities that range from approximately −0.03 to −0.06 for firms and −0.02 to −0.09 for workers. Using reduced-form models, we show that these effects imply an elasticity of population with respect to crime of −0.04 to −0.10. Third, controlling for crime causes estimates of agglomeration economies to increase by approximately 0.01–0.02 points, on average. Our findings confirm that crime is an important urban congestion cost that erodes productivity and well-being."
  },
  {
    "objectID": "research/articles/Donovan-2024-housing-economics/index.html#abstract",
    "href": "research/articles/Donovan-2024-housing-economics/index.html#abstract",
    "title": "An urban overhead? Crime, agglomeration, and amenity",
    "section": "",
    "text": "We study the effects of crime and agglomeration on urban amenity using data for 134 locations in New Zealand and report three key findings. First, the negative effects of crime operate mostly via rents, with elasticities that range from −0.15 to −0.44. Accounting for endogeneity leads to larger elasticities in some specifications, possibly due to sorting effects. Second, crime has negative effects on the value of urban amenities, with elasticities that range from approximately −0.03 to −0.06 for firms and −0.02 to −0.09 for workers. Using reduced-form models, we show that these effects imply an elasticity of population with respect to crime of −0.04 to −0.10. Third, controlling for crime causes estimates of agglomeration economies to increase by approximately 0.01–0.02 points, on average. Our findings confirm that crime is an important urban congestion cost that erodes productivity and well-being."
  },
  {
    "objectID": "research/articles/Donovan-2024-housing-economics/index.html#the-effect-of-crime-on-agglomeration-economies-in-consumption",
    "href": "research/articles/Donovan-2024-housing-economics/index.html#the-effect-of-crime-on-agglomeration-economies-in-consumption",
    "title": "An urban overhead? Crime, agglomeration, and amenity",
    "section": "The effect of crime on agglomeration economies in consumption",
    "text": "The effect of crime on agglomeration economies in consumption\nAn important finding of our paper is that controlling for crime causes our estimates of agglomeration economies in production and consumption, \\(E_d^y\\) and \\(E_d^u\\), to increase on average by 0.011 and 0.021 points, respectively. The magnitude of this increase is similar to that reported in (cf. Table 2 Donovan et al. 2022) when controlling for commuting costs, which implies that the negative effects of crime on agglomeration economies may be of a similar order of magnitude to those for road congestion—at least in the New Zealand context. Although the differences in our estimates of average agglomeration economies when controlling for crime are economically significant, they are not statistically significant. Nonetheless, we can draw on other evidence to confirm the effect of crime on agglomeration economies. The figure below illustrates this.\n\n\n\nLeft panel: Histogram of location-specific estimates of agglomeration economies in consumption, \\(E^u_{d,i}\\), from Donovan et al. (2022) where the dashed vertical line denotes the mean. Right panel: \\(E^u_{d,i}\\) (vertical axis) versus total victimisations, \\(\\ln V_i\\) (horizontal axis), where the vertical and horizontal lines denote 95% credibility intervals and the dashed vertical line denotes the population-weighted mean of \\(\\ln V_i\\)."
  },
  {
    "objectID": "research/articles/Donovan-2024-housing-economics/index.html#citation",
    "href": "research/articles/Donovan-2024-housing-economics/index.html#citation",
    "title": "An urban overhead? Crime, agglomeration, and amenity",
    "section": "Citation",
    "text": "Citation\n@article{donovan2024urban,\n  title={An urban overhead? Crime, agglomeration, and amenity},\n  author={Donovan, Stuart and de Graaff, Thomas and de Groot, Henri LF and Schiff, Aaron},\n  journal={Journal of Housing Economics},\n  pages={101994},\n  year={2024},\n  publisher={Elsevier}\n}"
  },
  {
    "objectID": "research/articles/bernasco-2016/index.html",
    "href": "research/articles/bernasco-2016/index.html",
    "title": "Social Interactions and Crime Revisited: An Investigation Using Individual Offender Rates",
    "section": "",
    "text": "Using data on the age, sex, ethnicity, and criminal involvement of more than 14 million residents of all ages residing in approximately 4,000 Dutch neighborhoods, we test if an individual’s criminal involvement is affected by the proportion of criminals living in his or her residential neighborhood. We develop a binomial discrete choice model for criminal involvement and estimate it on individual data. We control for both the endogeneity that may be related to unobserved neighborhood characteristics and for sorting behavior. We find significant social interaction effects, but our findings do not imply multiple equilibria or large multiplier effects"
  },
  {
    "objectID": "research/articles/bernasco-2016/index.html#abstract",
    "href": "research/articles/bernasco-2016/index.html#abstract",
    "title": "Social Interactions and Crime Revisited: An Investigation Using Individual Offender Rates",
    "section": "",
    "text": "Using data on the age, sex, ethnicity, and criminal involvement of more than 14 million residents of all ages residing in approximately 4,000 Dutch neighborhoods, we test if an individual’s criminal involvement is affected by the proportion of criminals living in his or her residential neighborhood. We develop a binomial discrete choice model for criminal involvement and estimate it on individual data. We control for both the endogeneity that may be related to unobserved neighborhood characteristics and for sorting behavior. We find significant social interaction effects, but our findings do not imply multiple equilibria or large multiplier effects"
  },
  {
    "objectID": "research/articles/bernasco-2016/index.html#illustration",
    "href": "research/articles/bernasco-2016/index.html#illustration",
    "title": "Social Interactions and Crime Revisited: An Investigation Using Individual Offender Rates",
    "section": "Illustration",
    "text": "Illustration\nWe use a sorting model to map propensity of crime to expected crime propensity yielding multiple equilibria (see below).\n\n\n\nMultiple equilibria for the propensity of crime"
  },
  {
    "objectID": "research/articles/wang-2016/index.html",
    "href": "research/articles/wang-2016/index.html",
    "title": "Cultural Diversity and Cultural Distance as Choice Determinants of Migration Destination",
    "section": "",
    "text": "This study analyses the impact of cultural composition on regional attractiveness from the perspective of international migrant sorting behaviour on a European regional NUTS1 level. We use an attitudinal survey to quantify cultural distances between natives and immigrants in the region concerned,and estimate the migrants’ varying preferences for both cultural diversity and cultural distance. To account for regional unobserved heterogeneity, our econometric analysis employs artificial instrumental variables,as developed by Bayer et al., [2004a. An equilibrium model of sorting in an urban housing market. NBER no. 10865]. The main conclusions are twofold. On the one hand, cultural diversity increases regional attractiveness. On the other hand, average cultural distance greatly weakens regional attractiveness"
  },
  {
    "objectID": "research/articles/wang-2016/index.html#abstract",
    "href": "research/articles/wang-2016/index.html#abstract",
    "title": "Cultural Diversity and Cultural Distance as Choice Determinants of Migration Destination",
    "section": "",
    "text": "This study analyses the impact of cultural composition on regional attractiveness from the perspective of international migrant sorting behaviour on a European regional NUTS1 level. We use an attitudinal survey to quantify cultural distances between natives and immigrants in the region concerned,and estimate the migrants’ varying preferences for both cultural diversity and cultural distance. To account for regional unobserved heterogeneity, our econometric analysis employs artificial instrumental variables,as developed by Bayer et al., [2004a. An equilibrium model of sorting in an urban housing market. NBER no. 10865]. The main conclusions are twofold. On the one hand, cultural diversity increases regional attractiveness. On the other hand, average cultural distance greatly weakens regional attractiveness"
  },
  {
    "objectID": "research/articles/wang-2016/index.html#figure",
    "href": "research/articles/wang-2016/index.html#figure",
    "title": "Cultural Diversity and Cultural Distance as Choice Determinants of Migration Destination",
    "section": "Figure",
    "text": "Figure"
  },
  {
    "objectID": "research/articles/wang-2018-barriers/index.html",
    "href": "research/articles/wang-2018-barriers/index.html",
    "title": "Barriers of Culture, Networks, and Language in International Migration: A Review",
    "section": "",
    "text": "Along with the increasing pace of globalization, recent decades faced a dramatic increase in international migrant flows as well. Compared to the flows of trade, capital and knowledge, we observe that contemporaneous complex institutional differences, historical backgrounds, and individuals’ diverse socio-demographic characteristics make the migrant workers’ choice of destination arguably much more uncontrollable. This study shows that migration is intertwined with culture, networks and language in a complex way, (i) by reviewing related studies on the barriers of culture, networks and language in international labor mobility, and (ii) by exploring missing gaps and prospective avenues for research. Nowadays, the migration pressure on Europe and the United states has created substantial challenges, leading to an urgent need to address the economic assimilation and social integration of migrants. Against this background, we emphasize that these non-economic factors have played an increasingly critical role in shaping international migration and its future socio-economic consequences for destination countries."
  },
  {
    "objectID": "research/articles/wang-2018-barriers/index.html#abstract",
    "href": "research/articles/wang-2018-barriers/index.html#abstract",
    "title": "Barriers of Culture, Networks, and Language in International Migration: A Review",
    "section": "",
    "text": "Along with the increasing pace of globalization, recent decades faced a dramatic increase in international migrant flows as well. Compared to the flows of trade, capital and knowledge, we observe that contemporaneous complex institutional differences, historical backgrounds, and individuals’ diverse socio-demographic characteristics make the migrant workers’ choice of destination arguably much more uncontrollable. This study shows that migration is intertwined with culture, networks and language in a complex way, (i) by reviewing related studies on the barriers of culture, networks and language in international labor mobility, and (ii) by exploring missing gaps and prospective avenues for research. Nowadays, the migration pressure on Europe and the United states has created substantial challenges, leading to an urgent need to address the economic assimilation and social integration of migrants. Against this background, we emphasize that these non-economic factors have played an increasingly critical role in shaping international migration and its future socio-economic consequences for destination countries."
  },
  {
    "objectID": "research/articles/wang-2018-barriers/index.html#figure",
    "href": "research/articles/wang-2018-barriers/index.html#figure",
    "title": "Barriers of Culture, Networks, and Language in International Migration: A Review",
    "section": "Figure",
    "text": "Figure\n\n\n\nSchematic overview"
  },
  {
    "objectID": "research/articles/donovan-2024/index.html",
    "href": "research/articles/donovan-2024/index.html",
    "title": "Unraveling urban advantages—A meta-analysis of agglomeration economies",
    "section": "",
    "text": "A large body of empirical literature considers the productive advantages of cities, or “agglomeration economies.” We present a meta-analysis of this literature that draws on 6684 agglomeration elasticities from294 studies spanning 54 countries and six decades. We find that elasticities are likely to lie in the range 0.015–0.039 and, like earlier reviews, that the controls enabled by detailed data are associated with smaller estimates. We make five main contributions: First, we find evidence that publication selection imparts a positive bias to the literature; second, we adopt novel methods that tend to yield more precise parameter estimates than conventional approaches; third, unlike earlier reviews, we do not find large differences between most countries nor an association with national income; fourth, we report new associations, for example between elasticities and the spatial scope of agglomeration, which may reflect underlying microeconomic channels; and fifth, we find evidence that elasticities for manufacturing sectors have fallen in recent decades, possibly due to lower freight costs and stricter environmental regulations.\n\n\n\nEstimated effect sizes"
  },
  {
    "objectID": "research/articles/donovan-2024/index.html#abstract",
    "href": "research/articles/donovan-2024/index.html#abstract",
    "title": "Unraveling urban advantages—A meta-analysis of agglomeration economies",
    "section": "",
    "text": "A large body of empirical literature considers the productive advantages of cities, or “agglomeration economies.” We present a meta-analysis of this literature that draws on 6684 agglomeration elasticities from294 studies spanning 54 countries and six decades. We find that elasticities are likely to lie in the range 0.015–0.039 and, like earlier reviews, that the controls enabled by detailed data are associated with smaller estimates. We make five main contributions: First, we find evidence that publication selection imparts a positive bias to the literature; second, we adopt novel methods that tend to yield more precise parameter estimates than conventional approaches; third, unlike earlier reviews, we do not find large differences between most countries nor an association with national income; fourth, we report new associations, for example between elasticities and the spatial scope of agglomeration, which may reflect underlying microeconomic channels; and fifth, we find evidence that elasticities for manufacturing sectors have fallen in recent decades, possibly due to lower freight costs and stricter environmental regulations.\n\n\n\nEstimated effect sizes"
  },
  {
    "objectID": "posts/post/krugman_neg/index.html",
    "href": "posts/post/krugman_neg/index.html",
    "title": "Krugman’s Increasing Returns and Economic Geography",
    "section": "",
    "text": "For educational purposes we teach in the second year’s course regional and urban economics a simplified version of Krugman’s model in his paper titled Increasing Returns and Economic Geography. The model we have adopted goes as follows:\nWe consider a simplified economy with two regions and 1 (million) workers ( \\(L=1\\) ) in total. Region 1 is inhabited by 100,000 farmers (bound to their land so immobile), while in Region 2 there are 200,000 farmers. Note that in the notation of Krugman this boils down to \\(\\pi_1 = 0.1\\) and \\(\\pi_2 = 0.2\\). All other workers work in manufacturing. Assume now that there is a representative firm that has to choose if and in which region if would settle or that it would settle in both regions by having two branches (one in each region). The fixed costs to establish a firm (or branch) is 0.15. The transportcosts to move goods between region 1 and 2 are equal to 1 and each worker consumer consumes exactly one unit of the final product.\nIt is now up to the student to determine the equilibria in this economy (whether stable or unstable) and identify the trade-off for the firm. Doing this the most insightful and simple way is draw the so-called PP-line and the MM-line. Using the Tikz package within Latex simplifies this enormously. The following code shows how this can be done.\n% Krugman91---Firm location in two regions \n% Author: Thomas de Graaff\n\\documentclass{article}\n\\usepackage{tikz, verbatim}\n\\usepackage{pgfplots}   %include other needed packages here\n\\usepackage[active,tightpage]{preview}\n\\PreviewEnvironment{tikzpicture}\n\\setlength\\PreviewBorder{0pt}%\n\\begin{comment}\n:Title: Krugman91---Firm location in two regions \n:Tags: Economic geography, economics, location behavior, \nmultiple equilbria\n:Author: Thomas de Graaff\n\\end{comment}\n\\begin{document}\n\n\\begin{tikzpicture}[scale=1,thick]\n\\usetikzlibrary{calc, intersections}   %allows coordinate calculations.\n\n% Define parameters\n\\def\\L{1}               % Total amount of workers (normalized)\n\\def\\Pa{0.1}            % Total amount of farmers in regio 1\n\\def\\Pb{0.2}            % Total amount of farmers in regio 2\n\\def\\x{1}               % Total demand (normalized)\n\\def\\F{0.15}            % Fixed costs to set up a plant\n\\def\\t{1}               % Transportcosts\n\n\\def\\Fa{\\F/(\\t*\\x)}\n\\def\\Fb{1-\\F/(\\t*\\x)}\n\\def\\Eq{(\\Pa/(\\Pa+\\Pb)}\n\\def\\Eqa{((\\Fa-\\Pa)/(1-\\Pa-\\Pb)}\n\\def\\Eqb{((\\Fb-\\Pa)/(1-\\Pa-\\Pb)}\n\\def\\Eqleft{min(\\Fa-\\Pa, 0)}\n\\def\\Eqright{max(\\Fb-(1-\\Pb), 0)}\n\n\\begin{axis}[\nrestrict y to domain=0:\\L,\nsamples = 1000,             \nxmin = 0, xmax = \\L,\nymin = 0, ymax = \\L,\nxlabel=$S_m$,\nylabel=$S_p$,\ny axis line style={-}, \nx axis line style={-},\ngrid=major,\nlegend pos=north west,\nlegend entries={45$^\\circ$ line,PP line, MM line}\n]\n\\addplot[dotted, mark=none, domain=0:\\L] {x};\n\\addplot[thick, red, mark=none, domain=0:\\L] coordinates {(0,\\Pa) (1,1-\\Pb)};\n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(0,0) (0,\\Fa)};          \n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(1,1) (1,\\Fb)};\n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(0,\\Fa) (\\Fa,\\Fa)};          \n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(1,\\Fb) (\\Fb,\\Fb)};\n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(\\Fa,\\Fa) (\\Fb,\\Fb)};\n\\addplot[thick, mark=*, fill=red!90] coordinates {(0,\\Pa+1000*\\Eqleft)};            \n\\addplot[thick, mark=*, fill=red!90] coordinates {(1,1-\\Pb+1000*\\Eqright)}; \n\\addplot[thick, mark=*, fill=red!90] coordinates {(\\Eq,\\Eq)};\n\\addplot[thick, mark=*, fill=red!10] coordinates {(\\Eqa,\\Fa)};              \n\\addplot[thick, mark=*, fill=red!10] coordinates {(\\Eqb,\\Fb)};          \n\\end{axis}\n\\end{tikzpicture}\n\\end{document}\nThis produces the following diagram:\n\n\n\nEquilibria in a stylized version of Krugman (1991)\n\n\nClearly, there are with this configuration 3 equilibria; two are stable and one is unstable."
  },
  {
    "objectID": "posts/post/krugman_neg/index.html#drawing-the-diagram-of-a-stylized-version-of-krugmans-increasing-returns-and-economic-geography",
    "href": "posts/post/krugman_neg/index.html#drawing-the-diagram-of-a-stylized-version-of-krugmans-increasing-returns-and-economic-geography",
    "title": "Krugman’s Increasing Returns and Economic Geography",
    "section": "",
    "text": "For educational purposes we teach in the second year’s course regional and urban economics a simplified version of Krugman’s model in his paper titled Increasing Returns and Economic Geography. The model we have adopted goes as follows:\nWe consider a simplified economy with two regions and 1 (million) workers ( \\(L=1\\) ) in total. Region 1 is inhabited by 100,000 farmers (bound to their land so immobile), while in Region 2 there are 200,000 farmers. Note that in the notation of Krugman this boils down to \\(\\pi_1 = 0.1\\) and \\(\\pi_2 = 0.2\\). All other workers work in manufacturing. Assume now that there is a representative firm that has to choose if and in which region if would settle or that it would settle in both regions by having two branches (one in each region). The fixed costs to establish a firm (or branch) is 0.15. The transportcosts to move goods between region 1 and 2 are equal to 1 and each worker consumer consumes exactly one unit of the final product.\nIt is now up to the student to determine the equilibria in this economy (whether stable or unstable) and identify the trade-off for the firm. Doing this the most insightful and simple way is draw the so-called PP-line and the MM-line. Using the Tikz package within Latex simplifies this enormously. The following code shows how this can be done.\n% Krugman91---Firm location in two regions \n% Author: Thomas de Graaff\n\\documentclass{article}\n\\usepackage{tikz, verbatim}\n\\usepackage{pgfplots}   %include other needed packages here\n\\usepackage[active,tightpage]{preview}\n\\PreviewEnvironment{tikzpicture}\n\\setlength\\PreviewBorder{0pt}%\n\\begin{comment}\n:Title: Krugman91---Firm location in two regions \n:Tags: Economic geography, economics, location behavior, \nmultiple equilbria\n:Author: Thomas de Graaff\n\\end{comment}\n\\begin{document}\n\n\\begin{tikzpicture}[scale=1,thick]\n\\usetikzlibrary{calc, intersections}   %allows coordinate calculations.\n\n% Define parameters\n\\def\\L{1}               % Total amount of workers (normalized)\n\\def\\Pa{0.1}            % Total amount of farmers in regio 1\n\\def\\Pb{0.2}            % Total amount of farmers in regio 2\n\\def\\x{1}               % Total demand (normalized)\n\\def\\F{0.15}            % Fixed costs to set up a plant\n\\def\\t{1}               % Transportcosts\n\n\\def\\Fa{\\F/(\\t*\\x)}\n\\def\\Fb{1-\\F/(\\t*\\x)}\n\\def\\Eq{(\\Pa/(\\Pa+\\Pb)}\n\\def\\Eqa{((\\Fa-\\Pa)/(1-\\Pa-\\Pb)}\n\\def\\Eqb{((\\Fb-\\Pa)/(1-\\Pa-\\Pb)}\n\\def\\Eqleft{min(\\Fa-\\Pa, 0)}\n\\def\\Eqright{max(\\Fb-(1-\\Pb), 0)}\n\n\\begin{axis}[\nrestrict y to domain=0:\\L,\nsamples = 1000,             \nxmin = 0, xmax = \\L,\nymin = 0, ymax = \\L,\nxlabel=$S_m$,\nylabel=$S_p$,\ny axis line style={-}, \nx axis line style={-},\ngrid=major,\nlegend pos=north west,\nlegend entries={45$^\\circ$ line,PP line, MM line}\n]\n\\addplot[dotted, mark=none, domain=0:\\L] {x};\n\\addplot[thick, red, mark=none, domain=0:\\L] coordinates {(0,\\Pa) (1,1-\\Pb)};\n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(0,0) (0,\\Fa)};          \n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(1,1) (1,\\Fb)};\n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(0,\\Fa) (\\Fa,\\Fa)};          \n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(1,\\Fb) (\\Fb,\\Fb)};\n\\addplot[thick, blue, mark=none, domain=0:\\L] coordinates {(\\Fa,\\Fa) (\\Fb,\\Fb)};\n\\addplot[thick, mark=*, fill=red!90] coordinates {(0,\\Pa+1000*\\Eqleft)};            \n\\addplot[thick, mark=*, fill=red!90] coordinates {(1,1-\\Pb+1000*\\Eqright)}; \n\\addplot[thick, mark=*, fill=red!90] coordinates {(\\Eq,\\Eq)};\n\\addplot[thick, mark=*, fill=red!10] coordinates {(\\Eqa,\\Fa)};              \n\\addplot[thick, mark=*, fill=red!10] coordinates {(\\Eqb,\\Fb)};          \n\\end{axis}\n\\end{tikzpicture}\n\\end{document}\nThis produces the following diagram:\n\n\n\nEquilibria in a stylized version of Krugman (1991)\n\n\nClearly, there are with this configuration 3 equilibria; two are stable and one is unstable."
  },
  {
    "objectID": "posts/post/research_agenda/index.html",
    "href": "posts/post/research_agenda/index.html",
    "title": "Do regional economists answer the right questions?",
    "section": "",
    "text": "The sexiest job in the next 10 years will be statisticians. (Varian 2014)\n\nThe quote above from Hal Varian is in one aspect wrong; nowadays, we do not call them statisticians but data scientists instead. Nevertheless, in the last two decades companies such as Google, Ebay, Whatsapp, Facebook, Booking.com and Airbnb, have not only witnessed enormous growth but to a considerable extent also changed the socio-economic landscape. Indeed, with the increasing abundance of (spatial) data and computer capacity, the ability to gather, process, and visualize data has become highly important and therefore highly in demand as well. And all the models and tools these data scientists within these companies use are very much data driven with often remarkable results.\nIn his controversial and path-breaking article, Breiman (2001) presented two different cultures in statistical science. One governed by a (probability) theory-driven modeling approach and one governed by a more (algorithmic) data-driven approach. These two cultures carry over to the econometric and ultimately the empirical regional economics domain  as well, where—commonly for all social sciences—the theory driven approach still very much dominates the landscape of the realm of contemporary regional economics.I use a wide definition for the regional economics domain, which consists of most aspects of regional science in general but for which the theoretical approach is always from an economic perspective. Topics such as, e.g, interregional migration, trade, transport flows and commuting on the one side and regional performance, regional clustering, population growth and specialisation on the other side fall all under this, admittedly, rather wide umbrella.\n\n\n\n\n\n\n\n\n\n\n\n(a) Model approach\n\n\n\n\n\n\n\n\n\n\n\n(b) Data approach\n\n\n\n\n\n\n\nFigure 1: Two cultures of statistical/econometric modeling (Insipired by Breiman 2001)\n\n\n\nFigure 2 is an adaptation from the one displayed in Breiman (2001) and describes the processes governing these two cultures. Figure 2 (a) is what I refer to as the modeling approach, where a statistical model is postulated and is central to this culture. This is the classical approach  where statistical probability theory meets the empiricism of Karl Popper. Usually the model assumed is stated as a linear model and in its most simple form can be denoted as:Sometimes as well referred to as the frequentists’ approach. However, this typically concerns the debate between classical statistics and Bayesian statistics, where the two approaches I refer to are more concerned with wider frameworks, of which the Bayesian approach is just one of the elements. I come back to Bayesian statistics and the frequentists’ approach later, but I do not see them necessarily as opposites. And I quite object to the term frequentists’ approach as Bayesian statistics is much more focuses on counting then the frequentists’ approach.To be honest, Popper himself was not a great fan of simply null hypothesis testing. He actually argued for the falsification of explanatory models, where in his view falsification does not only rely on statistics but on consensus amongst scientists as well.\n\\[\n    \\mathbf{y} = \\mathbf{x}\\beta + \\epsilon,\n\\tag{1}\\] where in (regional) economics language, \\(\\mathbf{x}\\) is referred to as the independent variable, \\(\\mathbf{y}\\) as the dependent variable and \\(\\epsilon\\) as a residual term. In this setup, using the data at hand, one constructs a statistical test to which extent the estimated coefficient (denoted with \\(\\hat{\\beta}\\)) deviates from a hypothesized value of the coefficient (denoted with \\(\\beta_0\\))—typically the hypothesis \\(H_0: \\hat{\\beta} = 0\\) is used with as alternative hypothesis that \\(H_1: \\hat{\\beta} \\neq 0\\). However, that is always within the context of the model. So, when the null-hypothesis is rejected, it not necessarily means that the true \\(\\beta\\) is unequal to zero, it might also be caused by errors in measuring \\(\\bf{x}\\) or even using the wrong ![One of the assumptions for regression techniques such as the one used here is actually no misspecification of the model, but—apart from some possible tests on the functional form a specific regression form—usually little attention is give on the validity of the model used. More importantly, within this framework the model itself is usually not tested .]There is another fallacy with this approach that is often overlooked and that is that the alternative hypothesis being true is a probability as well. Namely, most hypotheses researchers test are typically not very probable. Not taken this into account would actually lead to more null hypotheses to be rejected then should be (false positives).\nFigure 2 (b) yields a schematic overview of a more data driven approach. Here, we see an unknown model fed by predictors \\(\\mathbf{x}\\) that lead to one or multiple reponses \\(\\mathbf{y}\\). The main objective here is not to test hypotheses, but to find the best model instead which able to explain the data and to predict the data. Usually, the models are evaluated by some kind of criterion (e.g., the mean squared error), which is not completely unlike the modeling approach. However, there are two main differences between the two approaches. First, the data driven approach considers several models in a structural approach. For instance, the question which variables to include is captured by an exhaustive sourse of all combinations in the modeling approach (e.g., with classification and regression trees or random forests), while in the theory driven approach, the choice of variables is based on the theory and a small number of variations in the specification. Second, measurements on model performance are done in the data driven approach and, typically, in the model approach. The latter is not that important for hypothesis testing, but for prediction this matters enormously, because adding parameters might increase the in-sample fit, but actually worsen the out-of-sample fit (a phenomenon called overfitting).\nIn economics in general, and in regional economics in specific, most of the tools employed are very much theory or model driven instead of data driven. My (conservative) estimate would be that at least 90% of all empirical work in regional economics revolves around postulating a (linear) model and testing whether (a) key determinant(s) is (are) significantly different from a hypothesized value—usually zero. That is, the context of the model assumed.In a seminal contribution, Breiman (2001) states that deep into the 90s 98% of the statisticians actually employed the theory driven paradigm and only 2% a data driven paradigm. With the advent of the availability of internet connectivity, large (online) data sources, and faster computers the statistical realm changed dramatically. However, this has not permeated yet in the social sciences (see as well Varian 2014).\nAt best, this approach can be seen in a causal inference framework. If a determinant (such as a policy in the context of regional economics) \\(x\\) changes, does it cause then a change in the output \\(y\\) (most economists typically use some welfare measure). This approach thus provides a rigid and useful approach to regional policy evaluation. If we implement policy \\(x\\), does welfare measure \\(y\\) then improve? Note that this always considers a marginal change as \\(x\\) is usually isolated from other (confounding) factors.Most of this research actually intends to mimic a difference-in-difference approach and gained enormous momentum with the textbook of Angrist and Pischke (2008).\nHowever, policy makers oftentimes have different questions for which they need solutions. Usually, they revolve around questions starting with What determines performance measure \\(A\\)?, Which regions can we best invest in? or, more generally, What works for my region?. These types of questions require a different approach than the previous one. Namely, the former type requires an approach focused on explaining while the latter type requires an approach focused on predicting.\nThe remaining part of this position paper is structured as follows. Section 2 gives an overview of current modeling practices and describes the `traditional’ inference based approach as well as some data-driven approaches that have been used in the recent past (though by far not as often as the traditional methods). Section 3 sets out both a research and an education agenda as it addresses how to bridge the gap between the daily practices of regional economists and the demands of local policy makers. The final section shortly summarizes the main points raised in this position paper."
  },
  {
    "objectID": "posts/post/research_agenda/index.html#introduction-two-different-cultures",
    "href": "posts/post/research_agenda/index.html#introduction-two-different-cultures",
    "title": "Do regional economists answer the right questions?",
    "section": "",
    "text": "The sexiest job in the next 10 years will be statisticians. (Varian 2014)\n\nThe quote above from Hal Varian is in one aspect wrong; nowadays, we do not call them statisticians but data scientists instead. Nevertheless, in the last two decades companies such as Google, Ebay, Whatsapp, Facebook, Booking.com and Airbnb, have not only witnessed enormous growth but to a considerable extent also changed the socio-economic landscape. Indeed, with the increasing abundance of (spatial) data and computer capacity, the ability to gather, process, and visualize data has become highly important and therefore highly in demand as well. And all the models and tools these data scientists within these companies use are very much data driven with often remarkable results.\nIn his controversial and path-breaking article, Breiman (2001) presented two different cultures in statistical science. One governed by a (probability) theory-driven modeling approach and one governed by a more (algorithmic) data-driven approach. These two cultures carry over to the econometric and ultimately the empirical regional economics domain  as well, where—commonly for all social sciences—the theory driven approach still very much dominates the landscape of the realm of contemporary regional economics.I use a wide definition for the regional economics domain, which consists of most aspects of regional science in general but for which the theoretical approach is always from an economic perspective. Topics such as, e.g, interregional migration, trade, transport flows and commuting on the one side and regional performance, regional clustering, population growth and specialisation on the other side fall all under this, admittedly, rather wide umbrella.\n\n\n\n\n\n\n\n\n\n\n\n(a) Model approach\n\n\n\n\n\n\n\n\n\n\n\n(b) Data approach\n\n\n\n\n\n\n\nFigure 1: Two cultures of statistical/econometric modeling (Insipired by Breiman 2001)\n\n\n\nFigure 2 is an adaptation from the one displayed in Breiman (2001) and describes the processes governing these two cultures. Figure 2 (a) is what I refer to as the modeling approach, where a statistical model is postulated and is central to this culture. This is the classical approach  where statistical probability theory meets the empiricism of Karl Popper. Usually the model assumed is stated as a linear model and in its most simple form can be denoted as:Sometimes as well referred to as the frequentists’ approach. However, this typically concerns the debate between classical statistics and Bayesian statistics, where the two approaches I refer to are more concerned with wider frameworks, of which the Bayesian approach is just one of the elements. I come back to Bayesian statistics and the frequentists’ approach later, but I do not see them necessarily as opposites. And I quite object to the term frequentists’ approach as Bayesian statistics is much more focuses on counting then the frequentists’ approach.To be honest, Popper himself was not a great fan of simply null hypothesis testing. He actually argued for the falsification of explanatory models, where in his view falsification does not only rely on statistics but on consensus amongst scientists as well.\n\\[\n    \\mathbf{y} = \\mathbf{x}\\beta + \\epsilon,\n\\tag{1}\\] where in (regional) economics language, \\(\\mathbf{x}\\) is referred to as the independent variable, \\(\\mathbf{y}\\) as the dependent variable and \\(\\epsilon\\) as a residual term. In this setup, using the data at hand, one constructs a statistical test to which extent the estimated coefficient (denoted with \\(\\hat{\\beta}\\)) deviates from a hypothesized value of the coefficient (denoted with \\(\\beta_0\\))—typically the hypothesis \\(H_0: \\hat{\\beta} = 0\\) is used with as alternative hypothesis that \\(H_1: \\hat{\\beta} \\neq 0\\). However, that is always within the context of the model. So, when the null-hypothesis is rejected, it not necessarily means that the true \\(\\beta\\) is unequal to zero, it might also be caused by errors in measuring \\(\\bf{x}\\) or even using the wrong ![One of the assumptions for regression techniques such as the one used here is actually no misspecification of the model, but—apart from some possible tests on the functional form a specific regression form—usually little attention is give on the validity of the model used. More importantly, within this framework the model itself is usually not tested .]There is another fallacy with this approach that is often overlooked and that is that the alternative hypothesis being true is a probability as well. Namely, most hypotheses researchers test are typically not very probable. Not taken this into account would actually lead to more null hypotheses to be rejected then should be (false positives).\nFigure 2 (b) yields a schematic overview of a more data driven approach. Here, we see an unknown model fed by predictors \\(\\mathbf{x}\\) that lead to one or multiple reponses \\(\\mathbf{y}\\). The main objective here is not to test hypotheses, but to find the best model instead which able to explain the data and to predict the data. Usually, the models are evaluated by some kind of criterion (e.g., the mean squared error), which is not completely unlike the modeling approach. However, there are two main differences between the two approaches. First, the data driven approach considers several models in a structural approach. For instance, the question which variables to include is captured by an exhaustive sourse of all combinations in the modeling approach (e.g., with classification and regression trees or random forests), while in the theory driven approach, the choice of variables is based on the theory and a small number of variations in the specification. Second, measurements on model performance are done in the data driven approach and, typically, in the model approach. The latter is not that important for hypothesis testing, but for prediction this matters enormously, because adding parameters might increase the in-sample fit, but actually worsen the out-of-sample fit (a phenomenon called overfitting).\nIn economics in general, and in regional economics in specific, most of the tools employed are very much theory or model driven instead of data driven. My (conservative) estimate would be that at least 90% of all empirical work in regional economics revolves around postulating a (linear) model and testing whether (a) key determinant(s) is (are) significantly different from a hypothesized value—usually zero. That is, the context of the model assumed.In a seminal contribution, Breiman (2001) states that deep into the 90s 98% of the statisticians actually employed the theory driven paradigm and only 2% a data driven paradigm. With the advent of the availability of internet connectivity, large (online) data sources, and faster computers the statistical realm changed dramatically. However, this has not permeated yet in the social sciences (see as well Varian 2014).\nAt best, this approach can be seen in a causal inference framework. If a determinant (such as a policy in the context of regional economics) \\(x\\) changes, does it cause then a change in the output \\(y\\) (most economists typically use some welfare measure). This approach thus provides a rigid and useful approach to regional policy evaluation. If we implement policy \\(x\\), does welfare measure \\(y\\) then improve? Note that this always considers a marginal change as \\(x\\) is usually isolated from other (confounding) factors.Most of this research actually intends to mimic a difference-in-difference approach and gained enormous momentum with the textbook of Angrist and Pischke (2008).\nHowever, policy makers oftentimes have different questions for which they need solutions. Usually, they revolve around questions starting with What determines performance measure \\(A\\)?, Which regions can we best invest in? or, more generally, What works for my region?. These types of questions require a different approach than the previous one. Namely, the former type requires an approach focused on explaining while the latter type requires an approach focused on predicting.\nThe remaining part of this position paper is structured as follows. Section 2 gives an overview of current modeling practices and describes the `traditional’ inference based approach as well as some data-driven approaches that have been used in the recent past (though by far not as often as the traditional methods). Section 3 sets out both a research and an education agenda as it addresses how to bridge the gap between the daily practices of regional economists and the demands of local policy makers. The final section shortly summarizes the main points raised in this position paper."
  },
  {
    "objectID": "posts/post/research_agenda/index.html#sec-practices",
    "href": "posts/post/research_agenda/index.html#sec-practices",
    "title": "Do regional economists answer the right questions?",
    "section": "2 Regional economists turning the blind eye",
    "text": "2 Regional economists turning the blind eye\nUnmistakably, in the recent decade the two major changes to economic empirical research in general are the advent of increasingly larger data sources and the large increase in computer power (Einav and Levin 2014). The methods that most economists employ, however, have not changed. Linear regression or one of its close relatives (such as logistic, poisson or negative binomial regression), preferably in a causal framework, is still the most common tool. This also applies to regional economists, who—although coming from a tradition to use various methods from different disciplines—have increasingly used similar methods as in ``mainstream’’ economics.\nThis focus on marginal effects and causality is certainly very worthwhile and brought us many important insights. However, it is also typically done within a very narrow framework and, below, I will lay out what we are missing both in research and in our educational curricula, when our focus is on the framework above and as advocated so much as in Angrist and Pischke (2008).\n\n2.1 The blind eye in research\nThe traditional model of a (regional) economist looks as follows: \\[\n  y_i = \\alpha + \\beta x_i + \\mathbf{z}_i\\gamma + \\epsilon_i,\n\\tag{2}\\] where \\(y_i\\) is referred to as the dependent variable, \\(x_i\\) is the main variable of interest, and \\(\\mathbf{z}\\) is a vector of other variables. \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are parameters, where we are especially interested in the value of \\(\\beta\\). Finally, \\(\\epsilon_i\\) is an identical and independent distributed error term.\nUsually the main aim is to estimate \\(\\beta\\) as unbiased as possible in a causal framework. So, ideally, we would like to control for unobserved heterogeneity bias, specification bias, measurement error, reverse causality, selection bias, and so forth. Econometric theory has produced some very powerful techniques to control for some of these biases, such as instrumental variables, diff-in-diff procedures and the use of fixed effects. However, these methods are not panacea for everything. First, they work wonders for only specific research questions that have to do with the preferably causal effects of marginal changes. Second, some of these techniques require very specific and strong assumptions which are possibly not always met, which leaves doubts upon the validity of the results.\nBelow, I will deal with instrumental variables, diff-in-diff and fixed effect techniques consecutively. I will specifically focus on some of the disadvantages. Some of the arguments are adaptions from Deaton (2010) and I refer to this reference for a more complete treatise on the disadvantages of using instrumental variables and diff-in-diff methods. For all the advantages not dealt with in this paper, read Angrist and Pischke (2008).\n\n2.1.1 Exogeneity versus independence\nEconomists love instrumental variables, because a good instrumental variable can tackle reverse causality, measurement error and unobserved heterogeneity bias all at one. Originally, instrumental variables come from simultaneous economic models such as supply and demand models. A classical example in a regional context would be: \\[\n\\begin{aligned}\n  P_r &=  \\alpha + \\beta E_r + \\mathbf{z}_r\\gamma + \\epsilon_r, \\label{P}\\\\\n  E_r &=  \\delta + \\kappa P_r+ \\mathbf{w}_r\\lambda + \\nu_r,\\label{E}\n\\end{aligned}\n\\tag{3}\\] where \\(P\\) denotes population, \\(E\\) employment and \\(z\\) and \\(w\\) are vector of other regional \\(r\\) characteristics. \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\), \\(\\kappa\\) and \\(\\lambda\\) are parameters to be estimated.\nObviously, one can not directly estimate Equation 3 because of the intrinsic simultaneity. However, suppose one is interested in estimating the impact of employment on population growth, then one can use the second equation of Equation 3 and search for exogeneous variation in employment to use it as an instrumental variable. A possible strategy could be to look into the population changes of surrounding regions (but within commuting distance), as they might not have an impact of the population change in the current region (see Graaff, Oort, and Florax 2012a, 2012b)This is not really precise; I mean exogeneous to population variation. I will come back to the use of exogeneous later.\nThe main point , however, is that equations Equation 3 constitute a full-blown economic which has direct relations with underlying structural theoretical modeling frameworks [such as Roback (1982). And the instrument then comes directly (is internal) from the model.Directly from Deaton (2010).\nIn practice, however, researchers often take another approach. And that is to look for external instruments. Instruments that have no relation with a structural (simultaneity) model. And there are is (a large) potential pitfall when doing so and that is to end up with an instrumental variables that is not independent from the left-hand-side variable. As it seems, there is some confusion about terms as independence and exogeneity, so let’s first clarify the exact assumptions a valid instrument should satisfy.\nSuppose that somebody is interested in the impact of population on employment; so, one would like to identify \\(\\kappa\\) in Equation 3. To control for endogeneity researchers then search for an exogenous and relevant instrument, \\(Z_r\\). The latter indicates that the instrument has an impact on the possible endogeneous variable (\\(P_r\\)) and the former indicates that the instrument does not affect the left-hand-side variable (\\(E_r\\)), only via \\(P_r\\) and other instruments. In formal notation: \\(E_r \\perp Z_r|P_r, w_r\\). Thus, exogeneity means that the instrument and the left-hand-side variables are independent from each other conditional on the instruments.\nUnfortunately, exogeneity is often used as an argument for variables that are external to the system, denote a sudden shock or for phenomena that are considered to be exogenous in other fields (such as in geography). And this usually leads to instruments that do not satisfy the independence assumption. I will give three examples below.\nFirst, and very often use is the concept of deep lagging. So, in our case, we look for regional population say 100 years ago and use that as an instrument. It must be exogenous because we cannot change it, right? Well, it is definitely relevant, as regional population is remarkably resilient. Where people lived 100 years ago, they most likely live today. But, if we take model Equation 3 seriously, then the population 100 years ago, must at least have affected employment 100 years ago, and if population is resilient then most likely employment as well (and we even do not consider yearly temporal dynamics between population and employment). So, in all likeliness, employment and population 100 years are not (conditionally) independent.\nThe second type of instruments people often use are regional characteristics (preferably deeplagged as well), and specifically accessibility measures as road, railroads and canals. For a large audience the following story typically seems very plausible at first sight. At the end of the 19th century the large scale introduction of the railways enabled households to live further from home and escape the heavilty polluted inner cities where the factories remained (making use of the same railroads intersecting in city centres). Railroads thus changed the location of population and not that of employment. While this story is entirely possible, what is often overlooked is the fact that factories and thus employment changed location as well, but only 20-30 years later, and typically along the same links as opened up by railway lines. So, the railway network 140 years ago and contemporary location of employment are not (conditionally) independent.\nA last often used category of candidate instruments is geography-related variables. In our case that could be regions of municipalities. For instance, the Netherlands witnessed for a large period intensive population location policies. This entailed that the dutch government pointed out municipalities that were allowed to grow (in terms of housing policies). Using fixed effects of these specific municipalities then as instruments sound as a viable strategy. However, this requires strict assumptions. Namely, being a specific municipality will only have an effect on employment through being designated by the Dutch government; and by nothing else.\nIs this to say that instrumental variables is a bad technique? No, absolutely not. If the instrument is valid, this is one of the most powerful techniques in the econometric toolbox. The point made here is that good instruments are actually hard to find and that structural simultaneous models (typically, in the context of supply and demand) usually work better to find instruments than instruments that are completely external to your problem. And if you really need to use an external instrument, be very specific and open about the assumptions you need to make.\n\n\n2.1.2 Local and average treatment effects\nTwo concepts which have received quite some attention recently in econometrics, but is often overlooked in applied regional and urban economics are local average treatment effects and average treatment effects. The former deals with the interpretation of instrumental variables, the latter with the (interpretation) of spatial difference-in-difference methods. Even though these methods are different, they have similar consequences for the interpretation of research findings and their underlying assumptions.\nThe Local Average Treatment Effect (LATE) deals with the underlying assumptions that have to be made so that the instrumental variable estimation actually measures what we want [see Imbens and Angrist (1994). Referring again to our example above and say that we want to instrument regional population changes with municipalities being designated by a national policy to increase local housing supply. What we then actually measure is the effect of changes in population on employment of those municipalities that have actually complied with the national policy. Municipalities that dropped out in an earlier stage are not taken into account, but municipalities who did comply but never implemented the policy are.\nSo, what is actually measured is the designation of municipalities to a policy, which might be a very interesting research question indeed, but in all likelihood does not necessarily coincide with the coefficient \\(\\kappa\\) in model Equation 3 above. In almost all cases the LATE theorem points at a more restrictive effect (and thus interpretation) of the instrumental variable than the model sets out to estimate. Only under very strong assumptions—homogeneity of regions, perfect compliance, and so forth— the coefficient by the instrumental variable coincides with the coefficient of the structural model.\nA different but related issue is that of the average treatment effects. Since the seminal work of Angrist and Pischke (2008) difference-in-difference methods (and all its variants) gained enormously in popularity. As well as in regional economics where spatial difference-in-difference are applied as often as possible. The idea itself is rather straightforward and originates from the search for semi-experimental randomized controlled trials (RCT’s).\nFor the regional domain, assume the following: there is one group of municipalities that implement a policy (the treatment; \\(T = 1\\)) and one group of municipalities that does not (\\(T = 0\\)). Both groups of municipalities are measured before (\\(t = 0\\)) and after implementation (\\(t=1\\)). Then we can rewrite model Equation 2 as:\n\\[\n  y_r = \\alpha + \\gamma_1 T_r + \\gamma_2 t_r+ \\beta (T_r \\times t_r) + \\epsilon_r,  \n\\tag{4}\\]\nwhere \\(Y_r\\) denotes a specific regional outcome variable, \\(T_r\\) the set of regions that are treated and \\(t_r\\) the post implementation period. In this set-up \\(\\gamma_1\\) measures the average impact of the treated regions, \\(\\gamma_2\\) the impact of the time period, and \\(\\beta\\) is our coefficient of interest; being the impact of the treatment. Note that \\(\\beta\\) in this setting actually denotes the difference in the outcome of the treatment groups minus the difference in the outcome of the non-treated groups: hence, the name differences-in-differences.\nThe main assumption for this technique relies on the trueness of randomization of treatment across, in our case, municipalities. In reality, the concept of randomization is difficult to defend. Poor regions are more likely to receive governmental subsidies, accessibility improvement are usually implemented in dynamic and succesful regions, and so forth. To circumvent this, researchers look at borders between regions. Back to our example, we then look at individuals close to a border between two municipalities, where one municipality received a treatment and the other did not. It is then defendable that such a band around a border is relatively homogeneous in characteristics an that both regions are thus equal except for the receivement of treatment.\nThis approach has two main consequences. The most mentioned consequence is that the effect \\(\\beta\\) is a so-called mean treatment effect. Every region, firm or individual benefits (is harmed by) equally from the treatment. So, it might very well be that some regions benefit greatly from some a policy, while it is actually harmful for others. Making the treatment effect more heterogenous is difficult and requires a lot from the data as every subgroup needs its own semi-experimental randomized control trial.\nExtending this argument to spatial difference-in-difference methods leaves us even with the assumption that the whole region should be alike the border area in term of benefitting from the policy. Or one should be satisfied with the fact that \\(\\beta\\) only tells us something about the effect of the policy in a border area. An area most likely not very representative of the rest of the region.\nThe other consequence relates again to the compliance assumption. Regions and municipalities themselves can be argued to fit well in treatment or non-treatment groups. And if not, non-compliance should be easily detected. However, for firms and individuals, compliance to randomization of treatment is often a very harsh assumption. More and more, evidence is found that especially individuals are very resourceful to circumvent randomization, whether it by allocation to class sizes, elementary schools, or even military draft by lottery.\nRandomized controlled trials and difference-in-difference methods are strong techniques for the applied regional economist. The point here is, however, that without very strong assumptions, findings are mean effects that only applied to a limited part of the total sample.\n\n\n2.1.3 Fixed effects and heterogeneity\nAn often used technique in applied econometrics is the use of fixed effects. They work brilliantly in removing unobserved heterogeneity but they come at a price which is typically overlooked. Namely, they remove valuable variation as well in both the dependent (predictor) \\(x\\) and the independent (reponse) variable \\(y\\).\nConsider the following model in Equation 5, which is at the moment a heavily researched issue in both regional and urban economics. The issue here is to what extent city density increases individual productivity.\n\\[\n  \\ln(w_{ic}) = \\alpha + \\beta \\ln(d_{ic})+\\epsilon_{ic},\n\\tag{5}\\]\n\\(w_{ic}\\) denotes here individual wages (as a proxy for productivity) and \\(d_{ic}\\) density of the city \\(c\\) individual \\(i\\) lives in. \\(\\beta\\) is our parameter of interest and because of the log-log structure \\(\\beta\\) denotes an elasticity. Obviously, direct estimation of Equation 5, would lead to a misleading parameter \\(\\beta\\) if one is aiming to measure a causal effect. Namely, \\(\\beta\\) might be influenced by other (confounding) factors than only city density. One can think of factors such as skill level of the city population, accessibility of the city, sector structure of the city and city government. Moreover, a phenomenon called sorting might occur, where more ambitious, risk-seeking and high-skilled people migrate into larger and more dynamic cities.I specifically do not use the term here. Namely, Equation 5 is a perfectly fine model to measure the overall correlation (\\(\\beta\\)) between city density and individual wages and mosts non-economists are perfectly fine with this model (see, e.g., Bettencourt and West 2010). So, whether a parameter is biased depends ultimately upon the research question.\nTo answer the question to what extent density causes wages, researchers therefore resolved to using fixed effects. A baseline model can be seen in Equation 6.\n\\[\n  \\ln(w_{ic}) = \\nu_i + \\xi_c + \\beta \\ln(d_{ic})+\\epsilon_{ic},\n\\tag{6}\\]\nhere, \\(\\nu_i\\) denotes individual \\(i\\) specific fixed effects and \\(\\xi_c\\) city \\(c\\) specific fixed effects. So, everything that does not vary over time for individuals and cities is now controlled for. A more insightful way what exactly happens is to write Equation 6 in changes, such as: \\(\\Delta \\ln(w_{ic}) =\n\\beta \\Delta\\ln( d_{ic}) + \\epsilon_{ic}\\). So, our Equation 6 now identifies the effect by looking at the impact of a change in density on a change in wages .Using changes (first differences) to remove fixed effects is a viable but often overlooked technique for dealing with fixed effects.\nMultiple improvements have already been to this model including controlling for sector/task of work and migrating between cities. Including these fixed effects (and many more) has had a profound effect on the value of \\(\\beta\\). Directly estimating Equation 5 yields an elasticity of around \\(1.15\\), while estimating a model such as Equation 6 including many fixed effects would yield an elasticity of around \\(1.02-1.03\\). So, there are economies of agglomeration, but they are not very large.\n\n\n\n\n\n\n\n\n\n\n\n(a) fixed effects\n\n\n\n\n\n\n\n\n\n\n\n(b) heterogeneity in \\(\\beta\\)\n\n\n\n\n\n\n\nFigure 2: Heterogeneity in levels versus slopes\n\n\n\nIs this now the end of the story? Alas, it is not. At least three remarks can be made which put the above into perspective.\nFirst of all, note that we need changes over time—in our case in individual wages and city density. Now, if we take the extreme example of a subgroup of individuals who do not face wage changes and cities who remain relatively of equal size, than this subgroups will not be used for determination of \\(\\beta\\). Of course, not many observations will have these characteristics. Unfortunately, with more detailed data on sector structure and migration, we need individuals that move both residence and job for identification. All others are redundant. This increases the risk on what is called sample selection bias—identification is based on a specific subgroup with deviant characteristics. The point made here, is that with the use of many fixed effects, much is demanded from the data and one need always check whether the sample used for identification is not too restrictive.\nSecondly, if there are unobserved factors that both relate to wages and density, then it is actually very likely that these unobserved factors are related to their changes as well. One particular example here is technological change, which might affect density (suburbs) and wages at the same time, and is definitely not time-invariant. If one thinks about it, most interesting socio-economic phenomena are not time-invariant, except perhaps longitude and latitude. For example, a specific argument to use fixed effects is to control for local attractivity. But what individuals and firms find attractive does change of time, especially within cities, but across cities as well. Before air-conditioning cities in Florida and Nevada were definitely not as popular as today. And malaria-rich areas such as wetlands and river banks were always avoided until recently.\nThirdly, the use of fixed effects is based upon the assumption that all variation is based on variation of levels. That is, each fixed effect actually denotes a very specific constant (for each individual and city in our case). However, this really requires a very homogeneous sample except in levels. For illustration, assume that there are three individuals, where individual 3 has higher wages than individual 1 and 2, because of, say, differences in skill levels (see as well Figure 2 (a)). However, as Figure 2 (a) clearly shows as well, apart from individual level variation, returns to density are similar for individuals 1, 2 and 3. So, each individual benefits equally from moving from a small village not a large metropolitan area. Now, assume that individuals are different with respect to the returns by living in large and denser cities. Then the impact \\(\\beta\\) should also differ among individuals as is illustrated in Figure 2 (b). This is not an argument to say that using fixed effects is wrong. But if the sample might be heterogenous, i.e. that units respond differently to different predictors, then using fixed effect might not yield a complete pictures and in some specific cases even a distorted picture.\nFixed effect techniques is a must have for every empirical regional economists. However, the message I would like to convey here is that it does not remove time-invariant unobserved heterogeneity (of which there is more than most researchers realise), is not very suitable for tackling heterogeneity in your main effect and might lead in some cases to sample selection bias.\n\n\n\n2.2 The blind eye in education\nSo, if the main instruments of regional economists are not always applicable and we miss tools in our toolbox to tackle, e.g., heterogeneity, prediction and non-marginal changes, how do we then fare in teaching? Are the students who now graduate equipped with the right toolbox that they use as well in their later careers? And do we have a consistent curriculum using similar or complementary tools running from the bachelor to the graduate studies? These types of questions are not frequently asked, and, if at all, not very well met. Mostly because of vested interests of departments and researchers.\nIn this subsection I will, however, try to answer partly some of these questions and identify what is missing in our curriculum. I will first look at the traditional applied econometrics approach and then to the (non-existence) of other courses geared towards data science, including the use of statistical software.\n\n2.2.1 Statistics & Applied Econometrics\nIn contemporary economic bachelor curriculae students typically follow one applied statistics course, where some hands-on experience is offered by working with small datasets—typically in menu driven statistical software such as SPSS or STATA. In the master phase, if students start to specialise in, e.g., regional economics, students then follow one applied econometrics course with an emphasis on regression techniques, instrumental variables and the use of fixed effects.\nThe statistics courses are very much geared towards traditional socio-economic research where a hypothesis is formed (usually the difference between two groups not being zero), data is gather (via survey techniques) and statistical tests are applied on the difference between two groups (usually with the use of \\(t\\)-tests).\nFor most students, especially applied statistics feel as a very mechinal procedure using a pre-defined set or recipes. McElreath (2016) introduced a nice analogy with the old folkore of the Golem of Prague. The Golem was a mindless robot of clay that obeys orders. Scientists also use golems, especially with statistical procedures, where the tests or the estimations one performs are small golems in themselves. A mindless procedure that obeys what you tell them do. Sometimes for the better, sometimes for the worse.\nFor students this is not completely unlike: if you have this procedure, with these data, you should use this test—if not, use that test. Why to use that test is not an issue, one just follows a particular scheme and deploys one’s own golem. Figure 3 gives a typical example of such scheme or flowchart for the usage of statistical tests.\n\n\n\n\n\n\nFigure 3: An example of a flowchart for statistical tests.\n\n\n\nWhat is problematic with this approach is that students never completely understand what they are doing. Throughout their bachelor (and master) years, the relation between test statistics, \\(p\\)-values, significance level and confidence levels is typically lost on them.\nFor a large part, confusion amongst students is caused by the fact that (classical) statistics at the bachelor level is in way rather counter intuitive. Take, e.g., the following two statements about the 95% confidence interval.\n\nA 95% confidence interval means that for a given realized interval there is a 95% probability that the population parameter lies within the interval.\n\n\nWith numerous repeated samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 95%.\n\nMost students—in fact the audience at large and most scholars as well—would choose the first statement as being true for the 95% confidence interval. But in fact, the first statement is wrong and the second is true. The confidence interval is only formed by the (often implicit) assumption of numerous (infite) sampling. It does not resemble a statement about a probability of the population parameter even though most us feel intuitively that that should be the case.\nThese concepts of sampling and the associated confusion unfortunately, carry directly over to the applied econometrics domain. However, usually students find applied econometrics easier as less emphasis is put on the statistical background of the estimators. Unfortunately, applied econometrics only comes back in non-methods courses in the master phase, less so in the bachelor years, even though concepts as regression is taught in the first bachelor. This typically leaves bachelor students with a small amount of experience and less to none intuition when it comes to applied (econometric) work with empirical datasets.\nas I will argue in the next section there are other ways of teaching students concepts of statistics and probabilities which rely less on sampling and more on counting instead. However, for this, computers and statistical software packages are needed, but then at least we can make statement as the first one above, which feel far more intuitive.\n\n\n2.2.2 Data science\nIn addition to statistics and applied econometrics, students are now offered a (data science) programming language as well in the bachelor, mostly R of Python. They usually only learn the basics and typically do not work with databases, datascience of modeling techniques in these type of courses. And, unfortunately, subsequent bachelor courses do not use these programming languages for their exercises. This renders the added value of these courses quickly to zero.\nMaster courses now use more and more data science techniques and languages, although—in all honesty—typically outside the domain of (regional) economics. Unfortunately, without a solid background in dealing with data management and newer and more applied concepts of statistics, students approach these forms of techniques (e.g., classification and regression trees) again as mechanical golems by following recipes without truly understanding the underlying theory."
  },
  {
    "objectID": "posts/post/research_agenda/index.html#sec-agenda",
    "href": "posts/post/research_agenda/index.html#sec-agenda",
    "title": "Do regional economists answer the right questions?",
    "section": "3 Incorporating the data science culture agenda",
    "text": "3 Incorporating the data science culture agenda\nThe previous section discussed contemporary and cutting-edge applied econometric methods of (regional) economists. As argued, these methods have merits. Not least because they are all geared towards identifying causal relationships, and to a far greater extent then in other social sciences.\nHowever, these methods do come at some costs. First of all, the results should be interpreted as marginal effects. A small change in \\(x\\) causes a certain change in \\(y\\). Second, the effect is always ceteris paribus. All possible other factors are controlled for. Third, most of these methods face difficulties with heterogeneous effects. Fourth, and final, the underlying statistical framework is often difficult to interpret—for students, scholars, and the audience at large.\nThese disadvantages do have serious consequences for what this traditional toolkit can and what it cannot. First of all, it is very good in explaining but very bad in predicting. Second, system-wide changes and impacts are difficult to incorporate. Third, it has difficulties with different heterogeneous subgroups. Fourth, the underlying statistical framework makes it difficult to evaluate models. And, as last, the statistical framework also make it difficult to deal with non-linear models and non-parametric techniques are difficult to yield with this specification.\nBelow, I first explain how using techniques from the data driven approach side, or the data science side, can help research in the field of regional economics further in three directions: model comparison, heterogeneous effect sizes, and predicting.\nThereafter, I describe what needs to be changed in education, so that future students will better enabled to deal with the abundance of larger datasets, the need for better predictions and a more intuitive understanding of probabilities and testing.\n\n3.1 In research\n\n3.1.1 Dealing with heterogeneity\nOne of the weaknesses of the theory driven approach—or the more classical research methods—is dealing with heterogeneity. Fixed effects regressions only deal with removing level effects and not varying slope effects, difference-in-difference designs only give average treatment effects and instrumental variables have difficulties with incorporating heterogeneity.There are some advances made in introducing heterogeneous instruments in quantile regression techniques, but the exact mechanism is still not clear-cut.\nThe argument made against heterogeneity is that it only affects efficiency (i.e., the standard errors), but in most cases this is not true. In non-linear models, such as discrete choice and duration models heterogeneity affects the consistency (i.a., the parameter of interest) as well. Moreover, interpretation of the parameter of interest might be completely off when not allowing for heterogeneous groups.\n\n\n\n\n\n\nFigure 4: A mixture of two distributions of housing prices\n\n\n\nConsider Figure 4 where in the left panel a density distribution is given from a distribution of lognormally distributed housing prices. When interested in explaining the effect of a variable \\(x\\) on housing prices this is typically the first descriptive plot an applied researcher creates. The middle panel enlights this plot further by combining both a density plot and a histogram. But what if the sample consists of types of housing markets. One overheated and one with ample housing supply. Then most likely the mechanism on both markets are different and the effect \\(\\beta\\) could be very well different for both markets. Indeed, the right panel shows that the density distribution from the left panel is actually a realization of a mixture of two (in this case normal) distributions. The housing market with ample supply of houses is then represented by group 1 and the overheated housing market is represented by group 2.\nThese latent class approaches are typically not much applied in (regional) economics (see for an exception, e.g., Lankhuizen, De Graaff, and De Groot 2015). However, correct identification of submarkets or subgroups could be very important for policy makers as the average treatment effect may very well not even apply to anyone (an argument, in Dutch, made as well in Graaff 2014).In other economic or social science fields, such as market and transportation science, however, this is already a common approach.\nSlowly, the notation that fixed effects contain much useful information permeated in the regional economics domain. An insightful and straightforward way to do this is by adapting the wage model in Equation 6 as follows:\n\\[\n\\begin{align}\n  \\ln(w_{ic}) &= \\xi_c + \\beta \\ln(d_{ic})+\\mathbf{z_{ic}}\\gamma + \\epsilon_{ic} \\notag \\\\\n  \\xi_c&=\\alpha + \\mathbf{x_c}\\delta + \\mu_c,\n\\end{align}\n\\tag{7}\\]\nwhere the individual wage model is now split up in two stages. The first stage model the individual variation and regional fixed effects. The second stage now regresses regional variables on the estimated fixed effects. This approach is now frequently applied (for example, in the so-called sorting model Bayer, McMillan, and Rueben 2004; Bayer and Timmins 2007; Zhiling, Graaff, and Nijkamp 2016; and Bernasco et al. 2017).\nTwo large advantages of this approach are that the standard errors on both the individual and the regional level are correctly estimated and that, if needed, instrumental variable methods may be applied in the second stage. There is one disadvantage and that is the fixed effects in the second stage are not observed but estimated (imputed) and that has an effect on the standard errors.\nNote that model Equation 7 is very much alike multilevel models, which are very often used both in the data driven approach and in other social science apart from economics. Multilevel modeling works great in both correctly estimating a model with observations on various levels (such as individuals, firms, sectors and regions) and in retrieving heterogeneous estimates (both in levels and in slopes). And with the increasing advent of micro-data, combining a individual-regional model as in Equation 7 with the more rigorous structure of multilevel modeling is definitely worth more attention in the near future.\nInterestingly, more (spatial) non-parametric approaches (see, e.g., the geograpically weighted regression exercise in Thissen, Graaff, and Oort 2016) have become more popular as well in the last decade (typically, because of increased computer power). This approach needs more attention as well, as the connection with (economic) theory is often lost. And, especially regional geographers apply spatial non-parametric techniques, not the regional economists.\n\n\n3.1.2 Model comparison\nOne element that is notoriously weak in the theory-driven approach is model comparison—or the models should be nested. And in many cases, model comparison is often very much asked by policy makers and the audience at large, if not only for finding the correct specification. The latter question is concerned with the question which variables (predictors) to include in a model and which predictors of them perform best. Note that this is analogous to questions regional policy makers might have: such as, which policy instruments best to deploy given limited financial budgets.\nA typical example can be found in the field of spatial econometrics model where comparison is an important issue as typically there are several competing theories, non-nested, for the distance decay function (usually measured with a so-called spatial weight matrix \\(\\mathbf{W}\\)). And usually those theories are very much related (e.g., distance decay measured in Eucledian distance or generalized travel costs).\nAnother field where model comparison is of large importance is in the estimation of the strength of socio-ecoomic networks. In theory, socio-economic networks should produce so-called power-laws: or a loglinear relation between the size of nodes and the number of connections. Empirically, these relations often follow a slightly different distribution. What kind of distribution fits then best is still a matter of debate.One of the most famous of these relations is Zipf’s law, where the ordering of cities and the size of the population follows an almost perfect loglinear distribution; see for an in-depth treatment Gabaix (1999).\nFor proper model comparison, a Bayesian approach is almost unavoidable. The key difference between the frequentist and the Bayesian approach is how to interpret uncertainty. In the frequentist approach uncertainty originates from sampling, while in the Bayesian approach uncertainty is caused by not having enough information. So, a Bayesian statistician lives in a deterministic world but has a limited observational view.. Note that the rule of Bayes is not unique for Bayesian statistics. Namely, this rule is central for all probability theory.Both frequentists and bayesians rely heavily on sampling. However, sampling in frequentist statistics is a device to construct undercertainty around an estimate. Sampling in Bayesian statistics is a way to perform integral calculus (or to simulate observations).\nWhat is unique for each Bayesian model is that it has a prior and posterior. The prior is an assumption about something that you do not know (uncertainty measured by a parameter). With additional information (data), knowledge about the uncertainty is then updated (and hopefully the uncertainty is diminished). The updated probabilities are represented in a posterior distribution. To understand the probabilities then is simply a matter of sampling from the posterior distribution. So, the frequentist approach typically give a point estimate of a parameter, the Bayesian approach gives the whole distribution of the parameter. Note that under the Bayesian paradigm, everything (including the data) is regarded as an variable with associated uncertainty.\n\\[\n\\begin{align}\n  \\ln(h_r) & \\sim \\text{Normal}(\\mu_r, \\sigma) \\tag{likelihood}\\\\\n  \\mu_r & = \\alpha + \\beta x_r \\tag{linear model}\\\\\n  \\alpha & \\sim \\text{Normal}(12,3) \\tag{$\\alpha$ prior}\\\\\n  \\beta & \\sim \\text{Normal}(5,10) \\tag{$\\beta$ prior}\\\\\n  \\sigma &\\sim \\text{Uniform}(0,2) \\tag{$\\sigma$ prior}\n\\end{align}\n\\tag{8}\\]\nEquation 8 gives an example of a Bayesian linear regression model.. Here, we want to model the relation between regional housing prices (\\(h_r\\)) and the regional percentage of open space (\\(o_c\\)). Note that all parameters and the distribution of the data (likelihood) require distributional assumptions. This is a disadvantage in relation to the inference based frequentist approach, where no distributional assumptions are needed. But, note as well that Equation 8 specifies explicitly all assumptions for this model (e.g., a linear model and a normal distribution for the likelihood). If you think the model is incorrect you can rather easily change the assumptions.Under relatively mild assumptions this should yield similar results as the frequentist approach.\nEstimating Bayesian models have always been computationally cumbersome, especially with more parameters as sampling from the posterior distribution equalizes sampling from a multi-dimensional integral. Fortunately, computational power has increased dramatically in the last decades and techniques for sampling from the posterior distribution have become rather efficient (the most often used techniques nowadays are Monte Carlo Markov Chain algorithms which is basically a simulation of the posterior distribution).\nAlthough Bayesian statistics has already been applied to spatial econometrics (see the excellent textbook of LeSage and Pace 2009), applications have not permeated much to other fields in regional economics, such as in regional growth estimations, individual-regional multi-level modeling and population-employment modeling.\n\n\n3.1.3 Predicting\nA last field not well developed in (regional) economics is that of predicting. Most economists would shy away from predictions as, in their opinion, identifying causal relations is already difficult enough (they have a point there). What economists love to do is giving counterfactuals instead. For example, if regional open space would decrease significantly, what would happens with regional housing prices. This counterfactual approach looks very much as a prediction, however there are two large disadvantages associated with counterfactuals.In popular media, though, they are less hesitant to offer predictions.\nFirst, counterfactual are always made in sample. Actually, all marginal effects are made in-sample. Splitting the sample in a training set and a test set is not something that (regional) economists are prone to do. There is an intrinsic worry then for , especially when using many fixed effects. Explanatory power may be very high, but could also be very situation related. What works in one region, does not necessarily works in another region. Note that predicting in spatial settings is more difficult as the unit of analysis is typically a spatial system. And sub-setting a spatial system in a training and test set is often difficult.\nConsider, e.g., the following often used gravity model in linear form as depicted in Equation 9:\n\\[\n  \\ln(c_{ij}) = \\alpha + \\beta \\ln(P_i) + \\gamma \\ln(E_j) + \\ln(d_{ij}) + \\epsilon_{ij}.\n\\tag{9}\\]\nHere, we aim to model the number of commuters (\\(c\\)) from region \\(i\\) to region \\(j\\), by looking at the total labor force \\(P_i\\) in region \\(i\\), the total number of jobs \\(E_j\\) in region \\(j\\) and the distance (\\(d_{ij}\\)) between the two regions. Suppose, we can improve the distance between region \\(i\\) and \\(j\\) by, e.g., enlarging highway capacity. This does not only change the commuter flow between \\(i\\) and \\(j\\), but also between other region; say between \\(i\\) and \\(k\\). As usual there is no free lunch and total employment and population in each should remain constant, at least in the short-run.\nHowever, this make sub-setting difficult and correctly predicting cumbersome. But, Equation 9 of above is just one example of a large class of models that all face this difficulty. And policy makers (and firms) are actually very much interested in the questions associated with these predictions. Questions related to the impact of Brexit on other countries, total network effects of infrastructure improvements, identifying profitable routes for airlines, impact of housing projects on commuting quickly come to mind. So, it is especially the relation between predicting and spatial (interaction) systems that need considerable attention.\nA second problem with the counterfactual approach is that it considers marginal changes. Unfortunately, in models as Equation 9 this would not work. A marginal change on the link between \\(i\\) and \\(j\\) would have marginal changes on most other links. Marginal changes in a network setting is still a relatively underdeveloped area.That is, in applied empirical statistical work. Computational equilibrium models and to a lesser extent input-output models are able to model network-wide impacts. However, these models are cumbersome to construct and are less based on data.\nSo, on of the main research challenges in the regional economic domain for the near future would be to combine the data science models with the concept of spatial interaction models in such a way that both predictions can be made and that model restrictions are still satisfied.\n\n\n\n3.2 In education\nAs discussed above, in regional economics—in fact, in all social sciences—introductory statistics courses are like cook books. In this situation you need that recipe (test), in that situation you need that recipe (test). These recipes even perfectly coincide with the drop-down menu from certain, grapically user interface driven, statistical software packages such as SPSS. This causes students not to understand the underlying mechanism but just to apply procedures (or actually push buttons).\nWithout going into the need for using a frequentist or a Bayesian approach (they coincide more than most people think), I would actually argue very much for already using computers and coding in an early phase in students’ education. This could coincide with more traditional probability theory, but has an advantage that it is a general approach instead of a flowchart.\n\n\n\n\n\n\nFigure 5: Overlapping normal distributions\n\n\n\nAs an example, consider Figure 5 where two normal distributions are depicted. The left one has a mean of \\(-1\\), the right one has a mean of \\(1\\). Both have a standard deviation of 1. The question is now to what extent these distributions are different from each other. (Slightly rephrased: this is actually the problem whether two coefficients in a frequentist framework are different from each other). One approach is to search for a suitable test (and quickly run in a plethora of \\(F\\), \\(z\\) and \\(t\\)-tests); so, following a flowchart again.\nAnother approach would actually be to count—well, take the integral of —the number of observations in the area that belongs to both distributions. By hand this is infeasible, but with a computer this is rather easy. Just sample a reasonable amount of realisations from both distributions (say \\(N\\) times) and count how many times the realisation from the second distribution is smaller than the first distributions. To get a probability, divide by \\(N\\). In R the code simply boils down to:\n\n  N = 100000          # number of draws\n  # Draw from first distribution\n  n1 &lt;- rnorm(N, (-1), 1) \n  # Draw from second distribution\n  n2 &lt;- rnorm(N, 1, 1) \n  # Count how many times second dist.\n  # is smaller than first dist.\n  count &lt;- sum(n2 &lt; n1)    \n  count/N       # get probability \n\nAnd for those who are interested, the probability is approximately \\(0.079\\). Note that this is a full-blown probability and not so much a test. You could easily turn this into a test when comparing this with a pre-defined probability. If you find this probability to high, then you actually have large doubts whether these two coefficients are different.Note that I refrain from using the term significance level. This concept is truly a frequentists’ concept and on a fundamental level relies on the concept of infinite sampling to get uncertainty.\nAlthough this approach is definitely intuitive and arguable very powerful (if you understand the approach above, you basically understand Bayesian statistics as well) it does require computer literate skills from students. And contrary to popular belief, most students actually face large difficulties with coding, command line tools, working with file systems, and so on. This is caused by the fact that all tools they usually work with are driven by drop-down menu’s, templates and strong graphical user interfaces.Typically they work with Powerpoint, Excel and Word.\nThis is also caused by the fact that in regional economics (actually in al the social sciences), remarkably little attention has been given to the set of computer-related tools students could use, why they should use them and the relation between them (with some exceptions as, amongst some others, by Rey 2014; Arribas-Bel and Graaff 2015; Arribas-Bel, Graaff, and Rey \\noop{3002}Forthcoming). This is even more remarkable as reproducibility and robustness of results become more important in research and teaching.\nAnd this does not not apply for statistical software tools such as R or Python, but as well to other fields. Gathering data, manipulating data, visualising data and communing results are all skills that arguably are very important for students and scientist and become even more important in the future (Varian 2014). There are some exceptions as Schwabish (2014), but in (regional) economics these skill still receive not much attention—in research, but especially in education.\nI conclude this section by arguing that we miss three main elements in our curriculum. The first on being a larger emphasis on computer literature skills, such as coding, command line tools, visualization of data, an so forth. The second is more room for the data driven approach, where using software packages such as R of Python, problems are solved with data science techniques, with its larger emphasis on predicting and non-linear modeling. To be clear, simulation exercises as above functions as well as a data driven approach. Most importantly, students should understand the underlying mechanism, instead of applying procedures. The third and final element that is missing is consistently throughout the curriculum. This is often understood as applying the same tools for each course, but this is not necessarily the case. What I mean with consistently is that elements from method courses should come back in regular courses. Nowadays, most courses could implement an empirical element, such as regression techniques, data visualization, data manipulation, and perhaps coding as well. Why otherwise give a Python in the first year of the bachelor, without using that in other courses?"
  },
  {
    "objectID": "posts/post/research_agenda/index.html#into-the-abyss",
    "href": "posts/post/research_agenda/index.html#into-the-abyss",
    "title": "Do regional economists answer the right questions?",
    "section": "4 Into the abyss",
    "text": "4 Into the abyss\nI started this paper with the observation, that, in the words of Breiman (2001), there seems so be two cultures in statistical or econometric modeling; a theory driven and a data driven approach. These two approaches are not mutually exclusive, but complementary. And both have their own strengths and weaknesses. However, especially in economics—and thus in regional economics as well—the theory driven approach still seems to be highly dominant, even with the advent of increasingly larger (micro-)databases. Arguably, this is problematic as the theory driven approach has difficulties when answering questions typically asked by policy makers; questions such as What works best for my region?, What happens with the capacity of my whole network when I invest in a specific highway link? and In which region should I invest to get highest returns?.\nSo, the main argument of this paper lies in introducing more data approach/data science techniques in the toolkit of the regional economist. Other related fields, even in the social sciences, have already made large advances, such as predictive policing in criminology, latent class approaches in transportation modeling, and the use of deep learning techniques in marketing sciences.\nObviously, this needs large investments (mostly in time), both for researchers and for teachers. The first group needs to invest in new techniques and probably in new statistical software. The second group needs to change parts of the curriculum in terms of the specific contents of methods courses and exercises. Fortunately, many online and open source manuals, videos and even textbooks are available. Moreover, companies such as DataCamp allow for free subscriptions as long as the material is used for classes.}Especially for the R programming environment R Core Team (2017) there is a vast amount of material available on the internet, such as R for Data Science and Efficient R programming.\nTo conclude, I would like to note that apart from the intrinsic scientific arguments there are two other very compelling arguments to invest at least some time in data driven approaches. First, it coincides wonderfully with other techniques, such as versioning, blogging (publishing to HTML), and command line tools. All these approaches ensure that research becomes more reproducible. Something that becomes more and more a hard requirement by both university and the audience at large. Second, when looking at recent advances both in industry (e.g., all the dotcom companies but also others, such as more traditional media companies) and in other scientific disciplines, it is not the question if regional economists should invest more in the data science approach, but the question how soon can we start."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Thoughts & musings",
    "section": "",
    "text": "Do regional economists answer the right questions?\n\n\n\n\n\n\nRegional economics\n\n\nPredicting\n\n\nCausality\n\n\nTheory driven approach\n\n\nData science\n\n\n\n\n\n\nJun 5, 2021\n\n\nThomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nStated choice experiments\n\n\n\n\n\n\nTeaching\n\n\nReproducability\n\n\n\n\n\n\nApr 2, 2021\n\n\nThomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nNew Economic Geography model with R\n\n\n\n\n\n\nR\n\n\nEducation\n\n\nInteraction\n\n\n\n\n\n\nFeb 4, 2018\n\n\nThomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing statistical software packages for education in the social sciences\n\n\n\n\n\n\nReproducability\n\n\nEducation\n\n\n\n\n\n\nAug 12, 2017\n\n\nThomas de Graaff\n\n\n\n\n\n\n\n\n\n\n\n\nKrugman’s Increasing Returns and Economic Geography\n\n\n\n\n\n\nInteraction\n\n\nLaTeX\n\n\nEducation\n\n\n\n\n\n\nOct 5, 2015\n\n\nThomas de Graaff\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "I am a spatial economist employed at the department of Spatial Economics of the Vrije Universiteit Amsterdam. My recent research focuses on spatial interaction patterns (e,g., tourism, trade, migration and commuting), reproducibility (e.g., meta-analysis) and the application of Bayesian multilevel models in the domain of spatial economics."
  },
  {
    "objectID": "index.html#latest",
    "href": "index.html#latest",
    "title": "Welcome",
    "section": "Latest …",
    "text": "Latest …\n\n\n\n\n\n\n\n\nAn urban overhead? Crime, agglomeration, and amenity\n\n\nJournal of Housing Economics\n\n\n\nStuart Donovan, Thomas de Graaff, Henri L. F. de Groot, Aaron Schiff\n\n\nJun, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling urban advantages—A meta-analysis of agglomeration economies\n\n\nJournal of Economic Surveys\n\n\n\nStuart Donovan, Thomas de Graaff, Henri L. F. de Groot, Carl C. Koopman\n\n\nJan, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDo regional economists answer the right questions?\n\n\nThis position paper revolves around two main propositions; namely, (i) regional (or spatial) economists are very restrictive in the tool set they apply, and consequently…\n\n\n\nThomas de Graaff\n\n\nJun, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nStated choice experiments\n\n\nSome additional material for students to use a stated choice experiment\n\n\n\nThomas de Graaff\n\n\nApr, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean cultural heritage and tourism flows: The magnetic role of superstar World Heritage Sites\n\n\nPapers in Regional Science\n\n\n\nElisa Panzera, Thomas de Graaff, Henri L. F. de Groot\n\n\nFeb, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nValuation of ethnic diversity: heterogeneous effects in an integrated labor and housing market\n\n\nJournal of Economic Geography\n\n\n\nJessie Bakens, Thomas de Graaff\n\n\nJan, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post/new_economic_geography/index.html",
    "href": "posts/post/new_economic_geography/index.html",
    "title": "New Economic Geography model with R",
    "section": "",
    "text": "Why some regions have more economic activiy than others depend on a variety of factors, including regions’ endowments, good policy and just sheer luck (oftentimes called path dependency). In the 1990s Paul Krugman constructed a model, the Core-Periphery model, that was able to model all these three elements. This model received quite some positive criticism (including a Nobel price), but still is rather complex in wielding it. In this post I show how one can actually program and depict the short term and long term equilibria that the model yields. The derived estimations are coming from Henri de Groot.\n\n\nOne of the reason why this model is complex is that it can easility multiple equilibria like the figure below. Our goal in to prodcude the “Tomahawk” below.\n\n\n\n\n\n\nFigure 1: Multiple equilibria\n\n\n\n\n\n\nWe first need to read in some packages that we will use:\n\n################################################################\n# Read in libraries\n################################################################\n\nlibrary(nleqslv)  # for solving system of nonlinear equations\nlibrary(ggplot2)  # for structurally making plots\nlibrary(ggthemes) # for using economist theme\nlibrary(dplyr)    # for data wrangling\nlibrary(cowplot)  # for combining plots\n\nThen we define some constants. Note that you can change them if you need a different set-up.\n\n################################################################\n# Define parameters\n################################################################\n\nL       &lt;- 2.0  # Total labor force\nphi1    &lt;- 0.48 # fraction of food works living in region 1\ngam     &lt;- 0.3  # fraction that works in manufacturing\neps     &lt;- 5.0  # elasticity of demand\nrho     &lt;- 0.8  # substitution parameter of variety\nbet     &lt;- 0.8  # variable costs\nalp     &lt;- 0.08 # fixed costs\ndelta   &lt;- 0.4  # budget share manufacturing\n\nMoreover, we need some additional (none structural) constants, needed for the iteration and the granularity of our plots:\n\n################################################################\n# Define iterations and stepsize for transporation and lambda\n################################################################\n\niter_l &lt;- 999\nstep_l &lt;- 0.001\nstart_l &lt;- 0.001\n\niter_t &lt;- 51\nstart_t &lt;- 1.5\nstep_t &lt;- 0.01\n\nThe model containts 6 non-linear equations, namely:\n\\[\\begin{aligned}\nY_1 &= \\phi_1(1-\\gamma)L + \\lambda_1 \\gamma LW_1\\\\\\\\\\\\\nY_2 &= \\phi_2(1-\\gamma)L + (1-\\lambda_1) \\gamma LW_2\\\\\\\\\\\\\nW_1 &= \\rho \\beta^{-\\rho}\\left(\\frac{\\delta}{\\alpha(\\epsilon-1)}\\right)^{1/\\epsilon} \\left(Y_1 I_1^{\\epsilon-1} + T^{1-\\epsilon}Y_2 I_2^{\\epsilon-1}\\right)^{1/\\epsilon}\\\\\\\\\\\\\nW_2 &= \\rho \\beta^{-\\rho}\\left(\\frac{\\delta}{\\alpha(\\epsilon-1)}\\right)^{1/\\epsilon} \\left(T^{1-\\epsilon}Y_1 I_1^{\\epsilon-1} + Y_2 I_2^{\\epsilon-1}\\right)^{1/\\epsilon}\\\\\\\\\\\\\nI_1 &= \\left(\\frac{\\gamma L}{\\alpha \\epsilon} \\right)^{1/(1-\\epsilon)}\\left(\\frac{\\beta}{\\rho}\\right) \\left(\\lambda W_1^{1-\\epsilon} + (1-\\lambda)T^{1-\\epsilon} W_2^{1-\\epsilon}\\right)^{1/(1-\\epsilon)}\\\\\\\\\\\\\nI_2 &= \\left(\\frac{\\gamma L}{\\alpha \\epsilon} \\right)^{1/(1-\\epsilon)}\\left(\\frac{\\beta}{\\rho}\\right) \\left(\\lambda T^{1-\\epsilon} W_1^{1-\\epsilon} + (1-\\lambda) W_2^{1-\\epsilon}\\right)^{1/(1-\\epsilon)}\\\\\\\\\\\\\n\\end{aligned}\\]\nThe first two equations denote total regional income for regional 1 and 2, equation 3 and 4 give the regional wages for both regions and the last two equations determine regional price indices.\nThus, the key optimalisation procedure looks as follows:\n\n################################################################\n# Definite optimal function\n################################################################\n\nequilibrium &lt;- function(x){\n\n    Y1 &lt;- x[1]\n    Y2 &lt;- x[2]\n    W1 &lt;- x[3]\n    W2 &lt;- x[4]\n    I1 &lt;- x[5]\n    I2 &lt;- x[6]\n\n    y &lt;- rep(NA, length(x))\n\n    y[1] &lt;- Y1-phi1*(1-gam)*L-lam*gam*L*W1\n    y[2] &lt;- Y2-(1-phi1)*(1-gam)*L-(1-lam)*gam*L*W2\n    y[3] &lt;- W1-rho*bet^(-rho)*(delta/(alp*(eps-1)))^(1/eps)*(Y1*I1^(eps-1)+T^(1-eps)*Y2*I2^(eps-1))^(1/eps)\n    y[4] &lt;- W2-rho*bet^(-rho)*(delta/(alp*(eps-1)))^(1/eps)*(T^(1-eps)*Y1*I1^(eps-1)+Y2*I2^(eps-1))^(1/eps)\n    y[5] &lt;- I1-(gam*L/(alp*eps))^(1/(1-eps))*(bet/rho)*(lam*W1^(1-eps)+(1-lam)*T^(1-eps)*W2^(1-eps))^(1/(1-eps))\n    y[6] &lt;- I2-(gam*L/(alp*eps))^(1/(1-eps))*(bet/rho)*(lam*T^(1-eps)*W1^(1-eps)+(1-lam)*W2^(1-eps))^(1/(1-eps))\n\n    return(y)\n}\n\nAnd finally we need the loop below to create the figures\n\n################################################################\n# Create the vector where the output is stored\n# This is faster than using append\n# we will only append the equilibrium dataframe to find the\n# stable and unstable equiliria (do that in the slower (outer)\n# loop)\n################################################################\n\nrel       &lt;- vector(length = iter_l*iter_t)\nlambda    &lt;- vector(length = iter_l*iter_t)\ntransport &lt;- vector(length = iter_l*iter_t)\nwelfare   &lt;- vector(length = iter_l*iter_t)\nw_man_h   &lt;- vector(length = iter_l*iter_t)\nw_man_f   &lt;- vector(length = iter_l*iter_t)\nw_farm_h  &lt;- vector(length = iter_l*iter_t)\nw_farm_f  &lt;- vector(length = iter_l*iter_t)\n\n################################################################\n# Set the double loop for the  optimal solution using the\n# package nleqslv.\n# The fast (inner) loop is over gamma, The slow (outer) loop is\n# over the transportation costs\n################################################################\n\n# Completely parameterized\nloop_transport &lt;- seq( start_t, start_t + iter_t * step_t - step_t, by = step_t)\nloop_gamma &lt;- seq( start_l, start_l + iter_l * step_l - step_l, by = step_l )\nequilibria &lt;- data.frame(T = numeric(0), gamma = numeric(0), stable = numeric(0))\n\n# Create intial starting values\nstart &lt;- c(1,1,1,1,1,1)\n\niteration &lt;- 0 # General counter\nfor (T in loop_transport){\n  iter_eq &lt;- 0 # Counter to find the equilibria for lambda\n  lam_vec &lt;- vector(length = iter_l) # initialize lambda vector\n  t_vec   &lt;- vector(length = iter_l) # initialize transport vector\n  rel_vec &lt;- vector(length = iter_l) # initialize relative real wage diff vector\n  for (lam in loop_gamma){\n    iteration &lt;- iteration + 1\n    iter_eq   &lt;-  iter_eq + 1\n    opt &lt;- nleqslv(start, equilibrium)\n    Y1 &lt;- opt$x[1]\n    Y2 &lt;- opt$x[2]\n    W1 &lt;- opt$x[3]\n    W2 &lt;- opt$x[4]\n    I1 &lt;- opt$x[5]\n    I2 &lt;- opt$x[6]\n\n    # Fill the various vectors\n    rel[iteration]       &lt;- (W1/I1^delta)/(W2/I2^delta)\n    welfare[iteration]   &lt;- Y1/(I1^delta)+Y2/(I2^delta)\n    w_man_h[iteration]   &lt;- W1/I1^delta\n    w_man_f[iteration]   &lt;- W2/I2^delta\n    w_farm_h[iteration]  &lt;- 1/I1^delta\n    w_farm_f[iteration]  &lt;- 1/I2^delta\n    lambda[iteration]    &lt;- lam\n    transport[iteration] &lt;- T\n\n    # Needed to find the equilibria (a bit redundant but more readible so)\n    lam_vec[iter_eq]     &lt;- lam\n    t_vec[iter_eq]       &lt;- T\n    rel_vec[iter_eq]     &lt;- (W1/I1^delta)/(W2/I2^delta)\n  }\n  eq &lt;- data.frame(t_vec, lam_vec, rel_vec)\n  eq &lt;- eq %&gt;%\n    mutate(\n           dpos = ifelse( ( (rel_vec - 1) &gt;= 0 & ( lag(rel_vec) - 1)  &lt; 0), 1, 0 ),\n           dneg = ifelse( ( (rel_vec - 1) &lt;= 0 & ( lag(rel_vec) - 1)  &gt; 0), 1, 0 )\n           )\n  stable &lt;- eq %&gt;%\n    filter(dneg == 1) %&gt;%\n    mutate(stable =1) %&gt;%\n    select(-dpos)\n  unstable &lt;- eq %&gt;%\n    filter(dpos == 1) %&gt;%\n    mutate(stable =0) %&gt;%\n    select(-dneg)\n  if (nrow(stable) &gt; 0 ) {\n      equilibria &lt;- rbind(equilibria, data.frame(stable[1], stable[2], stable[5]))\n      }\n  if (nrow(unstable) &gt; 0) {\n      equilibria &lt;- rbind(equilibria, data.frame(unstable[1], unstable[2], unstable[5]))\n      }\n  if (nrow(unstable) == 1){\n    equilibria &lt;- rbind(equilibria, c(unstable[1,1], 0, 1))\n    equilibria &lt;- rbind(equilibria, c(unstable[1,1], 1, 1))\n  }\n  if ( (nrow(unstable) == 1) & (nrow(stable) == 1) ) {\n    if (stable$lam_vec[1] &gt; unstable$lam_vec[1] ){\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 0, 1))\n    }\n    if (stable$lam_vec[1] &lt; unstable$lam_vec[1] ){\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 1, 1))\n    }\n  }\n  if ((nrow(unstable) + nrow(stable)) == 3) {\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 1, 1))\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 0, 1))\n  }\n}\n\n################################################################\n# Create the dataframe called neg_data\n################################################################\n\nneg_data &lt;- data.frame(transport, lambda, rel, welfare,\n                       w_man_h, w_man_f, w_farm_h, w_farm_f)\n\n################################################################\n# For creating the plots\n################################################################\n\n#Indicate which lines should be highlighted\ntop_line &lt;- neg_data[neg_data$transport == \"1.5\", ]\nbottom_line &lt;- neg_data[neg_data$transport == \"2\", ]\nmid_line &lt;- neg_data[neg_data$transport == \"1.75\", ]\n\nSo we can get our wiggle and tomahawk picture we want:\n\nggplot(neg_data) + aes(lambda, rel, group = transport) + geom_line(size = 0.5, colour=\"grey\", alpha = 0.5) +\n  geom_line(data = top_line, aes(x = lambda, y = rel, group = transport, colour = \"steelblue\"), size = 1) +\n  geom_line(data = bottom_line, aes(x = lambda, y = rel, group = transport, colour = \"black\"), size = 1) +\n  geom_line(data = mid_line, aes(x = lambda, y = rel, group = transport, colour = \"red\"), size = 1) +\n  scale_colour_discrete(name = \"Transportation costs\", labels = c(\"High\", \"Medium\", \"Low\")) +\n  geom_hline(yintercept = 1, size = 1, colour = \"red\", linetype = 4) +\n  theme_economist() +\n  labs(title =\"Wiggle diagram\", y = \"Relative real wage\",\n       subtitle = \"Changes in relative real wage with varying lambda and transportation costs\")\n\n\n\n\n\n\n\n\n\nggplot(equilibria) + aes(t_vec, lam_vec) +\n  geom_point(aes(colour = factor(stable))) +\n  theme_economist() +\n  theme(legend.title=element_blank()) +\n  scale_colour_discrete(breaks = c(\"0\", \"1\"), labels=c(\"Unstable equilibrium\", \"Stable equilibrium\")) +\n  labs(title =\"Tomahawk\", y = \"lambda\", x = \"transportation costs\")"
  },
  {
    "objectID": "posts/post/new_economic_geography/index.html#new-economic-geography-model-with-r",
    "href": "posts/post/new_economic_geography/index.html#new-economic-geography-model-with-r",
    "title": "New Economic Geography model with R",
    "section": "",
    "text": "Why some regions have more economic activiy than others depend on a variety of factors, including regions’ endowments, good policy and just sheer luck (oftentimes called path dependency). In the 1990s Paul Krugman constructed a model, the Core-Periphery model, that was able to model all these three elements. This model received quite some positive criticism (including a Nobel price), but still is rather complex in wielding it. In this post I show how one can actually program and depict the short term and long term equilibria that the model yields. The derived estimations are coming from Henri de Groot.\n\n\nOne of the reason why this model is complex is that it can easility multiple equilibria like the figure below. Our goal in to prodcude the “Tomahawk” below.\n\n\n\n\n\n\nFigure 1: Multiple equilibria\n\n\n\n\n\n\nWe first need to read in some packages that we will use:\n\n################################################################\n# Read in libraries\n################################################################\n\nlibrary(nleqslv)  # for solving system of nonlinear equations\nlibrary(ggplot2)  # for structurally making plots\nlibrary(ggthemes) # for using economist theme\nlibrary(dplyr)    # for data wrangling\nlibrary(cowplot)  # for combining plots\n\nThen we define some constants. Note that you can change them if you need a different set-up.\n\n################################################################\n# Define parameters\n################################################################\n\nL       &lt;- 2.0  # Total labor force\nphi1    &lt;- 0.48 # fraction of food works living in region 1\ngam     &lt;- 0.3  # fraction that works in manufacturing\neps     &lt;- 5.0  # elasticity of demand\nrho     &lt;- 0.8  # substitution parameter of variety\nbet     &lt;- 0.8  # variable costs\nalp     &lt;- 0.08 # fixed costs\ndelta   &lt;- 0.4  # budget share manufacturing\n\nMoreover, we need some additional (none structural) constants, needed for the iteration and the granularity of our plots:\n\n################################################################\n# Define iterations and stepsize for transporation and lambda\n################################################################\n\niter_l &lt;- 999\nstep_l &lt;- 0.001\nstart_l &lt;- 0.001\n\niter_t &lt;- 51\nstart_t &lt;- 1.5\nstep_t &lt;- 0.01\n\nThe model containts 6 non-linear equations, namely:\n\\[\\begin{aligned}\nY_1 &= \\phi_1(1-\\gamma)L + \\lambda_1 \\gamma LW_1\\\\\\\\\\\\\nY_2 &= \\phi_2(1-\\gamma)L + (1-\\lambda_1) \\gamma LW_2\\\\\\\\\\\\\nW_1 &= \\rho \\beta^{-\\rho}\\left(\\frac{\\delta}{\\alpha(\\epsilon-1)}\\right)^{1/\\epsilon} \\left(Y_1 I_1^{\\epsilon-1} + T^{1-\\epsilon}Y_2 I_2^{\\epsilon-1}\\right)^{1/\\epsilon}\\\\\\\\\\\\\nW_2 &= \\rho \\beta^{-\\rho}\\left(\\frac{\\delta}{\\alpha(\\epsilon-1)}\\right)^{1/\\epsilon} \\left(T^{1-\\epsilon}Y_1 I_1^{\\epsilon-1} + Y_2 I_2^{\\epsilon-1}\\right)^{1/\\epsilon}\\\\\\\\\\\\\nI_1 &= \\left(\\frac{\\gamma L}{\\alpha \\epsilon} \\right)^{1/(1-\\epsilon)}\\left(\\frac{\\beta}{\\rho}\\right) \\left(\\lambda W_1^{1-\\epsilon} + (1-\\lambda)T^{1-\\epsilon} W_2^{1-\\epsilon}\\right)^{1/(1-\\epsilon)}\\\\\\\\\\\\\nI_2 &= \\left(\\frac{\\gamma L}{\\alpha \\epsilon} \\right)^{1/(1-\\epsilon)}\\left(\\frac{\\beta}{\\rho}\\right) \\left(\\lambda T^{1-\\epsilon} W_1^{1-\\epsilon} + (1-\\lambda) W_2^{1-\\epsilon}\\right)^{1/(1-\\epsilon)}\\\\\\\\\\\\\n\\end{aligned}\\]\nThe first two equations denote total regional income for regional 1 and 2, equation 3 and 4 give the regional wages for both regions and the last two equations determine regional price indices.\nThus, the key optimalisation procedure looks as follows:\n\n################################################################\n# Definite optimal function\n################################################################\n\nequilibrium &lt;- function(x){\n\n    Y1 &lt;- x[1]\n    Y2 &lt;- x[2]\n    W1 &lt;- x[3]\n    W2 &lt;- x[4]\n    I1 &lt;- x[5]\n    I2 &lt;- x[6]\n\n    y &lt;- rep(NA, length(x))\n\n    y[1] &lt;- Y1-phi1*(1-gam)*L-lam*gam*L*W1\n    y[2] &lt;- Y2-(1-phi1)*(1-gam)*L-(1-lam)*gam*L*W2\n    y[3] &lt;- W1-rho*bet^(-rho)*(delta/(alp*(eps-1)))^(1/eps)*(Y1*I1^(eps-1)+T^(1-eps)*Y2*I2^(eps-1))^(1/eps)\n    y[4] &lt;- W2-rho*bet^(-rho)*(delta/(alp*(eps-1)))^(1/eps)*(T^(1-eps)*Y1*I1^(eps-1)+Y2*I2^(eps-1))^(1/eps)\n    y[5] &lt;- I1-(gam*L/(alp*eps))^(1/(1-eps))*(bet/rho)*(lam*W1^(1-eps)+(1-lam)*T^(1-eps)*W2^(1-eps))^(1/(1-eps))\n    y[6] &lt;- I2-(gam*L/(alp*eps))^(1/(1-eps))*(bet/rho)*(lam*T^(1-eps)*W1^(1-eps)+(1-lam)*W2^(1-eps))^(1/(1-eps))\n\n    return(y)\n}\n\nAnd finally we need the loop below to create the figures\n\n################################################################\n# Create the vector where the output is stored\n# This is faster than using append\n# we will only append the equilibrium dataframe to find the\n# stable and unstable equiliria (do that in the slower (outer)\n# loop)\n################################################################\n\nrel       &lt;- vector(length = iter_l*iter_t)\nlambda    &lt;- vector(length = iter_l*iter_t)\ntransport &lt;- vector(length = iter_l*iter_t)\nwelfare   &lt;- vector(length = iter_l*iter_t)\nw_man_h   &lt;- vector(length = iter_l*iter_t)\nw_man_f   &lt;- vector(length = iter_l*iter_t)\nw_farm_h  &lt;- vector(length = iter_l*iter_t)\nw_farm_f  &lt;- vector(length = iter_l*iter_t)\n\n################################################################\n# Set the double loop for the  optimal solution using the\n# package nleqslv.\n# The fast (inner) loop is over gamma, The slow (outer) loop is\n# over the transportation costs\n################################################################\n\n# Completely parameterized\nloop_transport &lt;- seq( start_t, start_t + iter_t * step_t - step_t, by = step_t)\nloop_gamma &lt;- seq( start_l, start_l + iter_l * step_l - step_l, by = step_l )\nequilibria &lt;- data.frame(T = numeric(0), gamma = numeric(0), stable = numeric(0))\n\n# Create intial starting values\nstart &lt;- c(1,1,1,1,1,1)\n\niteration &lt;- 0 # General counter\nfor (T in loop_transport){\n  iter_eq &lt;- 0 # Counter to find the equilibria for lambda\n  lam_vec &lt;- vector(length = iter_l) # initialize lambda vector\n  t_vec   &lt;- vector(length = iter_l) # initialize transport vector\n  rel_vec &lt;- vector(length = iter_l) # initialize relative real wage diff vector\n  for (lam in loop_gamma){\n    iteration &lt;- iteration + 1\n    iter_eq   &lt;-  iter_eq + 1\n    opt &lt;- nleqslv(start, equilibrium)\n    Y1 &lt;- opt$x[1]\n    Y2 &lt;- opt$x[2]\n    W1 &lt;- opt$x[3]\n    W2 &lt;- opt$x[4]\n    I1 &lt;- opt$x[5]\n    I2 &lt;- opt$x[6]\n\n    # Fill the various vectors\n    rel[iteration]       &lt;- (W1/I1^delta)/(W2/I2^delta)\n    welfare[iteration]   &lt;- Y1/(I1^delta)+Y2/(I2^delta)\n    w_man_h[iteration]   &lt;- W1/I1^delta\n    w_man_f[iteration]   &lt;- W2/I2^delta\n    w_farm_h[iteration]  &lt;- 1/I1^delta\n    w_farm_f[iteration]  &lt;- 1/I2^delta\n    lambda[iteration]    &lt;- lam\n    transport[iteration] &lt;- T\n\n    # Needed to find the equilibria (a bit redundant but more readible so)\n    lam_vec[iter_eq]     &lt;- lam\n    t_vec[iter_eq]       &lt;- T\n    rel_vec[iter_eq]     &lt;- (W1/I1^delta)/(W2/I2^delta)\n  }\n  eq &lt;- data.frame(t_vec, lam_vec, rel_vec)\n  eq &lt;- eq %&gt;%\n    mutate(\n           dpos = ifelse( ( (rel_vec - 1) &gt;= 0 & ( lag(rel_vec) - 1)  &lt; 0), 1, 0 ),\n           dneg = ifelse( ( (rel_vec - 1) &lt;= 0 & ( lag(rel_vec) - 1)  &gt; 0), 1, 0 )\n           )\n  stable &lt;- eq %&gt;%\n    filter(dneg == 1) %&gt;%\n    mutate(stable =1) %&gt;%\n    select(-dpos)\n  unstable &lt;- eq %&gt;%\n    filter(dpos == 1) %&gt;%\n    mutate(stable =0) %&gt;%\n    select(-dneg)\n  if (nrow(stable) &gt; 0 ) {\n      equilibria &lt;- rbind(equilibria, data.frame(stable[1], stable[2], stable[5]))\n      }\n  if (nrow(unstable) &gt; 0) {\n      equilibria &lt;- rbind(equilibria, data.frame(unstable[1], unstable[2], unstable[5]))\n      }\n  if (nrow(unstable) == 1){\n    equilibria &lt;- rbind(equilibria, c(unstable[1,1], 0, 1))\n    equilibria &lt;- rbind(equilibria, c(unstable[1,1], 1, 1))\n  }\n  if ( (nrow(unstable) == 1) & (nrow(stable) == 1) ) {\n    if (stable$lam_vec[1] &gt; unstable$lam_vec[1] ){\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 0, 1))\n    }\n    if (stable$lam_vec[1] &lt; unstable$lam_vec[1] ){\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 1, 1))\n    }\n  }\n  if ((nrow(unstable) + nrow(stable)) == 3) {\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 1, 1))\n      equilibria &lt;- rbind(equilibria, c(unstable[1,1], 0, 1))\n  }\n}\n\n################################################################\n# Create the dataframe called neg_data\n################################################################\n\nneg_data &lt;- data.frame(transport, lambda, rel, welfare,\n                       w_man_h, w_man_f, w_farm_h, w_farm_f)\n\n################################################################\n# For creating the plots\n################################################################\n\n#Indicate which lines should be highlighted\ntop_line &lt;- neg_data[neg_data$transport == \"1.5\", ]\nbottom_line &lt;- neg_data[neg_data$transport == \"2\", ]\nmid_line &lt;- neg_data[neg_data$transport == \"1.75\", ]\n\nSo we can get our wiggle and tomahawk picture we want:\n\nggplot(neg_data) + aes(lambda, rel, group = transport) + geom_line(size = 0.5, colour=\"grey\", alpha = 0.5) +\n  geom_line(data = top_line, aes(x = lambda, y = rel, group = transport, colour = \"steelblue\"), size = 1) +\n  geom_line(data = bottom_line, aes(x = lambda, y = rel, group = transport, colour = \"black\"), size = 1) +\n  geom_line(data = mid_line, aes(x = lambda, y = rel, group = transport, colour = \"red\"), size = 1) +\n  scale_colour_discrete(name = \"Transportation costs\", labels = c(\"High\", \"Medium\", \"Low\")) +\n  geom_hline(yintercept = 1, size = 1, colour = \"red\", linetype = 4) +\n  theme_economist() +\n  labs(title =\"Wiggle diagram\", y = \"Relative real wage\",\n       subtitle = \"Changes in relative real wage with varying lambda and transportation costs\")\n\n\n\n\n\n\n\n\n\nggplot(equilibria) + aes(t_vec, lam_vec) +\n  geom_point(aes(colour = factor(stable))) +\n  theme_economist() +\n  theme(legend.title=element_blank()) +\n  scale_colour_discrete(breaks = c(\"0\", \"1\"), labels=c(\"Unstable equilibrium\", \"Stable equilibrium\")) +\n  labs(title =\"Tomahawk\", y = \"lambda\", x = \"transportation costs\")"
  },
  {
    "objectID": "research/articles/Panzera-2021/index.html",
    "href": "research/articles/Panzera-2021/index.html",
    "title": "European cultural heritage and tourism flows: The magnetic role of superstar World Heritage Sites",
    "section": "",
    "text": "Cultural heritage is a potentially important determinant of international tourism flows. Apart from being an enrichment for both individuals and communities and an opportunity for different cultures to meet, tourism also represents a significant industry for European economies. We empirically investigate the impact of the endowment of tangible cultural heritage on tourism attractiveness of European regions. We measure material forms of cultural heritage both as regional density of locally defined monuments, cultural landscapes and museums, and as number of cultural sites listed in the UNESCO World Heritage Sites international programme. Using a Bayesian multilevel gravity model, we find that UNESCO cultural World Heritage Sites are associated with an increase of 6,000 (one site) to 60,000 (eight sites) international tourists from each European country to an average European region. On the other hand, regionally or nationally defined tangible forms of heritage play a more limited role as pull‐factors for international tourism. Moreover, we show that the presence of UNESCO sites reduces the distance decay effect. International tourists are willing to travel longer distance if a destination is endowed with UNESCO cultural World Heritage Sites."
  },
  {
    "objectID": "research/articles/Panzera-2021/index.html#abstract",
    "href": "research/articles/Panzera-2021/index.html#abstract",
    "title": "European cultural heritage and tourism flows: The magnetic role of superstar World Heritage Sites",
    "section": "",
    "text": "Cultural heritage is a potentially important determinant of international tourism flows. Apart from being an enrichment for both individuals and communities and an opportunity for different cultures to meet, tourism also represents a significant industry for European economies. We empirically investigate the impact of the endowment of tangible cultural heritage on tourism attractiveness of European regions. We measure material forms of cultural heritage both as regional density of locally defined monuments, cultural landscapes and museums, and as number of cultural sites listed in the UNESCO World Heritage Sites international programme. Using a Bayesian multilevel gravity model, we find that UNESCO cultural World Heritage Sites are associated with an increase of 6,000 (one site) to 60,000 (eight sites) international tourists from each European country to an average European region. On the other hand, regionally or nationally defined tangible forms of heritage play a more limited role as pull‐factors for international tourism. Moreover, we show that the presence of UNESCO sites reduces the distance decay effect. International tourists are willing to travel longer distance if a destination is endowed with UNESCO cultural World Heritage Sites."
  },
  {
    "objectID": "research/articles/Panzera-2021/index.html#the-effect-of-crime-on-agglomeration-economies-in-consumption",
    "href": "research/articles/Panzera-2021/index.html#the-effect-of-crime-on-agglomeration-economies-in-consumption",
    "title": "European cultural heritage and tourism flows: The magnetic role of superstar World Heritage Sites",
    "section": "The effect of crime on agglomeration economies in consumption",
    "text": "The effect of crime on agglomeration economies in consumption\nn this paper we test the hypothesis that, because of their unique and idiosyncratic nature, tangible cultural heritage sites in destination regions enhance international inward tourist flows. To do so, we estimate a Bayesian multi-level gravity model to identify the determinants of international tourist arrivals to European NUTS 2 regions. The aim of the paper is twofold: (i) we contribute to the debate about the relationship between destination endowments of tangible cultural heritage and tourist movements focusing on European destinations; and (ii) draw attention to the heterogeneous appeal of different kinds of tangible cultural heritage. Since diverse measures of heritage carry differ- ent conceptual implications, we implement different proxies for the endowment of cultural heritage. To do so, we first use a bundle of quantitative measures provided by ESPON,1 namely territorial density of monuments, cultural landscapes and museums. As these indicators are built using national and regional heritage lists, we investigate whether regionally or nationally defined cultural heritage acts as a tourist attractor. Second, we consider the regional endowment of UNESCO World Heritage Sites starting from the idea that these kinds of sites are able to engage international tourist acting as an appealing territorial resource for tourism.2 This indicator of tangible heritage is more related to international visibility and promotion rather than to quantity and local relevance. Moreover, we test whether UNESCO World Heritage Sites represent a quality guarantee attracting tourists from further origins. In other words, whether tourists are more willing to travel longer distances to visit their destination if a UNESCO site is there. We therefore analyse whether UNESCO listings reduce the distance decay effect. Our main findings are that UNESCO World Heritage sites are an important determinant of attracting international tourists rather than nationally defined monuments, museums and cultural landscapes. We find that the regional presence of UNESCO World Heritage Sites correlates with an increase of 6,000 (one site) to about 60,000 (eight sites) in international tourist flows from each country in our dataset. Furthermore, we find that the presence of World Heritage Sites indeed flattens out the distance-decay curve, making regions endowed with these sites more appealing for tourists travelling from further away.\n\n\n\nDistance effects on amount of tourist arrivals by amount of UNESCO World Heritage Sites (all other variables are fixed at mean values)"
  },
  {
    "objectID": "research/articles/Panzera-2021/index.html#citation",
    "href": "research/articles/Panzera-2021/index.html#citation",
    "title": "European cultural heritage and tourism flows: The magnetic role of superstar World Heritage Sites",
    "section": "Citation",
    "text": "Citation\n@article{panzera2021european,\n  title={European cultural heritage and tourism flows: The magnetic role of superstar World Heritage Sites},\n  author={Panzera, Elisa and de Graaff, Thomas and de Groot, Henri LF},\n  journal={Papers in Regional Science},\n  volume={100},\n  number={1},\n  pages={101--123},\n  year={2021},\n  publisher={Elsevier}\n}"
  },
  {
    "objectID": "research/articles/bakens-2020/index.html",
    "href": "research/articles/bakens-2020/index.html",
    "title": "Valuation of ethnic diversity: heterogeneous effects in an integrated labor and housing market",
    "section": "",
    "text": "We estimate the heterogeneous impact of the scale, composition and consumer good effect of ethnic diversity on individuals’ job and residential location. Using an extensive pooled micro panel data set in which homeowners in the Netherlands are identified in both the housing and labor market, we can derive the combined effect of ethnic diversity in both markets. We test a model that integrates the utility and production function such that the location of work and residence is determined simultaneously by taking into account observed and unobserved heterogeneous individual behavior on both markets. We find that the scale of ethnic diversity, that is the share of immigrants, at the city level is mostly positively related to both wages and house prices. This is mainly through a positive productivity effect of immigrants, which results in negative implicit prices for housing (although small) in a city with a higher scale of ethnic diversity for the majority of the individuals in our data. The scale of ethnic diversity is only positively related to utility for a small group of homeowners, while the composition (diversity among immigrants) and the consumer good-effect (ethnic diversity of restaurants) of ethnic diversity show overall no significant effect on both markets nor significant implicit prices. Moreover, we find that the majority of Dutch homeowners do not sort themselves out over municipalities by their preferences for ethnic diversity."
  },
  {
    "objectID": "research/articles/bakens-2020/index.html#abstract",
    "href": "research/articles/bakens-2020/index.html#abstract",
    "title": "Valuation of ethnic diversity: heterogeneous effects in an integrated labor and housing market",
    "section": "",
    "text": "We estimate the heterogeneous impact of the scale, composition and consumer good effect of ethnic diversity on individuals’ job and residential location. Using an extensive pooled micro panel data set in which homeowners in the Netherlands are identified in both the housing and labor market, we can derive the combined effect of ethnic diversity in both markets. We test a model that integrates the utility and production function such that the location of work and residence is determined simultaneously by taking into account observed and unobserved heterogeneous individual behavior on both markets. We find that the scale of ethnic diversity, that is the share of immigrants, at the city level is mostly positively related to both wages and house prices. This is mainly through a positive productivity effect of immigrants, which results in negative implicit prices for housing (although small) in a city with a higher scale of ethnic diversity for the majority of the individuals in our data. The scale of ethnic diversity is only positively related to utility for a small group of homeowners, while the composition (diversity among immigrants) and the consumer good-effect (ethnic diversity of restaurants) of ethnic diversity show overall no significant effect on both markets nor significant implicit prices. Moreover, we find that the majority of Dutch homeowners do not sort themselves out over municipalities by their preferences for ethnic diversity."
  },
  {
    "objectID": "research/articles/bakens-2020/index.html#contribution",
    "href": "research/articles/bakens-2020/index.html#contribution",
    "title": "Valuation of ethnic diversity: heterogeneous effects in an integrated labor and housing market",
    "section": "Contribution",
    "text": "Contribution\nThe main contribution of our article to the literature is 3-fold. First, our approach overcomes the methodological issues inherent in most research on this topic that depends heavily on the specific subsample chosen or are focused on the ‘average’ effect. The latter issue is concerned with the fact that it is either assumed that all individuals benefit equally from ethnic diversity or that specific homogeneous groups of individuals are sampled to control for possible variation in the impacts (see, e.g. Florida, 2002; Ottaviano and Peri, 2005, 2006; Dalmazzo and de Blasio, 2011). The usual approach to account for possible heterogeneity in the effects is to use individual fixed effects. This approach controls for (un)observed individual characteristics that might determine differences in utility and preferences. However, it is precisely these (un)observed heterogeneous individual characteristics that play an important role in explaining the individual differences in wages and utility (Bakens et al., 2013). Thus, disregarding individual variation in characteristics by focusing on the average effects of ethnic diversity on wages and utility very likely provides a misguided view of the effects of ethnic diversity on specific population groups. By using a finite mixture model, no pre- determined samples of individuals need to be made, but effects can be estimated over different groups within the model. Although there is related research in which Ozgen and de Graaff (2013) and Nathan (2016) apply finite mixture models to analyze the effect of (ethnic) diversity on productivity and innovation, and Kemeny and Cooke (2018) analyze the effect of ethnic diversity on productivity along wage quartiles, we are not aware of other research that explores heterogeneous effects of population diversity on individual utility.\n\n\n\n\n\n\n\n\nFigure 1: Distribution of income over (ethnic) amenity quartiles at the city level.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Distribution of house prices (per square meter) over (ethnic) amenity quartiles at the city level.\n\n\n\n\nSecondly, by using a theoretical framework based upon Roback (1982, 1988), we show that failing to account for unobserved heterogeneity simultaneously in both labor and housing markets results in biased estimates of the impacts of diversity on wages and utility. Our empirical analysis indeed shows that estimating wage and housing prices separately yield different results when compared with our simultaneous estimation.\nFinally, we employ an extensive micro dataset consisting of a pooled cross-section of administrative income and housing data between 1998 and 2008 in the Netherlands. This data focus on homeowners in the labor and the housing market. The micro-data allow a simultaneous estimation of an individual’s correlated choice of location of living and work and we explicitly allow individuals to work and live in different cities. Relaxing the assumption that the amenity endowment in the area of work is equal to that of the area of residence accommodates the empirical setting in many countries, especially in the Netherlands, where half of the workers do not live in the same municipality in which they work. Because we use a pooled cross-section instead of panel data, this article is focused more on estimating possible heterogeneity in correlations for different individuals than on causal effects."
  },
  {
    "objectID": "research/articles/bakens-2020/index.html#citation",
    "href": "research/articles/bakens-2020/index.html#citation",
    "title": "Valuation of ethnic diversity: heterogeneous effects in an integrated labor and housing market",
    "section": "Citation",
    "text": "Citation\n@article{bakens2020valuation,\n  title={Valuation of ethnic diversity: Heterogeneous effects in an integrated labor and housing market},\n  author={Bakens, Jessie and De Graaff, Thomas},\n  journal={Journal of Economic Geography},\n  volume={20},\n  number={1},\n  pages={197--223},\n  year={2020},\n  publisher={Oxford University Press}\n}"
  },
  {
    "objectID": "research/articles/gubins-2019/index.html",
    "href": "research/articles/gubins-2019/index.html",
    "title": "Does new information technology change commuting behavior?",
    "section": "",
    "text": "We estimate the long-run causal effect of information technology, i.e., Internet and powerful computers, as measured by the adoption of teleworking, on average commuting distance within professions in the Netherlands. We employ data for 2 years—1996 when information technology was hardly adopted and 2010 when information technology was widely used in a wide range of professions. Variation in information technology adoption over time and between professions allows us to infer the causal effect of interest using difference-in-differences techniques combined with propensity score matching. Our results show that the long-run causal effect of information technology on commuting distance is too small to be identified and likely to be absent. This suggests that, contrary to some assertions, the advent of information technology did not have a profound impact on the spatial structure of the labor market.\n\n\n\nCommuting distances for treated and non-treated professions"
  },
  {
    "objectID": "research/articles/gubins-2019/index.html#abstract",
    "href": "research/articles/gubins-2019/index.html#abstract",
    "title": "Does new information technology change commuting behavior?",
    "section": "",
    "text": "We estimate the long-run causal effect of information technology, i.e., Internet and powerful computers, as measured by the adoption of teleworking, on average commuting distance within professions in the Netherlands. We employ data for 2 years—1996 when information technology was hardly adopted and 2010 when information technology was widely used in a wide range of professions. Variation in information technology adoption over time and between professions allows us to infer the causal effect of interest using difference-in-differences techniques combined with propensity score matching. Our results show that the long-run causal effect of information technology on commuting distance is too small to be identified and likely to be absent. This suggests that, contrary to some assertions, the advent of information technology did not have a profound impact on the spatial structure of the labor market.\n\n\n\nCommuting distances for treated and non-treated professions"
  },
  {
    "objectID": "research/articles/gubins-2012/index.html",
    "href": "research/articles/gubins-2012/index.html",
    "title": "Welfare effects of road pricing and traffic information under alternative ownership regimes",
    "section": "",
    "text": "This paper models strategic interactions between a road supplier, a provider of traffic information, and road users, with stochastic travel times. Using a game-theoretical analysis of suppliers’ pricing strategies, we assess the social welfare effects of traffic information under various ownership regimes. The results show that the distortive welfare effect of monopolistic information pricing appears relatively small. Collusion of the road operator and information provider yields higher social welfare than independent pricing by two firms. The intuition behind this result resembles that behind the welfare effects of double marginalization, but is not exactly the same, as traffic information is not strictly complementary to road use."
  },
  {
    "objectID": "research/articles/gubins-2012/index.html#abstract",
    "href": "research/articles/gubins-2012/index.html#abstract",
    "title": "Welfare effects of road pricing and traffic information under alternative ownership regimes",
    "section": "",
    "text": "This paper models strategic interactions between a road supplier, a provider of traffic information, and road users, with stochastic travel times. Using a game-theoretical analysis of suppliers’ pricing strategies, we assess the social welfare effects of traffic information under various ownership regimes. The results show that the distortive welfare effect of monopolistic information pricing appears relatively small. Collusion of the road operator and information provider yields higher social welfare than independent pricing by two firms. The intuition behind this result resembles that behind the welfare effects of double marginalization, but is not exactly the same, as traffic information is not strictly complementary to road use."
  },
  {
    "objectID": "research/articles/gubins-2012/index.html#illustrative-example-of-reaction-functions",
    "href": "research/articles/gubins-2012/index.html#illustrative-example-of-reaction-functions",
    "title": "Welfare effects of road pricing and traffic information under alternative ownership regimes",
    "section": "Illustrative example of reaction functions",
    "text": "Illustrative example of reaction functions\n\n\n\nPossible reaction functions of an information provider and a road operator."
  }
]